{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch_book.scan.crawler import crawl_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCHVISION_MODELS = [\n",
    "    \"alexnet\",\n",
    "    \"googlenet\",\n",
    "    \"vgg11\",\n",
    "    \"vgg11_bn\",\n",
    "    \"vgg13\",\n",
    "    \"vgg13_bn\",\n",
    "    \"vgg16\",\n",
    "    \"vgg16_bn\",\n",
    "    \"vgg19\",\n",
    "    \"vgg19_bn\",\n",
    "    \"resnet18\",\n",
    "    \"resnet34\",\n",
    "    \"resnet50\",\n",
    "    \"resnet101\",\n",
    "    \"resnet152\",\n",
    "    \"inception_v3\",\n",
    "    \"squeezenet1_0\",\n",
    "    \"squeezenet1_1\",\n",
    "    \"wide_resnet50_2\",\n",
    "    \"wide_resnet101_2\",\n",
    "    \"densenet121\",\n",
    "    \"densenet161\",\n",
    "    \"densenet169\",\n",
    "    \"densenet201\",\n",
    "    \"resnext50_32x4d\",\n",
    "    \"resnext101_32x8d\",\n",
    "    \"mobilenet_v2\",\n",
    "    \"shufflenet_v2_x0_5\",\n",
    "    \"shufflenet_v2_x1_0\",\n",
    "    \"shufflenet_v2_x1_5\",\n",
    "    \"shufflenet_v2_x2_0\",\n",
    "    \"mnasnet0_5\",\n",
    "    \"mnasnet0_75\",\n",
    "    \"mnasnet1_0\",\n",
    "    \"mnasnet1_3\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                   Params (M)    FLOPs (G)     MACs (G)      DMAs (G)      RF        \n",
      "------------------------------------------------------------------------------------------\n",
      "alexnet              | 61.10      | 1.43       | 0.71       | 0.72       | 1         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/tmp/cache/conda/envs/py311/lib/python3.11/site-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googlenet            | 6.62       | 3.01       | 1.51       | 1.53       | 1         \n",
      "vgg11                | 132.86     | 15.23      | 7.61       | 7.64       | 1         \n",
      "vgg11_bn             | 132.87     | 15.26      | 7.63       | 7.66       | 1         \n",
      "vgg13                | 133.05     | 22.63      | 11.31      | 11.35      | 1         \n",
      "vgg13_bn             | 133.05     | 22.68      | 11.33      | 11.37      | 1         \n",
      "vgg16                | 138.36     | 30.96      | 15.47      | 15.52      | 1         \n",
      "vgg16_bn             | 138.37     | 31.01      | 15.50      | 15.55      | 1         \n",
      "vgg19                | 143.67     | 39.28      | 19.63      | 19.69      | 1         \n",
      "vgg19_bn             | 143.68     | 39.34      | 19.66      | 19.72      | 1         \n",
      "resnet18             | 11.69      | 3.64       | 1.82       | 1.84       | 1         \n",
      "resnet34             | 21.80      | 7.34       | 3.67       | 3.70       | 1         \n",
      "resnet50             | 25.56      | 8.21       | 4.11       | 4.15       | 1         \n",
      "resnet101            | 44.55      | 15.66      | 7.83       | 7.90       | 1         \n",
      "resnet152            | 60.19      | 23.10      | 11.56      | 11.65      | 1         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/tmp/cache/conda/envs/py311/lib/python3.11/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception_v3         | 23.83      | 11.45      | 5.73       | 5.77       | 1         \n",
      "squeezenet1_0        | 1.25       | 1.64       | 0.82       | 0.83       | 1         \n",
      "squeezenet1_1        | 1.24       | 0.70       | 0.35       | 0.36       | 1         \n",
      "wide_resnet50_2      | 68.88      | 22.84      | 11.43      | 11.51      | 1         \n",
      "wide_resnet101_2     | 126.89     | 45.58      | 22.80      | 22.95      | 1         \n",
      "densenet121          | 7.98       | 5.74       | 2.87       | 2.90       | 1         \n",
      "densenet161          | 28.68      | 15.59      | 7.79       | 7.86       | 1         \n",
      "densenet169          | 14.15      | 6.81       | 3.40       | 3.44       | 1         \n",
      "densenet201          | 20.01      | 8.70       | 4.34       | 4.39       | 1         \n",
      "resnext50_32x4d      | 25.03      | 8.51       | 4.26       | 4.30       | 1         \n",
      "resnext101_32x8d     | 88.79      | 32.93      | 16.48      | 16.61      | 1         \n",
      "mobilenet_v2         | 3.50       | 0.63       | 0.31       | 0.33       | 1         \n",
      "shufflenet_v2_x0_5   | 1.37       | 0.09       | 0.04       | 0.05       | 1         \n",
      "shufflenet_v2_x1_0   | 2.28       | 0.30       | 0.15       | 0.15       | 1         \n",
      "shufflenet_v2_x1_5   | 3.50       | 0.60       | 0.30       | 0.31       | 1         \n",
      "shufflenet_v2_x2_0   | 7.39       | 1.18       | 0.59       | 0.60       | 1         \n",
      "mnasnet0_5           | 2.22       | 0.22       | 0.11       | 0.12       | 1         \n",
      "mnasnet0_75          | 3.17       | 0.45       | 0.23       | 0.24       | 1         \n",
      "mnasnet1_0           | 4.38       | 0.65       | 0.33       | 0.34       | 1         \n",
      "mnasnet1_3           | 6.28       | 1.08       | 0.54       | 0.56       | 1         \n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "margin = 4\n",
    "headers = [\"Model\", \"Params (M)\", \"FLOPs (G)\", \"MACs (G)\", \"DMAs (G)\", \"RF\"]\n",
    "max_w = [20, 10, 10, 10, 10, 10]\n",
    "\n",
    "info_str = [(\" \" * margin).join([f\"{col_name:<{col_w}}\" for col_name, col_w in zip(headers, max_w)])]\n",
    "info_str.append(\"-\" * len(info_str[0]))\n",
    "print(\"\\n\".join(info_str))\n",
    "for name in TORCHVISION_MODELS:\n",
    "    model = models.__dict__[name]().eval().to(device)\n",
    "    dsize = (3, 224, 224)\n",
    "    if \"inception\" in name:\n",
    "        dsize = (3, 299, 299)\n",
    "    model_info = crawl_module(model, dsize)\n",
    "\n",
    "    tot_params = sum(layer[\"grad_params\"] + layer[\"nograd_params\"] for layer in model_info[\"layers\"])\n",
    "    tot_flops = sum(layer[\"flops\"] for layer in model_info[\"layers\"])\n",
    "    tot_macs = sum(layer[\"macs\"] for layer in model_info[\"layers\"])\n",
    "    tot_dmas = sum(layer[\"dmas\"] for layer in model_info[\"layers\"])\n",
    "    rf = model_info[\"layers\"][0][\"rf\"]\n",
    "    print(\n",
    "        f\"{name:<{max_w[0]}} | {tot_params / 1e6:<{max_w[1]}.2f} | {tot_flops / 1e9:<{max_w[2]}.2f} | \"\n",
    "        f\"{tot_macs / 1e9:<{max_w[3]}.2f} | {tot_dmas / 1e9:<{max_w[4]}.2f} | {rf:<{max_w[5]}.0f}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pytest\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_book.scan import crawler\n",
    "\n",
    "\n",
    "def test_apply():\n",
    "    multi_convs = nn.Sequential(nn.Conv2d(16, 32, 3), nn.Conv2d(32, 64, 3))\n",
    "    mod = nn.Sequential(nn.Conv2d(3, 16, 3), multi_convs)\n",
    "\n",
    "    # Tag module attributes\n",
    "    def tag_name(mod, name):\n",
    "        mod.__depth__ = len(name.split(\".\")) - 1\n",
    "        mod.__name__ = name.rpartition(\".\")[-1]\n",
    "\n",
    "    crawler.apply(mod, tag_name)\n",
    "\n",
    "    assert mod[1][1].__depth__ == 2\n",
    "    assert mod[1][1].__name__ == \"1\"\n",
    "\n",
    "\n",
    "def test_crawl_module():\n",
    "\n",
    "    mod = nn.Conv2d(3, 8, 3)\n",
    "\n",
    "    res = crawler.crawl_module(mod, (3, 32, 32))\n",
    "    assert isinstance(res, dict)\n",
    "    assert res[\"overall\"][\"grad_params\"] == 224\n",
    "    assert res[\"layers\"][0][\"output_shape\"] == (-1, 8, 30, 30)\n",
    "\n",
    "\n",
    "def test_summary():\n",
    "\n",
    "    mod = nn.Conv2d(3, 8, 3)\n",
    "\n",
    "    # Redirect stdout with StringIO object\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output\n",
    "    crawler.summary(mod, (3, 32, 32))\n",
    "    # Reset redirect.\n",
    "    sys.stdout = sys.__stdout__\n",
    "    assert captured_output.getvalue().split(\"\\n\")[7] == \"Total params: 224\"\n",
    "\n",
    "    # Check receptive field\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output\n",
    "    crawler.summary(mod, (3, 32, 32), receptive_field=True)\n",
    "    # Reset redirect.\n",
    "    sys.stdout = sys.__stdout__\n",
    "    assert captured_output.getvalue().split(\"\\n\")[1].rpartition(\"  \")[-1] == \"Receptive field\"\n",
    "    assert captured_output.getvalue().split(\"\\n\")[3].split()[-1] == \"3\"\n",
    "    # Check effective stats\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output\n",
    "    crawler.summary(mod, (3, 32, 32), receptive_field=True, effective_rf_stats=True)\n",
    "    # Reset redirect.\n",
    "    sys.stdout = sys.__stdout__\n",
    "    assert captured_output.getvalue().split(\"\\n\")[1].rpartition(\"  \")[-1] == \"Effective padding\"\n",
    "    assert captured_output.getvalue().split(\"\\n\")[3].split()[-1] == \"0\"\n",
    "\n",
    "    # Max depth > model hierarchy\n",
    "    with pytest.raises(ValueError):\n",
    "        crawler.summary(mod, (3, 32, 32), max_depth=1)\n",
    "\n",
    "    mod = nn.Sequential(\n",
    "        OrderedDict(\n",
    "            [\n",
    "                (\"features\", nn.Sequential(nn.Conv2d(3, 8, 3), nn.ReLU(inplace=True))),\n",
    "                (\"pool\", nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(1))),\n",
    "                (\"classifier\", nn.Linear(8, 1)),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    captured_output = io.StringIO()\n",
    "    sys.stdout = captured_output\n",
    "    crawler.summary(mod, (3, 32, 32), max_depth=1)\n",
    "    # Reset redirect.\n",
    "    sys.stdout = sys.__stdout__\n",
    "    assert captured_output.getvalue().split(\"\\n\")[4].startswith(\"├─features \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch_book.scan import process\n",
    "\n",
    "\n",
    "def test_get_process_gpu_ram():\n",
    "\n",
    "    if torch.cuda.is_initialized:\n",
    "        assert process.get_process_gpu_ram(os.getpid()) >= 0\n",
    "    else:\n",
    "        assert process.get_process_gpu_ram(os.getpid()) == 0\n",
    "test_get_process_gpu_ram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "from torch_book.scan import utils\n",
    "\n",
    "\n",
    "def test_format_name():\n",
    "    name = \"mymodule\"\n",
    "    assert utils.format_name(name) == name\n",
    "    assert utils.format_name(name, depth=1) == f\"├─{name}\"\n",
    "    assert utils.format_name(name, depth=3) == f\"|    |    └─{name}\"\n",
    "\n",
    "\n",
    "def test_wrap_string():\n",
    "\n",
    "    example = \".\".join([\"a\" for _ in range(10)])\n",
    "    max_len = 10\n",
    "    wrap = \"[...]\"\n",
    "\n",
    "    assert utils.wrap_string(example, max_len, mode=\"end\") == example[: max_len - len(wrap)] + wrap\n",
    "    assert utils.wrap_string(example, max_len, mode=\"mid\") == f\"{example[:max_len - 2 - len(wrap)]}{wrap}.a\"\n",
    "    assert utils.wrap_string(example, len(example), mode=\"end\") == example\n",
    "    with pytest.raises(ValueError):\n",
    "        _ = utils.wrap_string(example, max_len, mode=\"test\")\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"input_val, num_val, unit\",\n",
    "    [\n",
    "        [3e14, 300, \"T\"],\n",
    "        [3e10, 30, \"G\"],\n",
    "        [3e7, 30, \"M\"],\n",
    "        [15e3, 15, \"k\"],\n",
    "        [500, 500, \"\"],\n",
    "    ],\n",
    ")\n",
    "def test_unit_scale(input_val, num_val, unit):\n",
    "    assert utils.unit_scale(input_val) == (num_val, unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch_book.scan import modules\n",
    "\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "def test_module_flops_warning():\n",
    "    with pytest.warns(UserWarning):\n",
    "        modules.module_flops(MyModule(), None, None)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"mod, input_shape, output_shape, expected_val\",\n",
    "    [\n",
    "        # Check for unknown module that it returns 0 and throws a warning\n",
    "        [MyModule(), (1,), (1,), 0],\n",
    "        # Fully-connected\n",
    "        [nn.Linear(8, 4), (1, 8), (1, 4), 4 * (2 * 8 - 1) + 4],\n",
    "        [nn.Linear(8, 4, bias=False), (1, 8), (1, 4), 4 * (2 * 8 - 1)],\n",
    "        [nn.Linear(8, 4), (1, 2, 8), (1, 2, 4), 2 * (4 * (2 * 8 - 1) + 4)],\n",
    "        # Activations\n",
    "        [nn.Identity(), (1, 8), (1, 8), 0],\n",
    "        [nn.Flatten(), (1, 8), (1, 8), 0],\n",
    "        [nn.ReLU(), (1, 8), (1, 8), 8],\n",
    "        [nn.ELU(), (1, 8), (1, 8), 48],\n",
    "        [nn.LeakyReLU(), (1, 8), (1, 8), 32],\n",
    "        [nn.ReLU6(), (1, 8), (1, 8), 16],\n",
    "        [nn.Tanh(), (1, 8), (1, 8), 48],\n",
    "        [nn.Sigmoid(), (1, 8), (1, 8), 32],\n",
    "        # BN\n",
    "        [nn.BatchNorm1d(8), (1, 8, 4), (1, 8, 4), 144 + 32 + 32 * 3 + 48],\n",
    "        # Pooling\n",
    "        [nn.MaxPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 3 * 32],\n",
    "        [nn.AvgPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 5 * 32],\n",
    "        [nn.AdaptiveMaxPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 3 * 32],\n",
    "        [nn.AdaptiveMaxPool2d(2), (1, 8, 4, 4), (1, 8, 2, 2), 3 * 32],\n",
    "        [nn.AdaptiveAvgPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 5 * 32],\n",
    "        [nn.AdaptiveAvgPool2d(2), (1, 8, 4, 4), (1, 8, 2, 2), 5 * 32],\n",
    "        # Dropout\n",
    "        [nn.Dropout(), (1, 8), (1, 8), 8],\n",
    "        [nn.Dropout(p=0), (1, 8), (1, 8), 0],\n",
    "        # Conv\n",
    "        [nn.Conv2d(3, 8, 3), (1, 3, 32, 32), (1, 8, 30, 30), 388800],\n",
    "        [nn.ConvTranspose2d(3, 8, 3), (1, 3, 32, 32), (1, 8, 34, 34), 499408],\n",
    "    ],\n",
    ")\n",
    "def test_module_flops(mod, input_shape, output_shape, expected_val):\n",
    "    assert modules.module_flops(mod, (torch.zeros(input_shape),), torch.zeros(output_shape)) == expected_val\n",
    "\n",
    "\n",
    "def test_transformer_flops():\n",
    "    mod = nn.Transformer(d_model=64, nhead=4, num_encoder_layers=3)\n",
    "    src = torch.rand((10, 16, 64))\n",
    "    tgt = torch.rand((20, 16, 64))\n",
    "    assert modules.module_flops(mod, (src, tgt), mod(src, tgt)) == 774952841\n",
    "\n",
    "\n",
    "def test_module_macs_warning():\n",
    "    with pytest.warns(UserWarning):\n",
    "        modules.module_macs(MyModule(), None, None)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"mod, input_shape, output_shape, expected_val\",\n",
    "    [\n",
    "        # Check for unknown module that it returns 0 and throws a warning\n",
    "        [MyModule(), (1,), (1,), 0],\n",
    "        # Fully-connected\n",
    "        [nn.Linear(8, 4), (1, 8), (1, 4), 8 * 4],\n",
    "        [nn.Linear(8, 4), (1, 2, 8), (1, 2, 4), 8 * 4 * 2],\n",
    "        # Activations\n",
    "        [nn.ReLU(), (1, 8), (1, 8), 0],\n",
    "        # BN\n",
    "        [nn.BatchNorm1d(8), (1, 8, 4), (1, 8, 4), 64 + 24 + 56 + 32],\n",
    "        # Pooling\n",
    "        [nn.MaxPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 3 * 32],\n",
    "        [nn.AvgPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 5 * 32],\n",
    "        [nn.AdaptiveMaxPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 3 * 32],\n",
    "        [nn.AdaptiveMaxPool2d(2), (1, 8, 4, 4), (1, 8, 2, 2), 3 * 32],\n",
    "        [nn.AdaptiveAvgPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 5 * 32],\n",
    "        [nn.AdaptiveAvgPool2d(2), (1, 8, 4, 4), (1, 8, 2, 2), 5 * 32],\n",
    "        # Dropout\n",
    "        [nn.Dropout(), (1, 8), (1, 8), 0],\n",
    "        # Conv\n",
    "        [nn.Conv2d(3, 8, 3), (1, 3, 32, 32), (1, 8, 30, 30), 194400],\n",
    "        [nn.ConvTranspose2d(3, 8, 3), (1, 3, 32, 32), (1, 8, 34, 34), 249704],\n",
    "    ],\n",
    ")\n",
    "def test_module_macs(mod, input_shape, output_shape, expected_val):\n",
    "\n",
    "    assert modules.module_macs(mod, torch.zeros(input_shape), torch.zeros(output_shape)) == expected_val\n",
    "\n",
    "\n",
    "def test_module_dmas_warning():\n",
    "    with pytest.warns(UserWarning):\n",
    "        modules.module_dmas(MyModule(), None, None)\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"mod, input_shape, output_shape, expected_val\",\n",
    "    [\n",
    "        # Check for unknown module that it returns 0 and throws a warning\n",
    "        [MyModule(), (1,), (1,), 0],\n",
    "        # Fully-connected\n",
    "        [nn.Linear(8, 4), (1, 8), (1, 4), 4 * (8 + 1) + 8 + 4],\n",
    "        [nn.Linear(8, 4), (1, 2, 8), (1, 2, 4), 4 * (8 + 1) + 2 * (8 + 4)],\n",
    "        # Activations\n",
    "        [nn.Identity(), (1, 8), (1, 8), 8],\n",
    "        [nn.Flatten(), (1, 8), (1, 8), 16],\n",
    "        [nn.ReLU(), (1, 8), (1, 8), 8 * 2],\n",
    "        [nn.ReLU(inplace=True), (1, 8), (1, 8), 8],\n",
    "        [nn.ELU(), (1, 8), (1, 8), 17],\n",
    "        [nn.Tanh(), (1, 8), (1, 8), 24],\n",
    "        [nn.Sigmoid(), (1, 8), (1, 8), 16],\n",
    "        # BN\n",
    "        [nn.BatchNorm1d(8), (1, 8, 4), (1, 8, 4), 32 + 17 + 16 + 1 + 17 + 32],\n",
    "        # Pooling\n",
    "        [nn.MaxPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 4 * 32 + 32],\n",
    "        [nn.MaxPool2d(2), (1, 8, 4, 4), (1, 8, 2, 2), 4 * 32 + 32],\n",
    "        [nn.AdaptiveMaxPool2d((2, 2)), (1, 8, 4, 4), (1, 8, 2, 2), 4 * 32 + 32],\n",
    "        [nn.AdaptiveMaxPool2d(2), (1, 8, 4, 4), (1, 8, 2, 2), 4 * 32 + 32],\n",
    "        # Dropout\n",
    "        [nn.Dropout(), (1, 8), (1, 8), 17],\n",
    "        # Conv\n",
    "        [nn.Conv2d(3, 8, 3), (1, 3, 32, 32), (1, 8, 30, 30), 201824],\n",
    "        [nn.ConvTranspose2d(3, 8, 3), (1, 3, 32, 32), (1, 8, 34, 34), 259178],\n",
    "    ],\n",
    ")\n",
    "def test_module_dmas(mod, input_shape, output_shape, expected_val):\n",
    "\n",
    "    assert modules.module_dmas(mod, torch.zeros(input_shape), torch.zeros(output_shape)) == expected_val\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test_module_rf(self):\n",
    "\n",
    "#     # Check for unknown module that it returns 0 and throws a warning\n",
    "#     self.assertEqual(modules.module_rf(MyModule(), None, None), (1, 1, 0))\n",
    "#     self.assertWarns(UserWarning, modules.module_rf, MyModule(), None, None)\n",
    "\n",
    "#     # Common unit tests\n",
    "#     # Linear\n",
    "#     self.assertEqual(modules.module_rf(nn.Linear(8, 4), torch.zeros((1, 8)), torch.zeros((1, 4))),\n",
    "#                      (1, 1, 0))\n",
    "#     # Activation\n",
    "#     self.assertEqual(modules.module_rf(nn.Identity(), torch.zeros((1, 8)), torch.zeros((1, 8))), (1, 1, 0))\n",
    "#     self.assertEqual(modules.module_rf(nn.Flatten(), torch.zeros((1, 8)), torch.zeros((1, 8))), (1, 1, 0))\n",
    "#     self.assertEqual(modules.module_rf(nn.ReLU(), torch.zeros((1, 8)), torch.zeros((1, 8))), (1, 1, 0))\n",
    "#     self.assertEqual(modules.module_rf(nn.ELU(), torch.zeros((1, 8)), torch.zeros((1, 8))), (1, 1, 0))\n",
    "#     self.assertEqual(modules.module_rf(nn.Sigmoid(), torch.zeros((1, 8)), torch.zeros((1, 8))), (1, 1, 0))\n",
    "#     self.assertEqual(modules.module_rf(nn.Tanh(), torch.zeros((1, 8)), torch.zeros((1, 8))), (1, 1, 0))\n",
    "#     # Conv\n",
    "#     input_t = torch.rand((1, 3, 32, 32))\n",
    "#     mod = nn.Conv2d(3, 8, 3)\n",
    "#     self.assertEqual(modules.module_rf(mod, input_t, mod(input_t)), (3, 1, 0))\n",
    "#     # Check for dilation support\n",
    "#     mod = nn.Conv2d(3, 8, 3, dilation=2)\n",
    "#     self.assertEqual(modules.module_rf(mod, input_t, mod(input_t)), (5, 1, 0))\n",
    "#     # ConvTranspose\n",
    "#     mod = nn.ConvTranspose2d(3, 8, 3)\n",
    "#     self.assertEqual(modules.module_rf(mod, input_t, mod(input_t)), (-3, 1, 0))\n",
    "#     # BN\n",
    "#     self.assertEqual(modules.module_rf(nn.BatchNorm1d(8), torch.zeros((1, 8, 4)), torch.zeros((1, 8, 4))),\n",
    "#                      (1, 1, 0))\n",
    "\n",
    "#     # Pooling\n",
    "#     self.assertEqual(modules.module_rf(nn.MaxPool2d((2, 2)),\n",
    "#                                        torch.zeros((1, 8, 4, 4)), torch.zeros((1, 8, 2, 2))),\n",
    "#                      (2, 2, 0))\n",
    "#     self.assertEqual(modules.module_rf(nn.AdaptiveMaxPool2d((2, 2)),\n",
    "#                                        torch.zeros((1, 8, 4, 4)), torch.zeros((1, 8, 2, 2))),\n",
    "#                      (2, 2, 0))\n",
    "\n",
    "#     # Dropout\n",
    "#     self.assertEqual(modules.module_rf(nn.Dropout(), torch.zeros((1, 8)), torch.zeros((1, 8))), (1, 1, 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
