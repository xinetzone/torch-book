{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 基准测试\n",
    "\n",
    "参考：[benchmark](https://pytorch.org/tutorials/recipes/recipes/benchmark.html) & [github](https://github.com/pytorch/benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义同一功能两种不同实现以备后续测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def batched_dot_mul_sum(a, b):\n",
    "    '''Computes batched dot by multiplying and summing'''\n",
    "    return a.mul(b).sum(-1)\n",
    "\n",
    "\n",
    "def batched_dot_bmm(a, b):\n",
    "    '''Computes batched dot by reducing to ``bmm``'''\n",
    "    a = a.reshape(-1, 1, a.shape[-1])\n",
    "    b = b.reshape(-1, b.shape[-1], 1)\n",
    "    return torch.bmm(a, b).flatten(-3)\n",
    "\n",
    "\n",
    "# Input for benchmarking\n",
    "x = torch.randn(10000, 64)\n",
    "\n",
    "# Ensure that both functions compute the same output\n",
    "assert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {mod}`timeit` 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):  106.9 us\n",
      "bmm(x, x):      117.7 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 {class}`torch.utils.benchmark.Timer` 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd2045ebfa0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  471.74 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd34474b580>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  888.97 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import benchmark\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管基本功能方面的 API 相同，但存在一些重要差异。{func}`benchmark.Timer.timeit` 返回每次运行的时间，而不是像 {func}`timeit.Timer.timeit` 那样返回总的运行时间。PyTorch 基准测试模块还为打印结果提供了格式化字符串表示。\n",
    "\n",
    "另一个重要差异，也是结果不同的原因，是 PyTorch 基准测试模块默认在单个线程中运行。我们可以通过 `num_threads` 参数更改线程数。\n",
    "\n",
    "{class}`torch.utils.benchmark.Timer` 还接受几个额外的参数，包括：`label`、`sub_label`、`description` 和 `env`，它们会改变返回的测量对象的 `__repr__`，并用于对结果进行分组（稍后会详细介绍）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 24 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd2045eaf80>\n",
      "Multithreaded batch dot: Implemented using mul and sum\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  102.00 us\n",
      "  1 measurement, 100 runs , 24 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd2045eb2b0>\n",
      "Multithreaded batch dot: Implemented using bmm\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  101.13 us\n",
      "  1 measurement, 100 runs , 24 threads\n"
     ]
    }
   ],
   "source": [
    "num_threads = torch.get_num_threads()\n",
    "print(f'Benchmarking on {num_threads} threads')\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x},\n",
    "    num_threads=num_threads,\n",
    "    label='Multithreaded batch dot',\n",
    "    sub_label='Implemented using mul and sum')\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x},\n",
    "    num_threads=num_threads,\n",
    "    label='Multithreaded batch dot',\n",
    "    sub_label='Implemented using bmm')\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用所有可用线程运行基准测试与 `timeit` 模块的结果相似。更重要的是，哪个版本更快取决于我们用多少个线程运行代码。这就是为什么使用代表实际用例的线程设置对代码进行基准测试很重要的原因。另一个要记住的重要事情是在 GPU 上进行基准测试时要同步 CPU 和 CUDA。让我们再次在 CUDA 张量上运行上述基准测试，看看会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):  634.3 us\n",
      "mul_sum(x, x):   35.7 us\n",
      "bmm(x, x):      2649.5 us\n",
      "bmm(x, x):       37.3 us\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10000, 1024, device='cuda')\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "# Ran each twice to show difference before/after warm-up\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd34db777c0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  150.93 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd34c0c2650>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  50.30 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "# Run only once since benchmark module does warm-up for us\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果揭示了一些有趣的事情。使用 `timeit` 模块的 `bmm` 版本第一次运行比第二次运行花费更长的时间。这是因为 `bmm` 调用 cuBLAS，而 cuBLAS 需要在第一次调用时加载，这需要一些时间。这就是为什么在基准测试之前进行预热运行很重要的原因，幸运的是，PyTorch 的基准测试模块可以处理这个问题。\n",
    "\n",
    "`timeit` 和 `benchmark` 模块之间的结果差异是因为 `timeit` 模块没有同步 CUDA，因此只计时内核启动的时间。PyTorch 的基准测试模块为我们执行了同步操作。\n",
    "\n",
    "## 使用Blocked Autorange 进行基准测试\n",
    "\n",
    "{func}`timeit.Timer.autorange` 至少进行 0.2 秒的连续测量，而 {func}`torch.utils.benchmark.blocked_autorange` 进行多次测量，其总时间至少为 0.2 秒（可以通过 `min_run_time` 参数进行更改），但受到计时开销占整体测量一小部分的限制。这是通过首先以不断增加的循环次数运行来实现的，直到运行时间远大于测量开销（这也作为预热），然后进行测量，直到达到目标时间。这种方法具有有用的特性，即浪费较少的数据，并允许我们计算统计数据来估计测量的可靠性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd34c0c1db0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  149.64 us\n",
      "  1 measurement, 10000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd34c0c1ba0>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  49.85 us\n",
      "  1 measurement, 10000 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "m0 = t0.blocked_autorange()\n",
    "m1 = t1.blocked_autorange()\n",
    "\n",
    "print(m0)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以检查从返回的测量对象中获取的单个统计数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:   149.64 us\n",
      "Median: 149.64 us\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean:   {m0.mean * 1e6:6.2f} us\")\n",
    "print(f\"Median: {m0.median * 1e6:6.2f} us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较基准测试结果\n",
    "到目前为止，我们一直在将两个版本的批量点积与单个输入进行比较。在实践中，我们还希望尝试不同数量的输入和线程的组合。`Compare` 类可以帮助以格式化的表格形式显示许多测量结果。它使用上述注释（标签、子标签、线程数等）以及描述来对表格进行分组和组织。让我们使用 `Compare` 来看看我们的函数在不同输入大小和线程数下的性能如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------- Batched dot ----------------]\n",
      "                      |  mul/sum   |    bmm   \n",
      "1 threads: -----------------------------------\n",
      "      [1, 1]          |       5.8  |       9.2\n",
      "      [1, 64]         |       6.2  |       9.6\n",
      "      [1, 1024]       |       6.5  |      10.4\n",
      "      [1, 10000]      |       9.2  |      11.7\n",
      "      [64, 1]         |       6.4  |       9.6\n",
      "      [64, 64]        |       8.2  |      13.8\n",
      "      [64, 1024]      |      36.5  |     229.7\n",
      "      [64, 10000]     |     289.0  |    2114.2\n",
      "      [1024, 1]       |       7.3  |      15.2\n",
      "      [1024, 64]      |      51.8  |      90.8\n",
      "      [1024, 1024]    |     461.1  |    3465.2\n",
      "      [1024, 10000]   |   27525.8  |   33693.0\n",
      "      [10000, 1]      |      24.7  |      73.8\n",
      "      [10000, 64]     |     327.4  |     684.2\n",
      "      [10000, 1024]   |   28438.8  |   35480.5\n",
      "      [10000, 10000]  |  288529.2  |  378751.1\n",
      "4 threads: -----------------------------------\n",
      "      [1, 1]          |       5.8  |       9.6\n",
      "      [1, 64]         |       6.3  |       9.3\n",
      "      [1, 1024]       |       6.4  |      10.5\n",
      "      [1, 10000]      |       9.3  |      11.9\n",
      "      [64, 1]         |       6.4  |       9.6\n",
      "      [64, 64]        |       8.3  |      20.3\n",
      "      [64, 1024]      |      48.1  |     307.2\n",
      "      [64, 10000]     |      90.0  |    3706.0\n",
      "      [1024, 1]       |      11.2  |      21.0\n",
      "      [1024, 64]      |      49.4  |      60.9\n",
      "      [1024, 1024]    |     139.1  |     944.6\n",
      "      [1024, 10000]   |   10918.2  |    8791.5\n",
      "      [10000, 1]      |      24.1  |      67.9\n",
      "      [10000, 64]     |     111.4  |     196.7\n",
      "      [10000, 1024]   |   10966.8  |    9261.8\n",
      "      [10000, 10000]  |  106744.5  |   91647.6\n",
      "16 threads: ----------------------------------\n",
      "      [1, 1]          |       8.1  |      11.9\n",
      "      [1, 64]         |       6.3  |      10.3\n",
      "      [1, 1024]       |      10.1  |      17.2\n",
      "      [1, 10000]      |       9.9  |      12.3\n",
      "      [64, 1]         |       6.4  |       9.6\n",
      "      [64, 64]        |       8.4  |      13.9\n",
      "      [64, 1024]      |      42.2  |     461.2\n",
      "      [64, 10000]     |      65.5  |    4675.4\n",
      "      [1024, 1]       |       7.6  |      15.3\n",
      "      [1024, 64]      |      51.3  |      58.8\n",
      "      [1024, 1024]    |      68.5  |     308.4\n",
      "      [1024, 10000]   |    9078.2  |    2671.1\n",
      "      [10000, 1]      |      18.4  |      66.3\n",
      "      [10000, 64]     |      73.4  |     104.2\n",
      "      [10000, 1024]   |    8494.2  |    2594.6\n",
      "      [10000, 10000]  |   73330.4  |   24553.6\n",
      "32 threads: ----------------------------------\n",
      "      [1, 1]          |       5.8  |       9.5\n",
      "      [1, 64]         |       8.7  |       9.5\n",
      "      [1, 1024]       |      10.4  |      11.1\n",
      "      [1, 10000]      |       9.8  |      11.8\n",
      "      [64, 1]         |       6.3  |       9.6\n",
      "      [64, 64]        |       8.4  |      14.1\n",
      "      [64, 1024]      |      59.2  |     706.2\n",
      "      [64, 10000]     |     124.4  |    7149.6\n",
      "      [1024, 1]       |       8.0  |      15.4\n",
      "      [1024, 64]      |      58.5  |      80.3\n",
      "      [1024, 1024]    |      73.5  |     211.3\n",
      "      [1024, 10000]   |    8362.5  |    1796.2\n",
      "      [10000, 1]      |      18.2  |      67.6\n",
      "      [10000, 64]     |     142.1  |     101.5\n",
      "      [10000, 1024]   |    7403.5  |    1360.9\n",
      "      [10000, 10000]  |   73554.7  |   15836.1\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Compare takes a list of measurements which we'll save in results.\n",
    "results = []\n",
    "\n",
    "sizes = [1, 64, 1024, 10000]\n",
    "for b, n in product(sizes, sizes):\n",
    "    # label and sub_label are the rows\n",
    "    # description is the column\n",
    "    label = 'Batched dot'\n",
    "    sub_label = f'[{b}, {n}]'\n",
    "    x = torch.ones((b, n))\n",
    "    for num_threads in [1, 4, 16, 32]:\n",
    "        results.append(benchmark.Timer(\n",
    "            stmt='batched_dot_mul_sum(x, x)',\n",
    "            setup='from __main__ import batched_dot_mul_sum',\n",
    "            globals={'x': x},\n",
    "            num_threads=num_threads,\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description='mul/sum',\n",
    "        ).blocked_autorange(min_run_time=1))\n",
    "        results.append(benchmark.Timer(\n",
    "            stmt='batched_dot_bmm(x, x)',\n",
    "            setup='from __main__ import batched_dot_bmm',\n",
    "            globals={'x': x},\n",
    "            num_threads=num_threads,\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description='bmm',\n",
    "        ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述结果表明，对于在多个线程上运行的大型张量，可以简化为 `bmm` 的版本更好，而对于较小和/或单线程代码，另一个版本更好。\n",
    "\n",
    "`Compare` 还提供了用于更改表格格式的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------- Batched dot --------------]\n",
      "                      |  mul/sum  |   bmm  \n",
      "1 threads: --------------------------------\n",
      "      [1, 1]          |  \u001b[92m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 64]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |        6  |      10\n",
      "      [1, 10000]      |        9  |      10\n",
      "      [64, 1]         |        6  |  \u001b[34m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [64, 64]        |        8  |      14\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     36\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   230\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m    289\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2100\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        7  |      15\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     52\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    91\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m    461\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  3500\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m  28000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 34000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     25\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    74\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m    327\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   680\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m  28400\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 40000\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m 300000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m400000\u001b[0m\u001b[0m\n",
      "4 threads: --------------------------------\n",
      "      [1, 1]          |  \u001b[92m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [1, 64]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |        6  |      10\n",
      "      [1, 10000]      |        9  |      10\n",
      "      [64, 1]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [64, 64]        |        8  |  \u001b[2m\u001b[91m    20\u001b[0m\u001b[0m\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     50\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   300\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m     90\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  4000\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |       11  |  \u001b[2m\u001b[91m    21\u001b[0m\u001b[0m\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     49\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    60\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m    140\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   940\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m  10920\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  9000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     24\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    70\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m    111\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   197\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m  10970\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  9260\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m 107000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 91600\u001b[0m\u001b[0m\n",
      "16 threads: -------------------------------\n",
      "      [1, 1]          |        8  |      10\n",
      "      [1, 64]         |  \u001b[92m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |       10  |      20\n",
      "      [1, 10000]      |       10  |      10\n",
      "      [64, 1]         |  \u001b[92m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [64, 64]        |        8  |      14\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     40\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   500\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m     66\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  5000\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        8  |      15\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     50\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    60\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m     70\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   308\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m   9100\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  3000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     18\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    66\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m     73\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   100\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m   8000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  3000\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m  70000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 25000\u001b[0m\u001b[0m\n",
      "32 threads: -------------------------------\n",
      "      [1, 1]          |  \u001b[92m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [1, 64]         |        9  |  \u001b[92m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |       10  |      10\n",
      "      [1, 10000]      |       10  |      10\n",
      "      [64, 1]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [64, 64]        |        8  |      14\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     60\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   700\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m    124\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  7000\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        8  |      15\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     58\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    80\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m     74\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   210\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m   8400\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     18\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    70\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m    140\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   101\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m   7000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  1000\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m  74000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 16000\u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存/加载基准测试结果\n",
    "测量结果可以通过 `pickle` 模块进行序列化。这使得A/B测试变得简单，因为您可以从两个单独的环境中收集测量结果，将其 `pickle` 化，然后在单个环境中加载它们。`Timer` 甚至接受 `env` 构造函数参数，以便这种 A/B 测试可以无缝地工作。\n",
    "\n",
    "让我们想象一下，不是使用两个 Python 函数，而是将 `add/sum` 和 `bmm` 方法分别添加到 PyTorch 的两个不同构建中。下面的示例演示了如何对它们进行 A/B 测试。为了简单起见，我们只使用形状的子集，并简单地通过 pickle 来回传递结果，而不是实际使用多个环境并将结果写入磁盘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Batched dot -------------------------------------]\n",
      "                                               |  [1, 1]  |  [1024, 10000]  |  [10000, 1]\n",
      "1 threads: ------------------------------------------------------------------------------\n",
      "  (environment A: mul/sum)  batched_dot(x, x)  |  \u001b[92m\u001b[1m   6  \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    29000    \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    20    \u001b[0m\u001b[0m\n",
      "  (environment B: bmm)      batched_dot(x, x)  |  \u001b[2m\u001b[91m  15  \u001b[0m\u001b[0m  |      30000      |  \u001b[2m\u001b[91m    73    \u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "ab_test_results = []\n",
    "for env in ('environment A: mul/sum', 'environment B: bmm'):\n",
    "    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n",
    "        x = torch.ones((b, n))\n",
    "        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n",
    "        m = benchmark.Timer(\n",
    "            stmt='batched_dot(x, x)',\n",
    "            globals={'x': x, 'batched_dot': dot_fn},\n",
    "            num_threads=1,\n",
    "            label='Batched dot',\n",
    "            description=f'[{b}, {n}]',\n",
    "            env=env,\n",
    "        ).blocked_autorange(min_run_time=1)\n",
    "        ab_test_results.append(pickle.dumps(m))\n",
    "\n",
    "ab_results = [pickle.loads(i) for i in ab_test_results]\n",
    "compare = benchmark.Compare(ab_results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And just to show that we can round trip all of the results from earlier:\n",
    "round_tripped_results = pickle.loads(pickle.dumps(results))\n",
    "assert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成带有模糊参数的输入\n",
    "\n",
    "正如我们在上一节中所看到的，根据输入张量的不同，可能会产生一些明显的性能差异。因此，在多个不同的输入上运行基准测试是一个好主意。然而，创建所有这些输入张量可能会很繁琐，这就是 `torch.utils.benchmark.Fuzzer` 和相关类的作用所在。让我们看看如何使用 `Fuzzer` 为基准测试创建一些测试用例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------- Batched dot ---------------------]\n",
      "                                     |  mul/sum  |   bmm \n",
      "1 threads: ----------------------------------------------\n",
      "      725    x 257                   |     102   |    199\n",
      "      49     x 383                   |      21   |     36\n",
      "      34     x 1468                  |      40   |    190\n",
      "      187    x 5039                  |     430   |   3100\n",
      "      2140   x 1296 (discontiguous)  |    1820   |  82000\n",
      "      78     x 1598                  |      63   |    420\n",
      "      519    x 763                   |     182   |   1320\n",
      "      141    x 1082                  |      76   |    510\n",
      "      78     x 5    (discontiguous)  |       7   |     13\n",
      "      187    x 1                     |       7   |     11\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n",
    "\n",
    "# Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a\n",
    "# ``loguniform`` distribution in [1, 10000], 40% of which will be discontiguous on average.\n",
    "example_fuzzer = Fuzzer(\n",
    "    parameters = [\n",
    "        FuzzedParameter('k0', minval=1, maxval=10000, distribution='loguniform'),\n",
    "        FuzzedParameter('k1', minval=1, maxval=10000, distribution='loguniform'),\n",
    "    ],\n",
    "    tensors = [\n",
    "        FuzzedTensor('x', size=('k0', 'k1'), min_elements=128, max_elements=10000000, probability_contiguous=0.6)\n",
    "    ],\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for tensors, tensor_params, params in example_fuzzer.take(10):\n",
    "    # description is the column label\n",
    "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_mul_sum(x, x)',\n",
    "        setup='from __main__ import batched_dot_mul_sum',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='mul/sum',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_bmm(x, x)',\n",
    "        setup='from __main__ import batched_dot_bmm',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='bmm',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义自己的模糊器具有很大的灵活性，这对于创建一组强大的输入来进行基准测试非常有用。但是为了让事情变得更简单，PyTorch基准测试模块附带了一些内置的模糊器，以满足常见的基准测试需求。让我们看看如何使用其中一个内置的模糊器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------------------- Batched dot ------------------------]\n",
      "                                         |  mul/sum  |   bmm  \n",
      "1 threads: ---------------------------------------------------\n",
      "      64     x 473  (discontiguous)      |  \u001b[92m\u001b[1m  13900\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m101000\u001b[0m\u001b[0m\n",
      "      16384  x 12642115 (discontiguous)  |  \u001b[92m\u001b[1m     32\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m   122\u001b[0m\u001b[0m\n",
      "      8192   x 892                       |  \u001b[92m\u001b[1m   6930\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 24000\u001b[0m\u001b[0m\n",
      "      512    x 64   (discontiguous)      |  \u001b[92m\u001b[1m 104000\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m369000\u001b[0m\u001b[0m\n",
      "      493    x 27   (discontiguous)      |  \u001b[92m\u001b[1m   2020\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  4500\u001b[0m\u001b[0m\n",
      "      118    x 32   (discontiguous)      |  \u001b[92m\u001b[1m    903\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  2650\u001b[0m\u001b[0m\n",
      "      16     x 495  (discontiguous)      |  \u001b[92m\u001b[1m  20000\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 35000\u001b[0m\u001b[0m\n",
      "      488    x 62374                     |  \u001b[92m\u001b[1m  84300\u001b[0m\u001b[0m  |   98700\n",
      "      240372 x 69                        |  \u001b[2m\u001b[91m  48000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 17900\u001b[0m\u001b[0m\n",
      "      40156  x 32   (discontiguous)      |  \u001b[92m\u001b[1m   1890\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  5100\u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark.op_fuzzers import binary\n",
    "\n",
    "results = []\n",
    "for tensors, tensor_params, params in binary.BinaryOpFuzzer(seed=0).take(10):\n",
    "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_mul_sum(x, x)',\n",
    "        setup='from __main__ import batched_dot_mul_sum',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='mul/sum',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_bmm(x, x)',\n",
    "        setup='from __main__ import batched_dot_bmm',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='bmm',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize(rowwise=True)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 收集带有Callgrind的指令计数\n",
    "\n",
    "优化代码的一个挑战是墙钟的时间变化和不透明度。有很多非确定性来源，从自适应时钟速度到与其他进程的资源竞争。此外，端到端时间无法提供关于在哪里花费时间的洞察，这是我们在优化代码时真正感兴趣的。\n",
    "\n",
    "补充方法是同时收集指令计数。这些计数是代理指标，并不捕获所有性能方面（例如内存或I/O绑定任务），但它们确实具有一些有用的属性。指令计数是可重复的，对环境变化不敏感，并提供了程序在何处花费周期的细粒度洞察。\n",
    "\n",
    "要查看指令计数的效用，让我们看看如何减少 `batched_dot_mul_sum` 的开销。显而易见的解决方案是将它移动到C++，这样我们就可以避免在Python和C++之间多次往返。\n",
    "\n",
    "幸运的是，源代码几乎相同。我们在 C++ 中必须问一个问题：我们应该通过值还是引用传递参数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd1dbbe9b10>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup:\n",
      "  from __main__ import batched_dot_mul_sum\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  8.92 us\n",
      "  1 measurement, 100000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd1dbbea680>\n",
      "cpp_lib.batched_dot_mul_sum_v0(x, x)\n",
      "setup:\n",
      "  import cpp_lib\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  7.33 us\n",
      "  1 measurement, 100000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fd34474b760>\n",
      "cpp_lib.batched_dot_mul_sum_v1(x, x)\n",
      "setup:\n",
      "  import cpp_lib\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  7.51 us\n",
      "  1 measurement, 100000 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "batched_dot_src = \"\"\"\\\n",
    "/* ---- Python ---- */\n",
    "// def batched_dot_mul_sum(a, b):\n",
    "//     return a.mul(b).sum(-1)\n",
    "\n",
    "torch::Tensor batched_dot_mul_sum_v0(\n",
    "    const torch::Tensor a,\n",
    "    const torch::Tensor b) {\n",
    "  return a.mul(b).sum(-1);\n",
    "}\n",
    "\n",
    "torch::Tensor batched_dot_mul_sum_v1(\n",
    "    const torch::Tensor& a,\n",
    "    const torch::Tensor& b) {\n",
    "  return a.mul(b).sum(-1);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# PyTorch makes it easy to test our C++ implementations by providing a utility\n",
    "# to JIT compile C++ source into Python extensions:\n",
    "import os\n",
    "from torch.utils import cpp_extension\n",
    "cpp_lib = cpp_extension.load_inline(\n",
    "    name='cpp_lib',\n",
    "    cpp_sources=batched_dot_src,\n",
    "    extra_cflags=['-O3'],\n",
    "    extra_include_paths=[\n",
    "        # `load_inline` needs to know where to find ``pybind11`` headers.\n",
    "        os.path.join(os.getenv('CONDA_PREFIX'), 'include')\n",
    "    ],\n",
    "    functions=['batched_dot_mul_sum_v0', 'batched_dot_mul_sum_v1']\n",
    ")\n",
    "\n",
    "# `load_inline` will create a shared object that is loaded into Python. When we collect\n",
    "# instruction counts Timer will create a subprocess, so we need to re-import it. The\n",
    "# import process is slightly more complicated for C extensions, but that's all we're\n",
    "# doing here.\n",
    "module_import_str = f\"\"\"\\\n",
    "# https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"cpp_lib\", {repr(cpp_lib.__file__)})\n",
    "cpp_lib = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cpp_lib)\"\"\"\n",
    "\n",
    "import textwrap\n",
    "def pretty_print(result):\n",
    "    \"\"\"Import machinery for ``cpp_lib.so`` can get repetitive to look at.\"\"\"\n",
    "    print(repr(result).replace(textwrap.indent(module_import_str, \"  \"), \"  import cpp_lib\"))\n",
    "\n",
    "\n",
    "t_baseline = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='''\\\n",
    "from __main__ import batched_dot_mul_sum\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='cpp_lib.batched_dot_mul_sum_v0(x, x)',\n",
    "    setup=f'''\\\n",
    "{module_import_str}\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='cpp_lib.batched_dot_mul_sum_v1(x, x)',\n",
    "    setup=f'''\\\n",
    "{module_import_str}\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "# Moving to C++ did indeed reduce overhead, but it's hard to tell which\n",
    "# calling convention is more efficient. v1 (call with references) seems to\n",
    "# be a bit faster, but it's within measurement error.\n",
    "pretty_print(t_baseline.blocked_autorange())\n",
    "pretty_print(t0.blocked_autorange())\n",
    "pretty_print(t1.blocked_autorange())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Missing: valgrind, callgrind_control, callgrind_annotate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/media/pc/data/lxw/ai/torch-book/doc/recipes/benchmark.ipynb 单元格 30\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/lxw/ai/torch-book/doc/recipes/benchmark.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Let's use ``Callgrind`` to determine which is better.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/lxw/ai/torch-book/doc/recipes/benchmark.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m stats_v0 \u001b[39m=\u001b[39m t0\u001b[39m.\u001b[39;49mcollect_callgrind()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/lxw/ai/torch-book/doc/recipes/benchmark.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m stats_v1 \u001b[39m=\u001b[39m t1\u001b[39m.\u001b[39mcollect_callgrind()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/lxw/ai/torch-book/doc/recipes/benchmark.ipynb#X60sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m pretty_print(stats_v0)\n",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:486\u001b[0m, in \u001b[0;36mTimer.collect_callgrind\u001b[0;34m(self, number, repeats, collect_baseline, retain_out_file)\u001b[0m\n\u001b[1;32m    484\u001b[0m is_python \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_language \u001b[39m==\u001b[39m Language\u001b[39m.\u001b[39mPYTHON)\n\u001b[1;32m    485\u001b[0m \u001b[39massert\u001b[39;00m is_python \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globals\n\u001b[0;32m--> 486\u001b[0m result \u001b[39m=\u001b[39m valgrind_timer_interface\u001b[39m.\u001b[39;49mwrapper_singleton()\u001b[39m.\u001b[39;49mcollect_callgrind(\n\u001b[1;32m    487\u001b[0m     task_spec\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_task_spec,\n\u001b[1;32m    488\u001b[0m     \u001b[39mglobals\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_globals,\n\u001b[1;32m    489\u001b[0m     number\u001b[39m=\u001b[39;49mnumber,\n\u001b[1;32m    490\u001b[0m     repeats\u001b[39m=\u001b[39;49mrepeats \u001b[39mor\u001b[39;49;00m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m    491\u001b[0m     collect_baseline\u001b[39m=\u001b[39;49mcollect_baseline \u001b[39mand\u001b[39;49;00m is_python,\n\u001b[1;32m    492\u001b[0m     is_python\u001b[39m=\u001b[39;49mis_python,\n\u001b[1;32m    493\u001b[0m     retain_out_file\u001b[39m=\u001b[39;49mretain_out_file,\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    496\u001b[0m \u001b[39mreturn\u001b[39;00m (result[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m repeats \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m result)\n",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py:527\u001b[0m, in \u001b[0;36m_ValgrindWrapper.collect_callgrind\u001b[0;34m(self, task_spec, globals, number, repeats, collect_baseline, is_python, retain_out_file)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect_callgrind\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     task_spec: common\u001b[39m.\u001b[39mTaskSpec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retain_out_file: \u001b[39mbool\u001b[39m,\n\u001b[1;32m    525\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[CallgrindStats, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[1;32m    526\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Collect stats, and attach a reference run which can be used to filter interpreter overhead.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate()\n\u001b[1;32m    528\u001b[0m     \u001b[39massert\u001b[39;00m is_python \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m collect_baseline\n\u001b[1;32m    530\u001b[0m     \u001b[39m*\u001b[39mtask_stats, baseline_stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invoke(\n\u001b[1;32m    531\u001b[0m         task_spec\u001b[39m=\u001b[39mtask_spec,\n\u001b[1;32m    532\u001b[0m         \u001b[39mglobals\u001b[39m\u001b[39m=\u001b[39m\u001b[39mglobals\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m         retain_out_file\u001b[39m=\u001b[39mretain_out_file,\n\u001b[1;32m    538\u001b[0m     )\n",
      "File \u001b[0;32m/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py:513\u001b[0m, in \u001b[0;36m_ValgrindWrapper._validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    511\u001b[0m missing_cmds \u001b[39m=\u001b[39m [cmd \u001b[39mfor\u001b[39;00m cmd, available \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_commands_available\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available]\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m missing_cmds:\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMissing: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(missing_cmds))\n",
      "\u001b[0;31mOSError\u001b[0m: Missing: valgrind, callgrind_control, callgrind_annotate"
     ]
    }
   ],
   "source": [
    "# Let's use ``Callgrind`` to determine which is better.\n",
    "stats_v0 = t0.collect_callgrind()\n",
    "stats_v1 = t1.collect_callgrind()\n",
    "\n",
    "pretty_print(stats_v0)\n",
    "pretty_print(stats_v1)\n",
    "\n",
    "# `.as_standardized` removes file names and some path prefixes, and makes\n",
    "# it easier to read the function symbols.\n",
    "stats_v0 = stats_v0.as_standardized()\n",
    "stats_v1 = stats_v1.as_standardized()\n",
    "\n",
    "# `.delta` diffs the instruction counts, and `.denoise` removes several\n",
    "# functions in the Python interpreter that are known to have significant\n",
    "# jitter.\n",
    "delta = stats_v1.delta(stats_v0).denoise()\n",
    "\n",
    "# `.transform` is a convenience API for transforming function names. It is\n",
    "# useful for increasing cancelation when ``diff-ing`` instructions, as well as\n",
    "# just generally improving readability.\n",
    "replacements = (\n",
    "    (\"???:void pybind11\", \"pybind11\"),\n",
    "    (\"batched_dot_mul_sum_v0\", \"batched_dot_mul_sum_v1\"),\n",
    "    (\"at::Tensor, at::Tensor\", \"...\"),\n",
    "    (\"at::Tensor const&, at::Tensor const&\", \"...\"),\n",
    "    (\"auto torch::detail::wrap_pybind_function_impl_\", \"wrap_pybind_function_impl_\"),\n",
    ")\n",
    "for before, after in replacements:\n",
    "    delta = delta.transform(lambda l: l.replace(before, after))\n",
    "\n",
    "# We can use print options to control how much of the function to display.\n",
    "torch.set_printoptions(linewidth=160)\n",
    "\n",
    "# Once parsed, the instruction counts make clear that passing `a` and `b`\n",
    "# by reference is more efficient as it skips some ``c10::TensorImpl`` bookkeeping\n",
    "# for the intermediate Tensors, and is also works better with ``pybind11``. This\n",
    "# is consistent with our noisy wall time observations.\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
