{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLOPs\n",
    "\n",
    "参考：[FLOPs](https://zhuanlan.zhihu.com/p/663566912?utm_psn=1701033625972346880)\n",
    "\n",
    "FLOPs（Floating Point Operations，浮点运算数）和 MACs（Multiply-Accumulate Operations，乘加运算数）是常用于计算深度学习模型计算复杂度的指标。它们是快速、简单地了解执行给定计算所需的算术运算数量的方法。例如，在为边缘设备使用不同的模型架构（如 MobileNet 或 DenseNet）时，人们使用 MACs 或 FLOPs 来估计模型性能。同时，使用“估计”这个词的原因是，这两个指标都是近似值，而不是实际运行时性能模型的捕获。然而，它们仍然可以提供有关能量消耗或计算要求的非常有用的洞察，这在边缘计算中非常有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLOPs 特指对浮点数进行的加法、减法、乘法和除法等浮点运算的数量。这些运算在机器学习中涉及的许多数学计算中非常常见，例如矩阵乘法、激活函数和梯度计算。FLOPs 通常用于衡量模型或模型内特定操作的计算成本或复杂度。当需要提供所需算术运算总数的估计时，这非常有用，通常用于衡量计算效率的上下文中。\n",
    "\n",
    "另一方面，MACs 只计算乘加操作的数量，这涉及将两个数字相乘并相加结果。这种运算是许多线性代数操作的基础，例如矩阵乘法、卷积和点积。在严重依赖线性代数运算的模型中，如卷积神经网络（CNN），MACs 通常用作计算复杂度的更具体度量。\n",
    "\n",
    "```{note}\n",
    "全大写的 FLOPS 是“每秒浮点运算数”的缩写，指的是计算速度，通常用作硬件性能的度量。FLOPS 中的“S”表示“秒”，与“P”（作为“每”）一起，通常用于表示比率。\n",
    "```\n",
    "\n",
    "\n",
    "一般AI社区的共识是，一个 MAC 大约等于两个 FLOP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # 忽略用户警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Any, Sequence\n",
    "# import traceback\n",
    "# import torch\n",
    "# from torch import nn, Tensor\n",
    "# from torch.fx.passes.shape_prop import ShapeProp\n",
    "# from torch.fx.node import Argument, Node, Target, map_aggregate\n",
    "# from torch.fx._compatibility import compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_book.scan.common import FLOPsABC\n",
    "from torch_book.scan.flop import ElementwiseFLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Number\n",
    "from torch.fx._compatibility import compatibility\n",
    "from torch_book.scan.common import FLOPsABC\n",
    "\n",
    "\n",
    "class ElementwiseFLOPs(FLOPsABC):\n",
    "    @compatibility(is_backward_compatible=True)\n",
    "    def fetch_method_flops(self, self_obj: Tensor, result: Tensor, *args_tail, **kwargs):\n",
    "        \"\"\"计算方法的FLOPs\"\"\"\n",
    "        return np.prod(result.shape)\n",
    "\n",
    "    @compatibility(is_backward_compatible=True)\n",
    "    def fetch_function_flops(self, result: Tensor|Number, *args, **kwargs) -> Any:\n",
    "        \"\"\"计算函数的FLOPs\"\"\"\n",
    "        assert len(args) == 2, len(args)\n",
    "        total_flops = None\n",
    "        if isinstance(result, Number):\n",
    "            total_flops = 1\n",
    "        elif isinstance(result, Tensor):\n",
    "            total_flops = np.prod(result.shape)\n",
    "        else:\n",
    "            raise TypeError(type(result))\n",
    "        return total_flops\n",
    "\n",
    "    @compatibility(is_backward_compatible=True)\n",
    "    def fetch_module_flops(self, module: nn.Module, result: Tensor, *args, **kwargs) -> Any:\n",
    "        \"\"\"计算模块的FLOPs\"\"\"\n",
    "        assert len(args) == 1\n",
    "        assert isinstance(args[0], Tensor)\n",
    "        assert isinstance(result, Tensor)\n",
    "        input_shape = args[0].shape  # [..., d_in]\n",
    "        result_shape = result.shape\n",
    "        assert input_shape == result_shape\n",
    "        total_flops = np.prod(result_shape)\n",
    "        return total_flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Demo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.layer = nn.Linear(5, 4, bias=True)\n",
    "        self.layer1 = nn.ReLU()\n",
    "        self.layer2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# model = nn.ReLU()\n",
    "model = Demo()\n",
    "gm = torch.fx.symbolic_trace(model)\n",
    "sample_input = torch.randn(1, 5, 32, 32)\n",
    "ElementwiseFLOPs(gm).propagate(sample_input);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_book.scan.show_flop import show_flops_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════╤═════════════╤═════════════╤═══════════════════════╤═════════╕\n",
      "│ node_name   │ node_op     │ op_target   │ nn_module_stack[-1]   │   FLOPs │\n",
      "╞═════════════╪═════════════╪═════════════╪═══════════════════════╪═════════╡\n",
      "│ x           │ placeholder │ x           │                       │       0 │\n",
      "├─────────────┼─────────────┼─────────────┼───────────────────────┼─────────┤\n",
      "│ layer1      │ call_module │ layer1      │ ReLU                  │    5120 │\n",
      "├─────────────┼─────────────┼─────────────┼───────────────────────┼─────────┤\n",
      "│ layer2      │ call_module │ layer2      │ Sigmoid               │    5120 │\n",
      "├─────────────┼─────────────┼─────────────┼───────────────────────┼─────────┤\n",
      "│ output      │ output      │ output      │                       │       0 │\n",
      "╘═════════════╧═════════════╧═════════════╧═══════════════════════╧═════════╛\n",
      "total_flops = 10,240\n"
     ]
    }
   ],
   "source": [
    "show_flops_table(gm, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result_tablew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult_tablew\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_tablew' is not defined"
     ]
    }
   ],
   "source": [
    "result_tablew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor, Size\n",
    "from torch.types import Number\n",
    "\n",
    "def flops_zero() -> int:\n",
    "    return 0\n",
    "\n",
    "def flops_elemwise(result_shape: Size) -> int:\n",
    "    return result_shape.numel()\n",
    "\n",
    "def flops_matmul(tensor1_shape: Size, tensor2_shape: Size, result_shape: Size) -> int:\n",
    "    # 可根据输入维度改为分情况处理，参考https://github.com/zhijian-liu/torchprofile/blob/6d80fe57bb8c6bc9f789da7925fac6547fa9502b/torchprofile/handlers.py#L35\n",
    "    def get_reduce_dim_shape(_s: Size, is_first_mat: bool):\n",
    "        return _s[0] if len(_s) == 1 else _s[-1 if is_first_mat else -2]\n",
    "    reduce_dim_shape = get_reduce_dim_shape(tensor1_shape, True)\n",
    "    assert reduce_dim_shape == get_reduce_dim_shape(tensor2_shape, False)\n",
    "    return (2 * reduce_dim_shape - 1) * result_shape.numel()\n",
    "\n",
    "class LinearFLOPs(FLOPsABC):\n",
    "    @compatibility(is_backward_compatible=True)\n",
    "    def fetch_method_flops(self, self_obj: Any, result: Tensor, *args_tail, **kwargs):\n",
    "        \"\"\"计算方法的FLOPs\"\"\"\n",
    "        ...\n",
    "\n",
    "    @compatibility(is_backward_compatible=True)\n",
    "    def fetch_function_flops(self, result: Tensor, *args, **kwargs) -> Any:\n",
    "        \"\"\"计算函数的FLOPs\"\"\"\n",
    "        ...\n",
    "\n",
    "    @compatibility(is_backward_compatible=True)\n",
    "    def fetch_module_flops(self, module: Any, result: Tensor, *args, **kwargs) -> Any:\n",
    "        \"\"\"计算模块的FLOPs\"\"\"\n",
    "        assert len(args) == 1\n",
    "        assert isinstance(args[0], Tensor)\n",
    "        assert isinstance(result, Tensor)\n",
    "        input_shape = args[0].shape  # [..., d_in]\n",
    "        weight_shape = module.weight.T.shape  # [d_out, d_in].T -> [d_in, d_out]\n",
    "        result_shape = result.shape\n",
    "\n",
    "        assert input_shape[-1] == weight_shape[0], f\"{input_shape}, {weight_shape}\"\n",
    "        matmul_shape = Size(list(input_shape[:-1]) + list(weight_shape[-1:]))\n",
    "        assert matmul_shape == result_shape\n",
    "\n",
    "        total_flops = flops_matmul(input_shape, weight_shape, result_shape)\n",
    "        if module.bias is not None:\n",
    "            total_flops += flops_elemwise(result_shape)\n",
    "        return total_flops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model = SimpleModel()\n",
    "gm = torch.fx.symbolic_trace(model)\n",
    "sample_input = torch.randn(1, 5)\n",
    "result = LinearFLOPs(gm).propagate(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.prod(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.fx.node import Node\n",
    "from tabulate import tabulate\n",
    "from d2py.utils.log_config import config_logging\n",
    "from torch_book.scan_temp.flop import get_FLOPs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # 忽略用户警告\n",
    "config_logging(\"flops.log\", filter_mod_names={\"torch\"}) # 配置日志信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(5, 4, bias=True)\n",
    "gm = torch.fx.symbolic_trace(model)\n",
    "sample_input = torch.randn(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
