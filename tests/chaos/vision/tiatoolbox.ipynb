{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWsXrOQGyiNu"
   },
   "source": [
    "# 使用 PyTorch 和 TIAToolbox 进行 WSIs 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本教程中，将展示如何使用 TIAToolbox 辅助的 PyTorch 深度学习模型对全量影像（Whole Slide Images，简称 WSI）进行分类。WSIs 是通过手术或活检获取的人体组织，并使用专用扫描仪进行扫描。病理学家和计算病理学研究人员使用它们来[在显微镜下研究癌症](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/)，以了解例如肿瘤生长并帮助改善患者的治疗。\n",
    "\n",
    "使 WSI 难以处理的是它们巨大的尺寸。例如，典型的幻灯片图像的大小约为[100,000x100,000像素](https://doi.org/10.1117%2F12.912388)，其中每个像素对应于幻灯片上约0.25x0.25微米的区域。这带来了加载和处理这些图像的挑战，更不用说在一个研究中可能有数百甚至数千张 WSIs（更大的研究会产生更好的结果）！\n",
    "\n",
    "传统的图像处理管道不适用于 WSI 处理，因此需要更好的工具。这就是 [TIAToolbox](https://github.com/TissueImageAnalytics/tiatoolbox) 可以提供帮助的地方，它带来了一套有用的工具，可以快速且计算高效地导入和处理组织幻灯片。通常，WSIs 以金字塔结构保存，具有多个相同图像的不同放大级别的副本，这些副本针对可视化进行了优化。金字塔的第0层（或底层）包含最高放大率或缩放级别的图像，而金字塔中的较高层具有较低分辨率的基图像副本。金字塔结构如下图所示。\n",
    "\n",
    "![WSI 金字塔堆栈](https://tia-toolbox.readthedocs.io/en/latest/_images/read_bounds_tissue.png)\n",
    "*WSI 金字塔堆栈（[来源](https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#)）*\n",
    "\n",
    "TIAToolbox 允许我们自动化常见的下游分析任务，例如[组织分类](https://doi.org/10.1016/j.media.2022.102685)。在本教程中，我们将向您展示如何：\n",
    "1. 使用 TIAToolbox 加载 WSI 图像；以及\n",
    "2. 使用不同的 PyTorch 模型对幻灯片进行批量分类。在本教程中，我们将提供使用 TorchVision 的 `ResNet18` 模型和自定义 [`HistoEncoder`](https://github.com/jopo666/HistoEncoder) 模型的示例。\n",
    "\n",
    "让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPiF6kU5yiN0"
   },
   "source": [
    "## 设置环境\n",
    "\n",
    "要运行本教程中提供的示例，需要以下软件包作为先决条件。\n",
    "\n",
    "1. OpenJpeg\n",
    "2. OpenSlide\n",
    "3. Pixman\n",
    "4. TIAToolbox\n",
    "5. HistoEncoder（用于自定义模型示例）\n",
    "\n",
    "请在终端中运行以下命令来安装这些软件包：\n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools libpixman-1-dev | tail -n 1\n",
    "pip install histoencoder | tail -n 1\n",
    "pip install git+https://github.com/TissueImageAnalytics/tiatoolbox.git@develop | tail -n 1\n",
    "echo \"Installation is done.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seaUmzYoSANq"
   },
   "source": [
    "或者，您可以在 MacOS 上运行 `brew install openjpeg openslide` 来安装先决条件软件包，而不是 `apt-get`。有关安装的更多信息可以在[这里](https://tia-toolbox.readthedocs.io/en/latest/installation.html)找到。您可能需要在页面顶部的运行时菜单中重新启动运行时，以继续本教程的其余部分，以便新安装的依赖项被识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T18:43:40.489228400Z",
     "start_time": "2023-11-10T18:43:39.434913Z"
    },
    "id": "SNbdWfvnFtG5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiatoolbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# TIAToolbox for WSI loading and processing\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtiatoolbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtiatoolbox\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchitecture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vanilla\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtiatoolbox\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatch_predictor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     IOPatchPredictorConfig,\n\u001b[1;32m     30\u001b[0m     PatchPredictor,\n\u001b[1;32m     31\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiatoolbox'"
     ]
    }
   ],
   "source": [
    "\"\"\"Import modules required to run the Jupyter notebook.\"\"\"\n",
    "# Configure logging\n",
    "import logging\n",
    "import warnings\n",
    "if logging.getLogger().hasHandlers():\n",
    "    logging.getLogger().handlers.clear()\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")\n",
    "\n",
    "# Downloading data and files\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Data processing and visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import PIL\n",
    "import contextlib\n",
    "import io\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# TIAToolbox for WSI loading and processing\n",
    "from tiatoolbox import logger\n",
    "from tiatoolbox.models.architecture import vanilla\n",
    "from tiatoolbox.models.engine.patch_predictor import (\n",
    "    IOPatchPredictorConfig,\n",
    "    PatchPredictor,\n",
    ")\n",
    "from tiatoolbox.utils.misc import download_data, grab_files_from_dir\n",
    "from tiatoolbox.utils.visualization import overlay_prediction_mask\n",
    "from tiatoolbox.wsicore.wsireader import WSIReader\n",
    "\n",
    "# Torch-related\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Configure plotting\n",
    "mpl.rcParams[\"figure.dpi\"] = 160  # for high resolution figure in notebook\n",
    "mpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode\n",
    "\n",
    "# If you are not using GPU, change ON_GPU to False\n",
    "ON_GPU = True\n",
    "\n",
    "# Function to suppress console output for overly verbose code blocks\n",
    "def suppress_console_output():\n",
    "    return contextlib.redirect_stderr(io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "X8dSUvDHSANq"
   },
   "source": [
    "### Clean-up before a run\n",
    "\n",
    "To ensure proper clean-up (for example in abnormal termination), all files downloaded or created in this run are saved in a single directory `global_save_dir`, which we set equal to \"./tmp/\". To simplify maintenance, the name of the directory occurs only at this one place, so that it can easily be changed, if desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T18:41:51.192871200Z",
     "start_time": "2023-11-10T18:41:51.160504Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YibjAicoAVS1",
    "outputId": "0006363f-003a-42d2-ee34-25105b6339a4",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "global_save_dir = Path(\"./tmp/\")\n",
    "\n",
    "\n",
    "def rmdir(dir_path: str | Path) -> None:\n",
    "    \"\"\"Helper function to delete directory.\"\"\"\n",
    "    if Path(dir_path).is_dir():\n",
    "        shutil.rmtree(dir_path)\n",
    "        logger.info(\"Removing directory %s\", dir_path)\n",
    "\n",
    "\n",
    "rmdir(global_save_dir)  # remove  directory if it exists from previous runs\n",
    "global_save_dir.mkdir()\n",
    "logger.info(\"Creating new directory %s\", global_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlgYO3n0FtG6"
   },
   "source": [
    "### Downloading the data\n",
    "For our sample data, we will use one whole-slide image, and patches from the validation subset of [Kather 100k](https://zenodo.org/record/1214456#.YJ-tn3mSkuU) dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T18:41:56.177054800Z",
     "start_time": "2023-11-10T18:41:56.104412700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7CzZGFHFtG6",
    "outputId": "39bd40d4-9f0c-4f0a-e18a-e7e982e8364e",
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "wsi_path = global_save_dir / \"sample_wsi.svs\"\n",
    "patches_path = global_save_dir / \"kather100k-validation-sample.zip\"\n",
    "weights_path = global_save_dir / \"resnet18-kather100k.pth\"\n",
    "\n",
    "logger.info(\"Download has started. Please wait...\")\n",
    "\n",
    "# Downloading and unzip a sample whole-slide image\n",
    "download_data(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs\",\n",
    "    wsi_path,\n",
    ")\n",
    "\n",
    "# Download and unzip a sample of the validation set used to train the Kather 100K dataset\n",
    "download_data(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/datasets/kather100k-validation-sample.zip\",\n",
    "    patches_path,\n",
    ")\n",
    "with ZipFile(patches_path, \"r\") as zipfile:\n",
    "    zipfile.extractall(path=global_save_dir)\n",
    "\n",
    "# Download pretrained model weights for WSI classification using ResNet18 architecture\n",
    "download_data(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/models/pc/resnet18-kather100k.pth\",\n",
    "    weights_path,\n",
    ")\n",
    "\n",
    "logger.info(\"Download is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdaSTKE8FtG7"
   },
   "source": [
    "## Reading the data\n",
    "\n",
    "We create a list of patches and a list of corresponding labels.\n",
    "For example, the first label in `label_list` will indicate the class of the first image patch in `patch_list`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T18:40:05.791111900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 886
    },
    "id": "5sF4Q-6Px6IV",
    "outputId": "4c474a52-24ca-4947-9cf0-08dcfe960702"
   },
   "outputs": [],
   "source": [
    "# Read the patch data and create a list of patches and a list of corresponding labels\n",
    "dataset_path = global_save_dir / \"kather100k-validation-sample\"\n",
    "\n",
    "# Set the path to the dataset\n",
    "image_ext = \".tif\"  # file extension of each image\n",
    "\n",
    "# Obtain the mapping between the label ID and the class name\n",
    "label_dict = {\n",
    "    \"BACK\": 0, # Background (empty glass region)\n",
    "    \"NORM\": 1, # Normal colon mucosa\n",
    "    \"DEB\": 2,  # Debris\n",
    "    \"TUM\": 3,  # Colorectal adenocarcinoma epithelium\n",
    "    \"ADI\": 4,  # Adipose\n",
    "    \"MUC\": 5,  # Mucus\n",
    "    \"MUS\": 6,  # Smooth muscle\n",
    "    \"STR\": 7,  # Cancer-associated stroma\n",
    "    \"LYM\": 8,  # Lymphocytes\n",
    "}\n",
    "\n",
    "class_names = list(label_dict.keys())\n",
    "class_labels = list(label_dict.values())\n",
    "\n",
    "# Generate a list of patches and generate the label from the filename\n",
    "patch_list = []\n",
    "label_list = []\n",
    "for class_name, label in label_dict.items():\n",
    "    dataset_class_path = dataset_path / class_name\n",
    "    patch_list_single_class = grab_files_from_dir(\n",
    "        dataset_class_path,\n",
    "        file_types=\"*\" + image_ext,\n",
    "    )\n",
    "    patch_list.extend(patch_list_single_class)\n",
    "    label_list.extend([label] * len(patch_list_single_class))\n",
    "\n",
    "# Show some dataset statistics\n",
    "plt.bar(class_names, [label_list.count(label) for label in class_labels])\n",
    "plt.xlabel(\"Patch types\")\n",
    "plt.ylabel(\"Number of patches\")\n",
    "\n",
    "# Count the number of examples per class\n",
    "for class_name, label in label_dict.items():\n",
    "    logger.info(\n",
    "        \"Class ID: %d -- Class Name: %s -- Number of images: %d\",\n",
    "        label,\n",
    "        class_name,\n",
    "        label_list.count(label),\n",
    "    )\n",
    "\n",
    "# Overall dataset statistics\n",
    "logger.info(\"Total number of patches: %d\", (len(patch_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8tg66bu48Vh"
   },
   "source": [
    "As you can see for this patch dataset, we have 9 classes/labels with IDs 0-8 and associated class names. describing the dominant tissue type in the patch:\n",
    "\n",
    "- BACK ⟶ Background (empty glass region)\n",
    "- LYM  ⟶ Lymphocytes\n",
    "- NORM ⟶ Normal colon mucosa\n",
    "- DEB  ⟶ Debris\n",
    "- MUS  ⟶ Smooth muscle\n",
    "- STR  ⟶ Cancer-associated stroma\n",
    "- ADI  ⟶ Adipose\n",
    "- MUC  ⟶ Mucus\n",
    "- TUM  ⟶ Colorectal adenocarcinoma epithelium\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxBdhIE-FtG7"
   },
   "source": [
    "## Classify image patches\n",
    "\n",
    "We demonstrate how to obtain a prediction for each patch within a digital slide first with the `patch` mode and then with a large slide using `wsi` mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8_S93fSVaFS"
   },
   "source": [
    "### Define `PatchPredictor` model\n",
    "\n",
    "The PatchPredictor class runs a CNN-based classifier written in PyTorch.\n",
    "\n",
    "- `model` can be any trained PyTorch model with the constraint that it should follow the [`tiatoolbox.models.abc.ModelABC`](https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.models.models_abc.ModelABC.html) class structure. For more information on this matter, please refer to [our example notebook on advanced model techniques](https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/07-advanced-modeling.ipynb). In order to load a custom model, you need to write a small preprocessing function, as in `preproc_func(img)`, which make sures the input tensors are in the right format for the loaded network.\n",
    "- Alternatively, you can pass `pretrained_model` as a string argument. This specifies the CNN model that performs the prediction, and it must be one of the models listed [here](https://tia-toolbox.readthedocs.io/en/latest/usage.html?highlight=pretrained%20models#tiatoolbox.models.architecture.get_pretrained_model). The command will look like this: `predictor = PatchPredictor(pretrained_model='resnet18-kather100k', pretrained_weights=weights_path, batch_size=32)`.\n",
    "- `pretrained_weights`: When using a `pretrained_model`, the corresponding pretrained weights will also be downloaded by default.  You can override the default with your own set of weights via the `pretrained_weight` argument.\n",
    "- `batch_size`: Number of images fed into the model each time. Higher values for this parameter require a larger (GPU) memory capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T18:40:05.805638800Z"
    },
    "id": "dlQu5878FtG8",
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "model = vanilla.CNNModel(backbone=\"resnet18\", num_classes=9) # Importing model from torchvision.models.resnet18\n",
    "model.load_state_dict(torch.load(weights_path, map_location=\"cpu\"), strict=True)\n",
    "def preproc_func(img):\n",
    "    img = PIL.Image.fromarray(img)\n",
    "    img = transforms.ToTensor()(img)\n",
    "    return img.permute(1, 2, 0)\n",
    "model.preproc_func = preproc_func\n",
    "predictor = PatchPredictor(model=model, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKUJrBKkSANr"
   },
   "source": [
    "### Predict patch labels\n",
    "\n",
    "We create a predictor object and then call the `predict` method using the `patch` mode. We then compute the classification accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_NpnknhSANr",
    "outputId": "eadde29a-8fdd-44d8-d238-8498c87edc59"
   },
   "outputs": [],
   "source": [
    "with suppress_console_output():\n",
    "    output = predictor.predict(imgs=patch_list, mode=\"patch\", on_gpu=ON_GPU)\n",
    "\n",
    "acc = accuracy_score(label_list, output[\"predictions\"])\n",
    "logger.info(\"Classification accuracy: %f\", acc)\n",
    "\n",
    "# Creating and visualizing the confusion matrix for patch classification results\n",
    "conf = confusion_matrix(label_list, output[\"predictions\"], normalize=\"true\")\n",
    "df_cm = pd.DataFrame(conf, index=class_names, columns=class_names)\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rmVxHVmSANs"
   },
   "source": [
    "### Predict patch labels for a whole slide\n",
    "\n",
    "We also introduce `IOPatchPredictorConfig`, a class that specifies the configuration of image reading and prediction writing for the model prediction engine. This is required to inform the classifier which level of the WSI pyramid the classifier should read, process data and generate output.\n",
    "\n",
    "Parameters of `IOPatchPredictorConfig` are defined as:\n",
    "\n",
    "- `input_resolutions`: A list, in the form of a dictionary, specifying the resolution of each input. List elements must be in the same order as in the target `model.forward()`. If your model accepts only one input, you just need to put one dictionary specifying `'units'` and `'resolution'`. Note that TIAToolbox supports a model with more than one input. For more information on units and resolution, please see [TIAToolbox documentation](https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#tiatoolbox.wsicore.wsireader.WSIReader.read_rect).\n",
    "- `patch_input_shape`: Shape of the largest input in (height, width) format.\n",
    "- `stride_shape`: The size of a stride (steps) between two consecutive patches, used in the patch extraction process. If the user sets `stride_shape` equal to `patch_input_shape`, patches will be extracted and processed without any overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T18:40:05.805638800Z"
    },
    "id": "9Kp1kx7wmOYq"
   },
   "outputs": [],
   "source": [
    "wsi_ioconfig = IOPatchPredictorConfig(\n",
    "    input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n",
    "    patch_input_shape=[224, 224],\n",
    "    stride_shape=[224, 224],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drn9RF4-SANs"
   },
   "source": [
    "The `predict` method applies the CNN on the input patches and get the results. Here are the arguments and their descriptions:\n",
    "\n",
    "- `mode`: Type of input to be processed. Choose from `patch`, `tile` or `wsi` according to your application.\n",
    "- `imgs`: List of inputs, which should be a list of paths to the input tiles or WSIs.\n",
    "- `return_probabilities`: Set to *__True__* to get per class probabilities alongside predicted labels of input patches. If you wish to merge the predictions to generate prediction maps for `tile` or `wsi` modes, you can set `return_probabilities=True`.\n",
    "- `ioconfig`: set the IO configuration information using the `IOPatchPredictorConfig` class.\n",
    "- `resolution` and `unit` (not shown below): These arguments specify the level or micron-per-pixel resolution of the WSI levels from which we plan to extract patches and can be used instead of `ioconfig`. Here we specify the WSI's level as `'baseline'`, which is equivalent to level 0. In general, this is the level of greatest resolution. In this particular case, the image has only one level. More information can be found in the [documentation](https://tia-toolbox.readthedocs.io/en/latest/usage.html?highlight=WSIReader.read_rect#tiatoolbox.wsicore.wsireader.WSIReader.read_rect).\n",
    "- `masks`: A list of paths corresponding to the masks of WSIs in the     `imgs` list. These masks specify the regions in the original WSIs from which we want to extract patches. If the mask of a particular WSI is specified as `None`, then the labels for all patches of that WSI (even background regions)  would be predicted. This could cause unnecessary computation.\n",
    "- `merge_predictions`: You can set this parameter to `True` if it's required to generate a 2D map of patch classification results. However, for large WSIs this will require large available memeory. An alternative (default) solution is to set `merge_predictions=False`, and then generate the 2D prediction maps using the `merge_predictions` function as you will see later on.\n",
    "\n",
    "Since we are using a large WSI the patch extraction and prediction processes may take some time (make sure to set the `ON_GPU=True` if you have access to Cuda enabled GPU and PyTorch+Cuda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUZTP0bKSANs",
    "outputId": "723a5ee7-7f0d-462c-ac59-c6acfb720c85"
   },
   "outputs": [],
   "source": [
    "with suppress_console_output():\n",
    "    wsi_output = predictor.predict(\n",
    "        imgs=[wsi_path],\n",
    "        masks=None,\n",
    "        mode=\"wsi\",\n",
    "        merge_predictions=False,\n",
    "        ioconfig=wsi_ioconfig,\n",
    "        return_probabilities=True,\n",
    "        save_dir=global_save_dir / \"wsi_predictions\",\n",
    "        on_gpu=ON_GPU,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noAAy35oSANs"
   },
   "source": [
    "We see how the prediction model works on our whole-slide images by visualizing the `wsi_output`. We first need to merge patch prediction outputs and then visualize them as an overlay on the original image. As before, the `merge_predictions` method is used to merge the patch predictions. Here we set the parameters `resolution=1.25, units='power'` to generate the prediction map at 1.25x magnification. If you would like to have higher/lower resolution (bigger/smaller) prediction maps, you need to change these parameters accordingly. When the predictions are merged, use the `overlay_patch_prediction` function to overlay the prediction map on the WSI thumbnail, which should be extracted at the resolution used for prediction merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-10T18:40:05.805638800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WF_vY2B4i1yi",
    "outputId": "04feef1f-6754-4181-c8a7-20afb35b345c"
   },
   "outputs": [],
   "source": [
    "overview_resolution = (\n",
    "    4  # the resolution in which we desire to merge and visualize the patch predictions\n",
    ")\n",
    "# the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\"\n",
    "overview_unit = \"mpp\"\n",
    "wsi = WSIReader.open(wsi_path)\n",
    "wsi_overview = wsi.slide_thumbnail(resolution=overview_resolution, units=overview_unit)\n",
    "plt.figure(), plt.imshow(wsi_overview)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruKBD5tSSANs"
   },
   "source": [
    "Overlaying the prediction map on this image as below gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RndmFblDSANs",
    "outputId": "48969f6f-55e9-4d7c-bfc8-c286089cd268"
   },
   "outputs": [],
   "source": [
    "# Visualization of whole-slide image patch-level prediction\n",
    "# first set up a label to color mapping\n",
    "label_color_dict = {}\n",
    "label_color_dict[0] = (\"empty\", (0, 0, 0))\n",
    "colors = cm.get_cmap(\"Set1\").colors\n",
    "for class_name, label in label_dict.items():\n",
    "    label_color_dict[label + 1] = (class_name, 255 * np.array(colors[label]))\n",
    "\n",
    "pred_map = predictor.merge_predictions(\n",
    "    wsi_path,\n",
    "    wsi_output[0],\n",
    "    resolution=overview_resolution,\n",
    "    units=overview_unit,\n",
    ")\n",
    "overlay = overlay_prediction_mask(\n",
    "    wsi_overview,\n",
    "    pred_map,\n",
    "    alpha=0.5,\n",
    "    label_info=label_color_dict,\n",
    "    return_ax=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D-rITa4SANs"
   },
   "source": [
    "## Feature extraction with a pathology-specific model\n",
    "\n",
    "In this section, we will show how to extract features from a pretrained pytorch model that exists outside TIAToolbox, using the WSI inference engines provided by tiatoolbox. To illustrate this we will use HistoEncoder, a computational-pathology specific model that has been trained in a self-supervised fashion to extract features from histology images. The model has been made available here:\n",
    "\n",
    "'HistoEncoder: Foundation models for digital pathology' (https://github.com/jopo666/HistoEncoder) by Pohjonen, Joona and team at the University of Helsinki.\n",
    "\n",
    "We will plot a umap reduction into 3D (rgb) of the feature map to visualize how the features capture the differences between some of the above mentioned tissue types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpInLNBLSANt"
   },
   "outputs": [],
   "source": [
    "# Import some extra modules\n",
    "import histoencoder.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from tiatoolbox.models.engine.semantic_segmentor import DeepFeatureExtractor, IOSegmentorConfig\n",
    "from tiatoolbox.models.models_abc import ModelABC\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8BFVjGESANt"
   },
   "source": [
    "TIAToolbox defines a ModelABC which is a class inheriting PyTorch [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and specifies how a model should look in order to be used in the TIAToolbox inference engines. The histoencoder model doesn't follow this structure, so we need to wrap it in a class whose output and methods are those that the TIAToolbox engine expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Af9QuM7PSANt"
   },
   "outputs": [],
   "source": [
    "class HistoEncWrapper(ModelABC):\n",
    "    \"\"\"Wrapper for HistoEnc model that conforms to tiatoolbox ModelABC interface.\"\"\"\n",
    "\n",
    "    def __init__(self: HistoEncWrapper, encoder) -> None:\n",
    "        super().__init__()\n",
    "        self.feat_extract = encoder\n",
    "\n",
    "    def forward(self: HistoEncWrapper, imgs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Pass input data through the model.\n",
    "\n",
    "        Args:\n",
    "            imgs (torch.Tensor):\n",
    "                Model input.\n",
    "\n",
    "        \"\"\"\n",
    "        out = F.extract_features(self.feat_extract, imgs, num_blocks=2, avg_pool=True)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def infer_batch(\n",
    "        model: nn.Module,\n",
    "        batch_data: torch.Tensor,\n",
    "        *,\n",
    "        on_gpu: bool,\n",
    "    ) -> list[np.ndarray]:\n",
    "        \"\"\"Run inference on an input batch.\n",
    "\n",
    "        Contains logic for forward operation as well as i/o aggregation.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module):\n",
    "                PyTorch defined model.\n",
    "            batch_data (torch.Tensor):\n",
    "                A batch of data generated by\n",
    "                `torch.utils.data.DataLoader`.\n",
    "            on_gpu (bool):\n",
    "                Whether to run inference on a GPU.\n",
    "\n",
    "        \"\"\"\n",
    "        img_patches_device = batch_data.to('cuda') if on_gpu else batch_data\n",
    "        model.eval()\n",
    "        # Do not compute the gradient (not training)\n",
    "        with torch.inference_mode():\n",
    "            output = model(img_patches_device)\n",
    "        return [output.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XQpoea5SANt"
   },
   "source": [
    "Now that we have our wrapper, we will create our feature extraction model and instantiate a [DeepFeatureExtractor](https://tia-toolbox.readthedocs.io/en/v1.4.1/_autosummary/tiatoolbox.models.engine.semantic_segmentor.DeepFeatureExtractor.html) to allow us to use this model over a WSI. We will use the same WSI as above, but this time we will extract features from the patches of the WSI using the HistoEncoder model, rather than predicting some label for each patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtSHvExqSANt"
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "encoder = F.create_encoder(\"prostate_medium\")\n",
    "model = HistoEncWrapper(encoder)\n",
    "\n",
    "# set the pre-processing function\n",
    "norm=transforms.Normalize(mean=[0.662, 0.446, 0.605],std=[0.169, 0.190, 0.155])\n",
    "trans = [\n",
    "    transforms.ToTensor(),\n",
    "    norm,\n",
    "]\n",
    "model.preproc_func = transforms.Compose(trans)\n",
    "\n",
    "wsi_ioconfig = IOSegmentorConfig(\n",
    "    input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n",
    "    patch_input_shape=[224, 224],\n",
    "    output_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n",
    "    patch_output_shape=[224, 224],\n",
    "    stride_shape=[224, 224],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6LrLhviSANt"
   },
   "source": [
    "When we create the `DeepFeatureExtractor`, we will pass the `auto_generate_mask=True` argument. This will automatically create a mask of the tissue region using otsu thresholding, so that the extractor processes only those patches containing tissue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoTLy4k0SANt",
    "outputId": "936b14d4-8d83-42e3-dfcc-ab637fc23c03"
   },
   "outputs": [],
   "source": [
    "# create the feature extractor and run it on the WSI\n",
    "extractor = DeepFeatureExtractor(model=model, auto_generate_mask=True, batch_size=32, num_loader_workers=4, num_postproc_workers=4)\n",
    "with suppress_console_output():\n",
    "    out = extractor.predict(imgs=[wsi_path], mode=\"wsi\", ioconfig=wsi_ioconfig, save_dir=global_save_dir / \"wsi_features\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMJKi5JkSANt"
   },
   "source": [
    "These features could be used to train a downstream model, but here in order to get some intuition for what the features represent, we will use a UMAP reduction to visualize the features in RGB space. The points labeled in a similar color should have similar features, so we can check if the features naturally separate out into the different tissue regions when we overlay the UMAP reduction on the WSI thumbnail. We will plot it along with the patch-level prediction map from above to see how the features compare to the patch-level predictions in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNIpM0dJSANt",
    "outputId": "d5dcd269-704d-486f-92da-5639ff642994"
   },
   "outputs": [],
   "source": [
    "# First we define a function to calculate the umap reduction\n",
    "def umap_reducer(x, dims=3, nns=10):\n",
    "    \"\"\"UMAP reduction of the input data.\"\"\"\n",
    "    reducer = umap.UMAP(n_neighbors=nns, n_components=dims, metric=\"manhattan\", spread=0.5, random_state=2)\n",
    "    reduced = reducer.fit_transform(x)\n",
    "    reduced -= reduced.min(axis=0)\n",
    "    reduced /= reduced.max(axis=0)\n",
    "    return reduced\n",
    "\n",
    "# load the features output by our feature extractor\n",
    "pos = np.load(global_save_dir / \"wsi_features\" / \"0.position.npy\")\n",
    "feats = np.load(global_save_dir / \"wsi_features\" / \"0.features.0.npy\")\n",
    "pos = pos / 8 # as we extracted at 0.5mpp, and we are overlaying on a thumbnail at 4mpp\n",
    "\n",
    "# reduce the features into 3 dimensional (rgb) space\n",
    "reduced = umap_reducer(feats)\n",
    "\n",
    "# plot the prediction map the classifier again\n",
    "overlay = overlay_prediction_mask(\n",
    "    wsi_overview,\n",
    "    pred_map,\n",
    "    alpha=0.5,\n",
    "    label_info=label_color_dict,\n",
    "    return_ax=True,\n",
    ")\n",
    "\n",
    "# plot the feature map reduction\n",
    "plt.figure()\n",
    "plt.imshow(wsi_overview)\n",
    "plt.scatter(pos[:,0], pos[:,1], c=reduced, s=1, alpha=0.5)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"UMAP reduction of HistoEnc features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixWAJc_ZSANt"
   },
   "source": [
    "We see that the prediction map from our patch-level predictor, and the feature map from our self-supervised feature encoder, capture similar information about the tissue types in the WSI. This is a good sanity check that our models are working as expected. It also shows that the features extracted by the HistoEncoder model are capturing the differences between the tissue types, and so that they are encoding histologically relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_1pb6BGGbVu"
   },
   "source": [
    "## Where to Go From Here\n",
    "\n",
    "In this notebook, we show how we can use the `PatchPredictor` and `DeepFeatureExtractor` classes and their `predict` method to predict the label, or extract features, for patches of big tiles and WSIs. We introduce `merge_predictions` and `overlay_prediction_mask` helper functions that merge the patch prediction outputs and visualize the resulting prediction map as an overlay on the input image/WSI.\n",
    "\n",
    "All the processes take place within TIAToolbox and we can easily put the pieces together, following our example code. Please make sure to set inputs and options correctly. We encourage you to further investigate the effect on the prediction output of changing `predict` function parameters. We have demonstrated how to use your own pretrained model or one provided by the research community for a specific task in the TIAToolbox framework to do inference on large WSIs even if the model structure is not defined in the TIAToolbox model class.\n",
    "\n",
    "You can learn more through the following resources:\n",
    "\n",
    "- [Advanced model handling with PyTorch and TIAToolbox](https://tia-toolbox.readthedocs.io/en/latest/_notebooks/jnb/07-advanced-modeling.html)\n",
    "- [Creating slide graphs for WSI with a custom PyTorch graph neural network](https://tia-toolbox.readthedocs.io/en/latest/_notebooks/jnb/full-pipelines/slide-graph.html)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
