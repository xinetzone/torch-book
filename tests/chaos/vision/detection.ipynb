{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TorchVision 目标检测微调教程\n",
    "\n",
    "参考: [intermediate/detection](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html?highlight=detection)\n",
    "\n",
    "在本教程中，我们将在 [Penn-Fudan 行人检测和分割数据库](https://www.cis.upenn.edu/~jshi/ped_html/) 上微调预训练的 [Mask R-CNN](https://arxiv.org/abs/1703.06870) 模型。它包含 170 张图像，其中有 345 个行人实例，我们将使用它来说明如何使用 torchvision 中的新功能在自定义数据集上训练目标检测和实例分割模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据集\n",
    "\n",
    "用于训练目标检测、实例分割和人体关键点检测的参考脚本允许轻松支持添加新的自定义数据集。数据集应继承自标准的 {class}`torch.utils.data.Dataset` 类，并实现 `__len__` 和 `__getitem__`。\n",
    "\n",
    "唯一的要求是数据集的 `__getitem__` 应该返回元组：\n",
    "\n",
    "-   图像：{class}`torchvision.tv_tensors.Image` 形状为 `[3, H, W]`，一个纯张量，或一个大小为 `(H, W)` 的 PIL 图像\n",
    "-   目标：包含以下字段的字典\n",
    "    -   `boxes`，{class}`torchvision.tv_tensors.BoundingBoxes` 形状为 `[N, 4]`：`N` 个边界框的坐标，格式为 `[x0, y0, x1, y1]`，范围从 `0` 到 `W` 和 `0` 到 `H`\n",
    "    -   `labels`，整数 `torch.Tensor` 形状为 `[N]`：每个边界框的标签。`0` 始终表示背景类。\n",
    "    -   `image_id`，整数：图像标识符。它在数据集中的所有图像中应该是唯一的，并在评估期间使用\n",
    "    -   `area`，浮点数 `torch.Tensor` 形状为 `[N]`：边界框的面积。这在使用 COCO 指标进行评估时使用，以区分小、中和大框的指标分数\n",
    "    -   `iscrowd`，uint8 `torch.Tensor` 形状为 `[N]`：`iscrowd=True` 的实例将在评估期间被忽略\n",
    "    -   （可选）`masks`，`torchvision.tv_tensors.Mask` 形状为 `[N, H, W]`：每个对象的分割掩码\n",
    "\n",
    "如果您的数据集符合上述要求，那么它将适用于参考脚本中的训练和评估代码。评估代码将使用 `pycocotools` 中的脚本，可以通过 `pip install pycocotools` 安装。\n",
    "\n",
    "关于 `labels` 的一个注意事项。模型将类 `0` 视为背景。如果您的数据集不包含背景类，则不应在 `labels` 中包含 `0`。例如，假设您只有两个类，*猫* 和 *狗*，您可以将 `1`（而不是 `0`）定义为表示 *猫*，将 `2` 定义为表示 *狗*。因此，例如，如果一张图像包含两个类，您的 `labels` 张量应如下所示 `[1, 2]`。\n",
    "\n",
    "此外，如果您想在训练期间使用宽高比分组（以便每个批次仅包含具有相似宽高比的图像），则建议还实现一个 `get_height_and_width` 方法，该方法返回图像的高度和宽度。如果未提供此方法，我们将通过 `__getitem__` 查询数据集的所有元素，这将加载内存中的图像，并且比提供自定义方法慢。\n",
    "\n",
    "### 为 PennFudan 编写自定义数据集\n",
    "\n",
    "让我们为 PennFudan 数据集编写一个数据集。首先，让我们下载数据集并提取 [zip 文件](https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip)：\n",
    "\n",
    "``` {.python}\n",
    "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\n",
    "cd data && unzip PennFudanPed.zip\n",
    "```\n",
    "\n",
    "有以下文件夹结构：\n",
    "\n",
    "    PennFudanPed/\n",
    "      PedMasks/\n",
    "        FudanPed00001_mask.png\n",
    "        FudanPed00002_mask.png\n",
    "        FudanPed00003_mask.png\n",
    "        FudanPed00004_mask.png\n",
    "        ...\n",
    "      PNGImages/\n",
    "        FudanPed00001.png\n",
    "        FudanPed00002.png\n",
    "        FudanPed00003.png\n",
    "        FudanPed00004.png\n",
    "\n",
    "以下是图像和分割掩码对的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n",
    "mask = read_image(\"data/PennFudanPed/PedMasks/FudanPed00046_mask.png\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.subplot(122)\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(mask.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each image has a corresponding segmentation mask, where each color\n",
    "correspond to a different instance. Let's write a\n",
    "`torch.utils.data.Dataset` class for\n",
    "this dataset. In the code below, we are wrapping images, bounding boxes\n",
    "and masks into `torchvision.tv_tensors.TVTensor`{.interpreted-text\n",
    "role=\"class\"} classes so that we will be able to apply torchvision\n",
    "built-in transformations ([new Transforms\n",
    "API](https://pytorch.org/vision/stable/transforms.html)) for the given\n",
    "object detection and segmentation task. Namely, image tensors will be\n",
    "wrapped by `torchvision.tv_tensors.Image`{.interpreted-text\n",
    "role=\"class\"}, bounding boxes into\n",
    "`torchvision.tv_tensors.BoundingBoxes`\n",
    "and masks into `torchvision.tv_tensors.Mask`{.interpreted-text\n",
    "role=\"class\"}. As `torchvision.tv_tensors.TVTensor`{.interpreted-text\n",
    "role=\"class\"} are `torch.Tensor`\n",
    "subclasses, wrapped objects are also tensors and inherit the plain\n",
    "`torch.Tensor` API. For more information\n",
    "about torchvision `tv_tensors` see [this\n",
    "documentation](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html#what-are-tvtensors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = read_image(img_path)\n",
    "        mask = read_image(mask_path)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = torch.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        num_objs = len(obj_ids)\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        boxes = masks_to_boxes(masks)\n",
    "\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
    "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for the dataset. Now let's define a model that can perform\n",
    "predictions on this dataset.\n",
    "\n",
    "Defining your model\n",
    "===================\n",
    "\n",
    "In this tutorial, we will be using [Mask\n",
    "R-CNN](https://arxiv.org/abs/1703.06870), which is based on top of\n",
    "[Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN is a\n",
    "model that predicts both bounding boxes and class scores for potential\n",
    "objects in the image.\n",
    "\n",
    "![image](https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image03.png)\n",
    "\n",
    "Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts\n",
    "segmentation masks for each instance.\n",
    "\n",
    "![image](https://pytorch.org/tutorials/_static/img/tv_tutorial/tv_image04.png)\n",
    "\n",
    "There are two common situations where one might want to modify one of\n",
    "the available models in TorchVision Model Zoo. The first is when we want\n",
    "to start from a pre-trained model, and just finetune the last layer. The\n",
    "other is when we want to replace the backbone of the model with a\n",
    "different one (for faster predictions, for example).\n",
    "\n",
    "Let's go see how we would do one or another in the following sections.\n",
    "\n",
    "1 - Finetuning from a pretrained model\n",
    "--------------------------------------\n",
    "\n",
    "Let's suppose that you want to start from a model pre-trained on COCO\n",
    "and want to finetune it for your particular classes. Here is a possible\n",
    "way of doing it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Modifying the model to add a different backbone\n",
    "===================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
    "# ``FasterRCNN`` needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
    ")\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "    featmap_names=['0'],\n",
    "    output_size=7,\n",
    "    sampling_ratio=2\n",
    ")\n",
    "\n",
    "# put the pieces together inside a Faster-RCNN model\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=2,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection and instance segmentation model for PennFudan Dataset\n",
    "======================================================================\n",
    "\n",
    "In our case, we want to finetune from a pre-trained model, given that\n",
    "our dataset is very small, so we will be following approach number 1.\n",
    "\n",
    "Here we want to also compute the instance segmentation masks, so we will\n",
    "be using Mask R-CNN:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, this will make `model` be ready to be trained and evaluated\n",
    "on your custom dataset.\n",
    "\n",
    "Putting everything together\n",
    "===========================\n",
    "\n",
    "In `references/detection/`, we have a number of helper functions to\n",
    "simplify training and evaluating detection models. Here, we will use\n",
    "`references/detection/engine.py` and `references/detection/utils.py`.\n",
    "Just download everything under `references/detection` to your folder and\n",
    "use them here. On Linux if you have `wget`, you can download them using\n",
    "below commands:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since v0.15.0 torchvision provides [new Transforms\n",
    "API](https://pytorch.org/vision/stable/transforms.html) to easily write\n",
    "data augmentation pipelines for Object Detection and Segmentation tasks.\n",
    "\n",
    "Let's write some helper functions for data augmentation /\n",
    "transformation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing `forward()` method (Optional)\n",
    "=====================================\n",
    "\n",
    "Before iterating over the dataset, it\\'s good to see what the model\n",
    "expects during training and inference time on sample data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)\n",
    "\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the main function which performs the training and the\n",
    "validation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it just for 2 epochs\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after one epoch of training, we obtain a COCO-style mAP \\> 50, and a\n",
    "mask mAP of 65.\n",
    "\n",
    "But what do the predictions look like? Let's take one image in the\n",
    "dataset and verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "\n",
    "image = read_image(\"data/PennFudanPed/PNGImages/FudanPed00046.png\")\n",
    "eval_transform = get_transform(train=False)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = predictions[0]\n",
    "\n",
    "\n",
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"pedestrian: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
    "\n",
    "masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
    "output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look good!\n",
    "\n",
    "Wrapping up\n",
    "===========\n",
    "\n",
    "In this tutorial, you have learned how to create your own training\n",
    "pipeline for object detection models on a custom dataset. For that, you\n",
    "wrote a `torch.utils.data.Dataset` class\n",
    "that returns the images and the ground truth boxes and segmentation\n",
    "masks. You also leveraged a Mask R-CNN model pre-trained on COCO\n",
    "train2017 in order to perform transfer learning on this new dataset.\n",
    "\n",
    "For a more complete example, which includes multi-machine / multi-GPU\n",
    "training, check `references/detection/train.py`, which is present in the\n",
    "torchvision repository.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
