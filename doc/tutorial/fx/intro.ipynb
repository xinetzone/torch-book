{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FX 简介\n",
    "\n",
    "FX 是供开发人员用来转换 {class}`~torch.nn.Module` 实例的工具包。FX 由三个主要组件组成：符号跟踪器（symbolic tracer）、中间表示（intermediate representation，简写 IR）和 Python 代码生成（Python code generation）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, fx\n",
    "\n",
    "# 用于演示的简单模块\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n",
    "\n",
    "module = MyModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 符号跟踪前端（Symbolic tracing frontend）\n",
    "```\n",
    "\n",
    "```{note}\n",
    "**符号跟踪器** 执行 Python 代码的“符号执行”。它通过代码提供虚假的值，称为 **代理**。记录对这些代理的运算。有关符号跟踪的更多信息可以在 {func}`~torch.fx.symbolic_trace` 和 {class}`~torch.fx.Tracer` 文档中找到。\n",
    "```\n",
    "\n",
    "捕获模块的语义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_traced: fx.GraphModule = fx.symbolic_trace(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 高级中间表示（intermediate representationIR）\n",
    "```\n",
    "\n",
    "```{note}\n",
    "**中间表示** 是符号跟踪期间记录的运算的容器。它由一组 node 组成，这些 node 表示函数输入、调用站点（callsites，即函数、方法或 {class}`~torch.nn.Module` 实例）和返回值。关于 IR 的更多信息可以在 {class}`~torch.fx.Graph` 的文档中找到。IR 是应用变换（transformations）的格式。\n",
    "```\n",
    "\n",
    "计算图（graph）表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [#users=1] = placeholder[target=x]\n",
      "    %param : [#users=1] = get_attr[target=param]\n",
      "    %add : [#users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n",
      "    %linear : [#users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n",
      "    %clamp : [#users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n",
      "    return clamp\n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 代码生成（Code generation）\n",
    "```\n",
    "\n",
    "```{note}\n",
    "**Python 代码生成** 使 FX 成为 Python 到 Python （或 Module-to-Module）的变换工具包。对于每个 Graph IR，可以创建与 Graph 语义匹配的有效 Python 代码。该功能封装在 {class}`~torch.fx.GraphModule` 中，它是 {class}`~torch.nn.Module` 实例，包含 Graph 以及从 {class}`~torch.fx.Graph` 生成的 {meth}`forward` 方法。\n",
    "```\n",
    "\n",
    "有效的 Python 代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    param = self.param\n",
      "    add = x + param;  x = param = None\n",
      "    linear = self.linear(add);  add = None\n",
      "    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n",
      "    return clamp\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总的来说，这个组件管道（symbolic tracing -> intermediate representation -> transforms -> Python code generation）构成了 FX 的 Python-to-Python 变换管道（pipeline）。此外，这些组件可以单独使用。例如，可以单独使用符号跟踪来捕获代码的形式，以便进行分析（而不是变换）。代码生成可以用于以编程方式生成模型，例如从配置文件生成模型。\n",
    "\n",
    "## 编写变换\n",
    "\n",
    "什么是 FX 变换？本质上，它是这样的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, fx\n",
    "\n",
    "def transform(m: nn.Module,\n",
    "              tracer_class: type = fx.Tracer) -> nn.Module:\n",
    "    # 步骤 1：获取表示 `m` 代码的计算图表示\n",
    "\n",
    "    # NOTE: fx.symbolic_trace 是对 fx.Tracer.trace 调用和构造 GraphModule 的包装器。\n",
    "    # 将在变换中分离它，以允许调用者自定义 tracing 行为。\n",
    "    graph: fx.Graph = tracer_class().trace(m)\n",
    "\n",
    "    # 步骤 2: 修改此 Graph 或创建新的 Graph\n",
    "    graph = ...\n",
    "\n",
    "    # 步骤 3：返回构造的 Module\n",
    "    return fx.GraphModule(m, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transformation` 函数需要 {class}`~torch.nn.Module` 作为输入, 然后从该 {class}`~torch.nn.Module` 获得 {class}`~torch.fx.Graph` （即 IR）对其进行修改, 然后返回新的 {class}`~torch.nn.Module`。你应该把返回的 {class}`~torch.nn.Module` 想成和正常的  {class}`~torch.nn.Module` 一样：你可以把它传递给另一个 FX 变换，你可以把它传递给 TorchScript，或者你可以运行它。确保 FX 变换的输入和输出是 {class}`~torch.nn.Module` 将允许可组合性。\n",
    "\n",
    "````{note}\n",
    "也可以修改现有的 {class}`~torch.fx.GraphModule`，而不是创建新的 {class}`~torch.fx.GraphModule`，如下所示：\n",
    "```python\n",
    "\n",
    "def transform(m : nn.Module) -> nn.Module:\n",
    "    gm : fx.GraphModule = fx.symbolic_trace(m)\n",
    "\n",
    "    # Modify gm.graph\n",
    "    # <...>\n",
    "\n",
    "    # Recompile the forward() method of `gm` from its Graph\n",
    "    gm.recompile()\n",
    "\n",
    "    return gm\n",
    "```\n",
    "```{tip}\n",
    "注意，你必须调用 {meth}`~torch.fx.GraphModule.recompile` 来将 {class}`~torch.fx.GraphModule` 上生成的 {func}`forward` 方法与修改后的 {class}`~torch.fx.Graph` 同步。\n",
    "```\n",
    "````\n",
    "\n",
    "假设您已经传入了被跟踪到 {class}`~torch.fx.Graph` 中的 {class}`~torch.nn.Module`，那么现在您可以采用两种主要方法来构建新的 {class}`~torch.fx.Graph`。\n",
    "\n",
    "## Graph 入门\n",
    "\n",
    "{class}`~torch.fx.Graph` 的语义可以在 {class}`~torch.fx.Graph` 文档中找到完整的处理方法，但是在这里只介绍基础知识。{class}`~torch.fx.Graph` 是一个数据结构，表示 {class}`~torch.fx.GraphModule` 上的方法。这需要的信息是：\n",
    "\n",
    "- 此方法的输入是什么？\n",
    "- 此方法当中执行了哪些运算？\n",
    "- 此方法的输出是什么？\n",
    "\n",
    "这三个概念都用 {class}`~torch.fx.Node` 实例表示。\n",
    "\n",
    "用简短的例子来看看这是什么意思："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name           target                                                   args                kwargs\n",
      "-------------  -------------  -------------------------------------------------------  ------------------  -----------\n",
      "placeholder    x              x                                                        ()                  {}\n",
      "get_attr       linear_weight  linear.weight                                            ()                  {}\n",
      "call_function  add            <built-in function add>                                  (x, linear_weight)  {}\n",
      "call_module    linear         linear                                                   (add,)              {}\n",
      "call_method    relu           relu                                                     (linear,)           {}\n",
      "call_function  sum_1          <built-in method sum of type object at 0x7f3584fb2200>   (relu,)             {'dim': -1}\n",
      "call_function  topk           <built-in method topk of type object at 0x7f3584fb2200>  (sum_1, 3)          {}\n",
      "output         output         output                                                   (topk,)             {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import fx, nn\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), \n",
    "                                    dim=-1), 3)\n",
    "\n",
    "m = MyModule()\n",
    "gm = fx.symbolic_trace(m)\n",
    "\n",
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里定义了模块 `MyModule`，用于演示，实例化它，象征性地跟踪它，然后调用 {meth}`~torch.fx.Graph.print_tabular` 方法打印出一个表，显示这个图的节点。\n",
    "\n",
    "可以使用这些信息来回答上面提出的问题。\n",
    "\n",
    "上述表格足以回答我们的三个问题：\n",
    "\n",
    "1. 这个方法的输入是什么？在 FX 中, 方法输入被表示为 `placeholder` 节点。在我们的例子中，只有一个 `placeholder`，可以推断出来我们的 `forward` 的函数除了首参数 `self` 外只有一个额外的输入（即 `x`）。\n",
    "2. 这个方法当中执行了哪些运算？我们可以看到 `get_attr`、`call_funcation`、`call_module` 等节点表示了方法中的运算。\n",
    "3. 这个方法的输出是什么？我们使用特别的 `output` 来表示 {class}`~torch.fx.Graph` 的输出。\n",
    "\n",
    "现在知道了方法是如何在 {mod}`torch.fx` 中被记录表示的, 下一步便是通过 {class}`~torch.fx.Graph` 修改它。\n",
    "\n",
    "````{note}\n",
    "{class}`~torch.fx.Node` 是表示 {class}`~torch.fx.Graph` 中各个运算的数据结构。在大多数情况下，{class}`~torch.fx.Node` 表示对各种实体的调用点，如算子、方法和模块（一些例外包括指定函数输入和输出的 {class}`~torch.fx.Node`）。每个 {class}`~torch.fx.Node` 都有一个由 op 属性指定的函数。\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
