{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解释器模式\n",
    "\n",
    "FX 中一个有用的代码组织模式是循环遍历 {class}`~torch.fx.Graph` 中的所有 {class}`~torch.fx.Node` 并执行它们。这可以用于一些事情，包括对流经 {class}`~torch.fx.Graph` 的值的运行时分析，或者通过使用 {class}`~torch.fx.Proxy` 进行重跟踪的代码变换。例如，假设想要运行 {class}`~torch.fx。GraphModule` 并记录 {class}`~torch.Tensor` shape 和节点上的 dtype 属性，就像我们在运行时看到的那样。它可能看起来像:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "from torch.fx.node import Node\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "class ShapeProp:\n",
    "    \"\"\"\n",
    "    Shape propagation. This class takes a `GraphModule`.\n",
    "    Then, its `propagate` method executes the `GraphModule`\n",
    "    node-by-node with the given arguments. As each operation\n",
    "    executes, the ShapeProp class stores away the shape and\n",
    "    element type for the output values of each operation on\n",
    "    the `shape` and `dtype` attributes of the operation's\n",
    "    `Node`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mod):\n",
    "        self.mod = mod\n",
    "        self.graph = mod.graph\n",
    "        self.modules = dict(self.mod.named_modules())\n",
    "\n",
    "    def propagate(self, *args):\n",
    "        args_iter = iter(args)\n",
    "        env : Dict[str, Node] = {}\n",
    "\n",
    "        def load_arg(a):\n",
    "            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n",
    "\n",
    "        def fetch_attr(target : str):\n",
    "            target_atoms = target.split('.')\n",
    "            attr_itr = self.mod\n",
    "            for i, atom in enumerate(target_atoms):\n",
    "                if not hasattr(attr_itr, atom):\n",
    "                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
    "                attr_itr = getattr(attr_itr, atom)\n",
    "            return attr_itr\n",
    "\n",
    "        for node in self.graph.nodes:\n",
    "            if node.op == 'placeholder':\n",
    "                result = next(args_iter)\n",
    "            elif node.op == 'get_attr':\n",
    "                result = fetch_attr(node.target)\n",
    "            elif node.op == 'call_function':\n",
    "                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n",
    "            elif node.op == 'call_method':\n",
    "                self_obj, *args = load_arg(node.args)\n",
    "                kwargs = load_arg(node.kwargs)\n",
    "                result = getattr(self_obj, node.target)(*args, **kwargs)\n",
    "            elif node.op == 'call_module':\n",
    "                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n",
    "\n",
    "            # This is the only code specific to shape propagation.\n",
    "            # you can delete this `if` branch and this becomes\n",
    "            # a generic GraphModule interpreter.\n",
    "            if isinstance(result, torch.Tensor):\n",
    "                node.shape = result.shape\n",
    "                node.dtype = result.dtype\n",
    "\n",
    "            env[node.name] = result\n",
    "\n",
    "        return load_arg(self.graph.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如您所看到的，一个完整的 FX 解释器（interpreter）并不复杂，但它可能非常有用。为了方便使用这种模式，提供了 {class}`torch.fx.Interpreter` 类，它以一种可以通过方法重写来重写解释器执行的某些方面的方式包含了上述逻辑。\n",
    "\n",
    "除了执行运算之外，还可以通过解释器提供 {class}`torch.fx.Proxy` 值来生成新的 {class}`torch.fx.Graph`。类似地，提供 {class}`torch.fx.Transformer` 类来包含此模式。{class}`torch.fx.Transformer` 的行为类似于 {class}`torch.fx.Interpreter`，但不是调用 `run` 方法从模块中获取具体的输出值，而是调用 {meth}`torch.fx.Transformer.transform` 方法来返回新的 {class}`torch.fx.GraphModule`，它服从于作为覆盖方法安装的任何变换规则。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
