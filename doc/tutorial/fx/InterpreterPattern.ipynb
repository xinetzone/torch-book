{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解释器模式\n",
    "\n",
    "FX 中一个有用的代码组织模式是循环遍历 {class}`~torch.fx.Graph` 中的所有 {class}`~torch.fx.Node` 并执行它们。这可以用于一些事情，包括对流经 {class}`~torch.fx.Graph` 的值的运行时分析，或者通过使用 {class}`~torch.fx.Proxy` 进行重跟踪的代码变换。\n",
    "\n",
    "## 实例\n",
    "\n",
    "假设想要交换 {func}`torch.sigmoid`，{func}`torch.neg` 运算顺序（包括它们的 {class}`~torch.Tensor` 方法等量物）。可以像这样子类化 {class}`~torch.fx.Interpreter`：from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "import torch\n",
    "from torch import nn, fx\n",
    "\n",
    "\n",
    "class NegSigmSwapInterpreter(fx.Interpreter):\n",
    "    def call_function(self, target: fx.node.Target,\n",
    "                      args: Tuple, kwargs: Dict) -> Any:\n",
    "        if target == torch.sigmoid:\n",
    "            return torch.neg(*args, **kwargs)\n",
    "        return super().call_function(target, args, kwargs)\n",
    "\n",
    "    def call_method(self, target: fx.node.Target,\n",
    "                    args: Tuple, kwargs: Dict) -> Any:\n",
    "        if target == 'neg':\n",
    "            call_self, *args_tail = args\n",
    "            return call_self.sigmoid(*args_tail, **kwargs)\n",
    "        return super().call_function(target, args, kwargs)\n",
    "\n",
    "def fn(x):\n",
    "    return torch.sigmoid(x).neg()\n",
    "\n",
    "gm = fx.symbolic_trace(fn)\n",
    "inputs = torch.randn(3, 4)\n",
    "result = NegSigmSwapInterpreter(gm).run(inputs)\n",
    "torch.testing.assert_close(result, \n",
    "                           torch.neg(inputs).sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了执行运算之外，还可以通过解释器提供 {class}`~torch.fx.Proxy` 值来生成新的 {class}`~torch.fx.Graph`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FX {class}`~torch.fx.Transformer`\n",
    "\n",
    "类似地，提供 {class}`~torch.fx.Transformer` 类（一种特殊类型的 {class}`~torch.fx.Interpreter`）来包含此模式。{class}`~torch.fx.Transformer` 的行为类似于 {class}`~torch.fx.Interpreter`，但不是调用 `run` 方法从模块中获取具体的输出值，而是调用 {meth}`~torch.fx.Transformer.transform` 方法来返回新的 {class}`~torch.fx.GraphModule`，它服从于作为覆盖方法安装的任何变换规则。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegSigmSwapXformer(fx.Transformer):\n",
    "    def call_function(self, target: 'Target', \n",
    "                      args: Tuple[fx.node.Argument, ...], \n",
    "                      kwargs: Dict[str, Any]) -> Any:\n",
    "        if target == torch.sigmoid:\n",
    "            return torch.neg(*args, **kwargs)\n",
    "        return super().call_function(n)\n",
    "\n",
    "    def call_method(self, target: 'Target', \n",
    "                    args: Tuple[fx.node.Argument, ...], \n",
    "                    kwargs: Dict[str, Any]) -> Any:\n",
    "        if target == 'neg':\n",
    "            call_self, *args_tail = args\n",
    "            return call_self.sigmoid(*args_tail, **kwargs)\n",
    "        return super().call_method(n)\n",
    "\n",
    "def fn(x):\n",
    "    return torch.sigmoid(x).neg()\n",
    "\n",
    "gm = fx.symbolic_trace(fn)\n",
    "\n",
    "transformed: nn.Module = NegSigmSwapXformer(gm).transform()\n",
    "inputs = torch.randn(3, 4)\n",
    "torch.testing.assert_close(transformed(inputs), \n",
    "                           torch.neg(inputs).sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape 传播\n",
    "\n",
    "例如，假设想要运行 {class}`~torch.fx.GraphModule` 并记录 {class}`~torch.Tensor` shape 和节点上的 dtype 属性，就像我们在运行时看到的那样。它可能看起来像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeProp:\n",
    "    \"\"\"\n",
    "    Shape 传播。这个类接受 `GraphModule`。\n",
    "    然后，使用给定的参数逐个节点地执行 `GraphModule` 的 `propagate` 方法。\n",
    "    当每个运算执行时，ShapeProp 类存储每个运算的输出值 `Node` 的属性 `shape` 和 `dtype`。\n",
    "    \"\"\"\n",
    "    def __init__(self, mod):\n",
    "        self.mod = mod\n",
    "        self.graph = mod.graph\n",
    "        self.modules = dict(self.mod.named_modules())\n",
    "\n",
    "    def propagate(self, *args):\n",
    "        args_iter = iter(args)\n",
    "        env : Dict[str, Node] = {}\n",
    "\n",
    "        def load_arg(a):\n",
    "            return fx.graph.map_arg(a, lambda n: env[n.name])\n",
    "\n",
    "        def fetch_attr(target : str):\n",
    "            target_atoms = target.split('.')\n",
    "            attr_itr = self.mod\n",
    "            for i, atom in enumerate(target_atoms):\n",
    "                if not hasattr(attr_itr, atom):\n",
    "                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
    "                attr_itr = getattr(attr_itr, atom)\n",
    "            return attr_itr\n",
    "\n",
    "        for node in self.graph.nodes:\n",
    "            if node.op == 'placeholder':\n",
    "                result = next(args_iter)\n",
    "            elif node.op == 'get_attr':\n",
    "                result = fetch_attr(node.target)\n",
    "            elif node.op == 'call_function':\n",
    "                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n",
    "            elif node.op == 'call_method':\n",
    "                self_obj, *args = load_arg(node.args)\n",
    "                kwargs = load_arg(node.kwargs)\n",
    "                result = getattr(self_obj, node.target)(*args, **kwargs)\n",
    "            elif node.op == 'call_module':\n",
    "                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n",
    "                \n",
    "            # 这是唯一专门用于 shape 传播的代码。\n",
    "            # 你可以删除 `if` 分支，它就变成了通用的 GraphModule 解释器。\n",
    "            if isinstance(result, torch.Tensor):\n",
    "                node.shape = result.shape\n",
    "                node.dtype = result.dtype\n",
    "\n",
    "            env[node.name] = result\n",
    "\n",
    "        return load_arg(self.graph.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如您所看到的，完整的 FX 解释器（interpreter）并不复杂，但它可能非常有用。为了方便使用这种模式，提供了 {class}`~torch.fx.Interpreter` 类，它以一种可以通过方法重写来重写解释器执行的某些方面的方式包含了上述逻辑。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
