{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 操控 {class}`~torch.fx.Graph`\n",
    "\n",
    "构建新 {class}`~torch.fx.Graph` 的一种方法是直接操控旧图。为了帮助实现这一点，可以简单地从符号跟踪中获取 {class}`~torch.fx.Graph` 并对其进行修改。例如，假设希望用 {func}`torch.mul` 调用替换 {func}`torch.add` 调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "# 样例模块\n",
    "class M(torch.nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看节点信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x placeholder x\n",
      "y placeholder y\n",
      "add call_function <built-in method add of type object at 0x7f5bb4da7200>\n",
      "output output output\n"
     ]
    }
   ],
   "source": [
    "m = M()\n",
    "gm: torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n",
    "for node in gm.graph.nodes:\n",
    "    print(node, node.op, node.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(m: torch.nn.Module,\n",
    "              tracer_class: type = fx.Tracer) -> torch.nn.Module:\n",
    "    graph: fx.Graph = tracer_class().trace(m)\n",
    "    # FX 将其 Graph 表示为节点的有序列表，因此可以遍历它们。\n",
    "    for node in graph.nodes:\n",
    "        # 检查是否正在调用函数（例如：torch.add）\n",
    "        if node.op == 'call_function':\n",
    "            # target 属性是 call_function 调用的函数。\n",
    "            if node.target == torch.add:\n",
    "                node.target = torch.mul\n",
    "\n",
    "    graph.lint() # 做一些检查，以确保 Graph 是格式良好的。\n",
    "    return fx.GraphModule(m, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者使用更简洁的写法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name    target                                                  args    kwargs\n",
      "-------------  ------  ------------------------------------------------------  ------  --------\n",
      "placeholder    x       x                                                       ()      {}\n",
      "placeholder    y       y                                                       ()      {}\n",
      "call_function  add     <built-in method mul of type object at 0x7f5bb4da7200>  (x, y)  {}\n",
      "output         output  output                                                  (add,)  {}\n"
     ]
    }
   ],
   "source": [
    "m = M()\n",
    "traced: torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n",
    "for node in traced.graph.nodes:\n",
    "    if node.op == 'call_function':\n",
    "        # target 属性是 call_function 调用的函数。\n",
    "        if node.target == torch.add:\n",
    "            node.target = torch.mul\n",
    "traced.graph.lint() # 做一些检查，以确保 Graph 是格式良好的。\n",
    "traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以进行更复杂的 {class}`~torch.fx.Graph` 重写，比如删除或追加节点。为了帮助完成这些变换，FX 提供了变换 {class}`~torch.fx.Graph` 的实用函数。下面是使用这些 API 附加 {func}`~torch.relu` 调用的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserting_after(node, new_node=torch.relu):\n",
    "    \"\"\"指定插入点，并在此范围内添加到 Graph 中的任何节点都将插入到 `node` 之后\"\"\"\n",
    "    with traced.graph.inserting_after(node):\n",
    "        # 插入新的 `call_function` 节点调用 `torch.relu``\n",
    "        new_node = traced.graph.call_function(new_node, args=(node,))\n",
    "         \n",
    "        # 希望所有使用 `node` 值的节点后添加 `relu` 回调\n",
    "        # 使用 `replace_all_uses_with` API 来做到这一点。\n",
    "        node.replace_all_uses_with(new_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于仅由替换组成的简单变换，还可以使用 {mod}`torch.fx.subgraph_rewriter`。\n",
    "\n",
    "## {func}`~torch.fx.replace_pattern` 重写子图\n",
    "\n",
    "FX 在直接 {class}`~torch.fx.Graph` 操作的基础上还提供了另一个自动化级别。{func}`~torch.fx.replace_pattern` API 本质上是编辑 {class}`~torch.fx.Graph` 的“查找/替换”工具。它允许您指定 `pattern` 和 `replacement`，它将跟踪这些函数，在 `pattern` graph 中查找运算组的实例，并用 `replacement` graph 的副本替换这些实例。这有助于极大地自动化繁琐的 graph 操作代码，随着变换变得更加复杂，这些代码可能会变得笨拙。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match(anchor=max_1, nodes_map={output: max_1, sum_1: sum_1, cat: cat, w1: w1, w2: w2}),\n",
       " Match(anchor=max_2, nodes_map={output: max_2, sum_1: sum_2, cat: cat_1, w1: w1, w2: w2})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace, subgraph_rewriter\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, w1, w2):\n",
    "        m1 = torch.cat([w1, w2]).sum()\n",
    "        m2 = torch.cat([w1, w2]).sum()\n",
    "        return x + torch.max(m1) + torch.max(m2)\n",
    "\n",
    "def pattern(w1, w2):\n",
    "    return torch.cat([w1, w2]).sum()\n",
    "\n",
    "def replacement(w1, w2):\n",
    "    return torch.stack([w1, w2])\n",
    "\n",
    "traced_module = symbolic_trace(M())\n",
    "\n",
    "subgraph_rewriter.replace_pattern(traced_module, pattern, replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proxy/Retracing\n",
    "\n",
    "另一种操作 {class}`~torch.fx.Graph` 的方法是重用符号跟踪中使用的 {class}`~torch.fx.Proxy` 机制。例如，假设想要编写一个变换，将 PyTorch 函数分解为更小的运算。它将把每个 `F.relu(x)` 调用变换为 `(x > 0) * x`。一种可能是执行必要的 graph 重写，在 `F.relu` 之后插入比较和乘法，然后清理原来的 `F.relu`。但是，可以通过使用 {class}`~torch.fx.Proxy` 对象自动地将运算记录到 {class}`~torch.fx.Graph` 中来自动化这个过程。\n",
    "\n",
    "要使用此方法，将希望插入的运算编写为常规 PyTorch 代码，并使用 {class}`~torch.fx.Proxy` 对象作为参数调用该代码。这些代理对象将捕获对它们执行的操作，并将它们附加到 {class}`~torch.fx.Graph` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 注意，这个分解（decomposition）规则可以理解为普通的Python\n",
    "def relu_decomposition(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "decomposition_rules = {}\n",
    "decomposition_rules[F.relu] = relu_decomposition\n",
    "\n",
    "def decompose(model: torch.nn.Module,\n",
    "              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    将 `model` 分解为更小的复合运算。\n",
    "    目前，它只支持将 ReLU 分解为它的数学定义：(x > 0) * x\n",
    "    \"\"\"\n",
    "    graph : fx.Graph = tracer_class().trace(model)\n",
    "    new_graph = fx.Graph()\n",
    "    env = {}\n",
    "    tracer = fx.proxy.GraphAppendingTracer(graph)\n",
    "    for node in graph.nodes:\n",
    "        if node.op == 'call_function' and node.target in decomposition_rules:\n",
    "            # 通过使用代理包装参数，可以分派到适当的分解规则，\n",
    "            # 并通过符号跟踪隐式地将其添加到 Graph 中。\n",
    "            proxy_args = [fx.Proxy(env[x.name], tracer) \n",
    "                          if isinstance(x, fx.Node) else x for x in node.args]\n",
    "            output_proxy = decomposition_rules[node.target](*proxy_args)\n",
    "            \n",
    "            # 对 `Proxy` 的运算总是产生新的 `Proxy`，分解规则的返回值也不例外。\n",
    "            # 需要从 `Proxy` 中提取底层的 `Node`，以便在此变换的后续迭代中使用它。\n",
    "            new_node = output_proxy.node\n",
    "            env[node.name] = new_node\n",
    "        else:\n",
    "            # 默认情况：没有此节点的分解规则，所以只需要将它复制到新的 Graph 中。\n",
    "            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n",
    "            env[node.name] = new_node\n",
    "    return fx.GraphModule(model, new_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了避免显式的 {class}`~torch.fx.Graph` 操作之外，使用 {class}`~torch.fx.Proxy` 还允许将重写规则指定为原生 Python 代码。对于需要大量重写规则的变换（如 vmap 或 grad），这通常可以提高规则的可读性和可维护性。注意，在调用 {class}`~torch.fx.Proxy` 时，还传递了指向底层变量 graph 的跟踪器。如果 graph 中的操作是 n-ary 的（例如 add 是二进制算子），那么调用 {class}`~torch.fx.Proxy` 不会创建 graph 跟踪器的多个实例，这会导致意外的运行时错误。推荐这种使用 {class}`~torch.fx.Proxy` 的方法，特别是当底层算子不能被安全地假定为一元的时候。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何使用代理对象创建计算图\n",
    "\n",
    "可以直接在原始节点周围创建代理对象。这可用于创建独立于符号跟踪的 {class}`~torch.fx.Graph`。\n",
    "\n",
    "下面的代码演示了如何使用带有原始节点的代理将运算附加到新 {class}`~torch.fx.Graph`。将创建两个参数( `x` 和 `y` )，对这些参数执行一些运算，然后将创建的所有内容添加到新的 {class}`~torch.fx.Graph`中。然后将把这个 {class}`~torch.fx.Graph` 包装到 `{class}`~torch.fx.GraphModule` 中。这样做会创建  ``nn.Module`` 的可运行实例。\n",
    "\n",
    "创建独立于符号跟踪的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = fx.Graph()\n",
    "tracer = fx.proxy.GraphAppendingTracer(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建输入节点:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1 = graph.placeholder('x')\n",
    "raw2 = graph.placeholder('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用原始节点和图的默认跟踪器初始化代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fx.Proxy(raw1, tracer)\n",
    "z = fx.Proxy(raw2, tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成其他运算:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat([y, z])\n",
    "b = torch.tanh(a)\n",
    "c = torch.neg(b)\n",
    "z = torch.add(b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建新的输出节点并将其添加到图中。通过这样做，图将包含刚刚创建的所有节点(因为它们都链接到输出节点)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "output"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.output(c.node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将创建的图包装到 {class}`~torch.fx.GraphModule` 中，以获得最终的、可运行的 {class}`~torch.nn.Module` 的实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = fx.GraphModule(torch.nn.Module(), graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x, y):\n",
      "    cat = torch.cat([x, y])\n",
      "    tanh = torch.tanh(cat);  cat = None\n",
      "    neg = torch.neg(tanh);  tanh = None\n",
      "    cat_1 = torch.cat([x, y]);  x = y = None\n",
      "    tanh_1 = torch.tanh(cat_1);  cat_1 = None\n",
      "    neg_1 = torch.neg(tanh_1)\n",
      "    add = torch.add(tanh_1, neg_1);  tanh_1 = None\n",
      "    return neg_1\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(mod.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
