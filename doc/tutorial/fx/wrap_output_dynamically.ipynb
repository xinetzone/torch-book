{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态包装计算图输出\n",
    "\n",
    "下面的代码演示了如何根据运行时指定的参数更改现有的 {class}`~torch.fx.Graph`。我们将让用户从预定义的 Enum 列表中指定激活函数，然后对其进行符号跟踪。接下来，我们将从图中的最后一个运算创建 {class}`~torch.fx.Proxy`。我们将使用这个代理调用跟踪的激活函数，并将调用中的 `output` 节点插入到我们的图中。(最后一步将自动内联整个跟踪函数。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = torch.cat([x, y])\n",
    "        return y\n",
    "\n",
    "# 符号追踪 `M` 实例\n",
    "traced = fx.symbolic_trace(M())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择激活函数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(Enum):\n",
    "    RELU = auto()\n",
    "    LEAKY_RELU = auto()\n",
    "    PRELU = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将激活函数名称映射到它们的实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = {\n",
    "    ActivationFunction.RELU: torch.nn.ReLU(),\n",
    "    ActivationFunction.LEAKY_RELU: torch.nn.LeakyReLU(),\n",
    "    ActivationFunction.PRELU: torch.nn.PReLU(),\n",
    "}\n",
    "\n",
    "def wrap_in_activation_function(m: fx.GraphModule, fn: ActivationFunction) -> fx.GraphModule:\n",
    "    # Get output node\n",
    "    output_node: Node|None = None\n",
    "    for n in reversed(m.graph.nodes):\n",
    "        if n.op == \"output\":\n",
    "            output_node = n\n",
    "            break\n",
    "    assert output_node\n",
    "\n",
    "    # Get the actual output (the \"input\" of the output node). This is\n",
    "    # the Node we want to wrap in a user-specified activation function\n",
    "    assert len(output_node.all_input_nodes) == 1\n",
    "    wrap_node = output_node.all_input_nodes[0]\n",
    "\n",
    "    # Wrap the actual output in a Proxy\n",
    "    wrap_proxy = fx.Proxy(wrap_node)\n",
    "\n",
    "    # Get the implementation of the specified activation function and\n",
    "    # symbolically trace it\n",
    "    fn_impl = activation_functions[fn]\n",
    "    fn_impl_traced = fx.symbolic_trace(fn_impl)\n",
    "\n",
    "    # Call the specified activation function using the Proxy wrapper for\n",
    "    # `output_op`. The result of this call is another Proxy, which we\n",
    "    # can hook into our existing Graph.\n",
    "    with traced.graph.inserting_after(wrap_node):\n",
    "        fn_impl_output_node = fn_impl_traced(wrap_proxy)\n",
    "        new_args = (fn_impl_output_node.node,)\n",
    "        output_node.args = new_args\n",
    "\n",
    "    m.recompile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = torch.randn(5, 3), torch.randn(5, 3)\n",
    "orig_output = traced(x, y)\n",
    "\n",
    "wrap_in_activation_function(traced, ActivationFunction.LEAKY_RELU)\n",
    "new_output = traced(x, y)\n",
    "\n",
    "torch.testing.assert_close(new_output, torch.nn.LeakyReLU()(orig_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
