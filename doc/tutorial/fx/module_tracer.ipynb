{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义跟踪器\n",
    "\n",
    "## `ModulePathTracer`\n",
    "\n",
    "将定义自定义的 {class}`~torch.fx.Tracer` 实例，对于每个记录的运算，也记下该运算起源于的模块的限定名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Optional, Tuple\n",
    "import torch\n",
    "from torch import fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModulePathTracer(fx.Tracer):\n",
    "    \"\"\"\n",
    "    ModulePathTracer 是 FX 跟踪器，对于每个运算，它还记录了运算起源于的模块的限定名。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 正在跟踪的模块的当前限定名。\n",
    "    # 顶级模块由空字符串表示。\n",
    "    # 在进入 ``call_module`` 时更新，在退出 ``call_module`` 时恢复\n",
    "    current_module_qualified_name : str = ''\n",
    "    # 从 FX 节点到它起源模块的 qualname 的映射\n",
    "    # 这在记录运算时由 `create_proxy` 记录\n",
    "    node_to_originating_module : Dict[torch.fx.Node, str] = {}\n",
    "\n",
    "    def call_module(self, m: torch.nn.Module, forward: Callable[..., Any],\n",
    "                    args : Tuple[Any, ...], kwargs : Dict[str, Any]) -> Any:\n",
    "        \"\"\"\n",
    "        1. 存储调用者的限定名称以便稍后恢复\n",
    "        2. 在 `current_module_qualified_name` 中安装(install)调用者的限定名，以供 `create_proxy` 检索。\n",
    "        3. 委托到正常的 Tracer.call_module 方法\n",
    "        4. 将调用者的限定名恢复到 current_module_qualified_name 中\n",
    "        \"\"\"\n",
    "        old_qualname = self.current_module_qualified_name\n",
    "        try:\n",
    "            self.current_module_qualified_name = self.path_of_module(m)\n",
    "            return super().call_module(m, forward, args, kwargs)\n",
    "        finally:\n",
    "            self.current_module_qualified_name = old_qualname\n",
    "\n",
    "    def create_proxy(self, kind: str, target: torch.fx.node.Target, args: Tuple[Any, ...],\n",
    "                     kwargs: Dict[str, Any], name: Optional[str] = None, type_expr: Optional[Any] = None):\n",
    "        \"\"\"\n",
    "        Override of `Tracer.create_proxy`. This override intercepts the recording\n",
    "        of every operation and stores away the current traced module's qualified\n",
    "        name in `node_to_originating_module`\n",
    "        \"\"\"\n",
    "        proxy = super().create_proxy(kind, target, args, kwargs, name, type_expr)\n",
    "        self.node_to_originating_module[proxy.node] = self.current_module_qualified_name\n",
    "        return proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node x is from module \n",
      "Node conv1 is from module conv1\n",
      "Node bn1 is from module bn1\n",
      "Node relu is from module relu\n",
      "Node maxpool is from module maxpool\n",
      "Node layer1_0_conv1 is from module layer1.0.conv1\n",
      "Node layer1_0_bn1 is from module layer1.0.bn1\n",
      "Node layer1_0_relu is from module layer1.0.relu\n",
      "Node layer1_0_conv2 is from module layer1.0.conv2\n",
      "Node layer1_0_bn2 is from module layer1.0.bn2\n",
      "Node add is from module layer1.0\n",
      "Node layer1_0_relu_1 is from module layer1.0.relu\n",
      "Node layer1_1_conv1 is from module layer1.1.conv1\n",
      "Node layer1_1_bn1 is from module layer1.1.bn1\n",
      "Node layer1_1_relu is from module layer1.1.relu\n",
      "Node layer1_1_conv2 is from module layer1.1.conv2\n",
      "Node layer1_1_bn2 is from module layer1.1.bn2\n",
      "Node add_1 is from module layer1.1\n",
      "Node layer1_1_relu_1 is from module layer1.1.relu\n",
      "Node layer2_0_conv1 is from module layer2.0.conv1\n",
      "Node layer2_0_bn1 is from module layer2.0.bn1\n",
      "Node layer2_0_relu is from module layer2.0.relu\n",
      "Node layer2_0_conv2 is from module layer2.0.conv2\n",
      "Node layer2_0_bn2 is from module layer2.0.bn2\n",
      "Node layer2_0_downsample_0 is from module layer2.0.downsample.0\n",
      "Node layer2_0_downsample_1 is from module layer2.0.downsample.1\n",
      "Node add_2 is from module layer2.0\n",
      "Node layer2_0_relu_1 is from module layer2.0.relu\n",
      "Node layer2_1_conv1 is from module layer2.1.conv1\n",
      "Node layer2_1_bn1 is from module layer2.1.bn1\n",
      "Node layer2_1_relu is from module layer2.1.relu\n",
      "Node layer2_1_conv2 is from module layer2.1.conv2\n",
      "Node layer2_1_bn2 is from module layer2.1.bn2\n",
      "Node add_3 is from module layer2.1\n",
      "Node layer2_1_relu_1 is from module layer2.1.relu\n",
      "Node layer3_0_conv1 is from module layer3.0.conv1\n",
      "Node layer3_0_bn1 is from module layer3.0.bn1\n",
      "Node layer3_0_relu is from module layer3.0.relu\n",
      "Node layer3_0_conv2 is from module layer3.0.conv2\n",
      "Node layer3_0_bn2 is from module layer3.0.bn2\n",
      "Node layer3_0_downsample_0 is from module layer3.0.downsample.0\n",
      "Node layer3_0_downsample_1 is from module layer3.0.downsample.1\n",
      "Node add_4 is from module layer3.0\n",
      "Node layer3_0_relu_1 is from module layer3.0.relu\n",
      "Node layer3_1_conv1 is from module layer3.1.conv1\n",
      "Node layer3_1_bn1 is from module layer3.1.bn1\n",
      "Node layer3_1_relu is from module layer3.1.relu\n",
      "Node layer3_1_conv2 is from module layer3.1.conv2\n",
      "Node layer3_1_bn2 is from module layer3.1.bn2\n",
      "Node add_5 is from module layer3.1\n",
      "Node layer3_1_relu_1 is from module layer3.1.relu\n",
      "Node layer4_0_conv1 is from module layer4.0.conv1\n",
      "Node layer4_0_bn1 is from module layer4.0.bn1\n",
      "Node layer4_0_relu is from module layer4.0.relu\n",
      "Node layer4_0_conv2 is from module layer4.0.conv2\n",
      "Node layer4_0_bn2 is from module layer4.0.bn2\n",
      "Node layer4_0_downsample_0 is from module layer4.0.downsample.0\n",
      "Node layer4_0_downsample_1 is from module layer4.0.downsample.1\n",
      "Node add_6 is from module layer4.0\n",
      "Node layer4_0_relu_1 is from module layer4.0.relu\n",
      "Node layer4_1_conv1 is from module layer4.1.conv1\n",
      "Node layer4_1_bn1 is from module layer4.1.bn1\n",
      "Node layer4_1_relu is from module layer4.1.relu\n",
      "Node layer4_1_conv2 is from module layer4.1.conv2\n",
      "Node layer4_1_bn2 is from module layer4.1.bn2\n",
      "Node add_7 is from module layer4.1\n",
      "Node layer4_1_relu_1 is from module layer4.1.relu\n",
      "Node avgpool is from module avgpool\n",
      "Node flatten is from module \n",
      "Node fc is from module fc\n",
      "Node output is from module None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing: let's see how this works on a torchvision ResNet18 model\n",
    "import torchvision.models as models\n",
    "\n",
    "# Model under test\n",
    "rn18 = models.resnet18()\n",
    "\n",
    "# Instantiate our ModulePathTracer and use that to trace our ResNet18\n",
    "tracer = ModulePathTracer()\n",
    "traced_rn18 = tracer.trace(rn18)\n",
    "\n",
    "# Print (node, module qualified name) for every node in the Graph\n",
    "for node in traced_rn18.nodes:\n",
    "    module_qualname = tracer.node_to_originating_module.get(node)\n",
    "    print('Node', node, 'is from module', module_qualname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追踪全部的 {class}`~torch.nn.ReLU` 子模块\n",
    "\n",
    "在符号跟踪过程中，跟踪一些子模块并记录它们的组成运算;其他子模块在 IR 中显示为原子 \"call_module\" 节点。后一类中的模块称为“叶模块”。默认情况下，PyTorch 标准库({mod}`torch.nn`)中的所有模块都是叶模块。可以通过创建自定义跟踪器并重写 `is_leaf_module` 来改变这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M1(\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "class M1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x)\n",
    "\n",
    "default_traced: fx.GraphModule = fx.symbolic_trace(M1())\n",
    "default_traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode       name    target    args     kwargs\n",
      "-----------  ------  --------  -------  --------\n",
      "placeholder  x       x         ()       {}\n",
      "call_module  relu    relu      (x,)     {}\n",
      "output       output  output    (relu,)  {}\n"
     ]
    }
   ],
   "source": [
    "default_traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更改 {class}`torch.nn.ReLU` 的默认行为:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowerReluTracer(fx.Tracer):\n",
    "    def is_leaf_module(self, m: torch.nn.Module, qualname: str):\n",
    "        if isinstance(m, torch.nn.ReLU):\n",
    "            return False\n",
    "        return super().is_leaf_module(m, qualname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name    target                             args     kwargs\n",
      "-------------  ------  ---------------------------------  -------  ------------------\n",
      "placeholder    x       x                                  ()       {}\n",
      "call_function  relu    <function relu at 0x7ff559aac160>  (x,)     {'inplace': False}\n",
      "output         output  output                             (relu,)  {}\n"
     ]
    }
   ],
   "source": [
    "lower_relu_tracer = LowerReluTracer()\n",
    "custom_traced_graph: fx.Graph = lower_relu_tracer.trace(M1())\n",
    "custom_traced_graph.print_tabular()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为每个节点添加额外的属性\n",
    "\n",
    "在这里，将重写 `create_node`，以便在创建每个 Node 时向其添加新属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class M2(torch.nn.Module):\n",
    "    def forward(self, a, b):\n",
    "        return a + b\n",
    "\n",
    "class TaggingTracer(fx.Tracer):\n",
    "    def create_node(self, kind : str, target:  str | Callable,\n",
    "                    args: Tuple[Any], kwargs: Dict[str, Any], name: str | None=None,\n",
    "                    type_expr: Any | None=None) -> fx.Node:\n",
    "        n = super().create_node(kind, target, args, kwargs, name)\n",
    "        n.tag = \"foo\"\n",
    "        return n\n",
    "\n",
    "custom_traced_graph: fx.Graph = TaggingTracer().trace(M2())\n",
    "\n",
    "def assert_all_nodes_have_tags(g: fx.Graph) -> bool:\n",
    "    for n in g.nodes:\n",
    "        if not hasattr(n, \"tag\") or not n.tag == \"foo\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Prints \"True\"\n",
    "print(assert_all_nodes_have_tags(custom_traced_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内联函数到现有的 Graph\n",
    "\n",
    "您可能希望内联函数的原因是避开 FX 的默认跟踪行为。例如，除非您已经定义了自定义跟踪器，否则 ``symbolic_trace`` 的开箱即用实现将导致引用 ``torch.nn`` 模块实例的显式 `call_module` 调用，而不是被跟踪。假设这种行为几乎是你所需要的;唯一的问题是，您希望用函数的内联跟踪来替换单个模块调用。创建自定义跟踪器的工作量太大了。相反，您可以使用 **代理** 来完成此任务。\n",
    "\n",
    "下面的代码演示了如何使用 {class}`~torch.fx.Proxy` 跟踪模块并将其内联到现有的 {class}`~torch.fx.Graph` 中。我们将跟踪 Graph，然后遍历它的节点，直到找到用内联跟踪替换 `call_module` 节点的正确位置。在这一点上，我们将从节点的 `args` 和 `kwargs` 创建代理。最后，我们将调用要用那些代理替换的函数——从本质上讲，这将“跟踪”该函数。最后，我们将把调用的结果插入到我们的 {class}`~torch.fx.Graph` 中。(最后一步将自动内联函数。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x) + 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "符号跟踪 `M` 实例。跟踪后, `self.relu` 被表示为 `call_module` 节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = fx.symbolic_trace(M())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 {class}`torch.nn.ReLU` graph 中插入节点，取代原来的调用 `self.relu`.\n",
    "\n",
    "创建指向原始 Graph 的图附加跟踪程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = fx.proxy.GraphAppendingTracer(m.graph)\n",
    "for node in m.graph.nodes:\n",
    "    # Find `call_module` Node in `m` that corresponds to `self.relu`.\n",
    "    # This is the Node we want to swap out for an inlined version of the\n",
    "    # same call\n",
    "    if (node.op, node.target) == (\"call_module\", \"relu\"):\n",
    "        with m.graph.inserting_before(node):\n",
    "            # Create a Proxy from each Node in the current Node's\n",
    "            # args/kwargs\n",
    "            proxy_args = fx.map_arg(node.args, lambda n: fx.Proxy(n, tracer))\n",
    "            proxy_kwargs = fx.map_arg(node.kwargs, lambda n: fx.Proxy(n, tracer))\n",
    "            # Call `m.relu` with the newly-created Proxy arguments.\n",
    "            # `m.relu` is the generic version of the function; by\n",
    "            # calling it with Proxies created from Nodes in `m`, we're\n",
    "            # emitting Nodes that reference exiting values in the IR.\n",
    "            # The result of this call is another Proxy, which we can\n",
    "            # hook into our existing Graph to complete the function\n",
    "            # inlining.\n",
    "            proxy_output = m.relu(*proxy_args, **proxy_kwargs)\n",
    "            # Replace the relu `call_module` node with the inlined\n",
    "            # version of the function\n",
    "            node.replace_all_uses_with(proxy_output.node)\n",
    "            # Make sure that the old relu Node is erased\n",
    "            m.graph.erase_node(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FX 计算 反函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆映射是接受函数 `f(x)` 并返回函数 `g` 使 `f(g(x)) == x` 的映射。例如，由于 `log(exp(x)) == x`，所以 `exp` 和 `log` 是逆映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_mapping = {}\n",
    "def add_inverse(a, b):\n",
    "    invert_mapping[a] = b\n",
    "    invert_mapping[b] = a\n",
    "inverses = [\n",
    "    (torch.sin, torch.arcsin),\n",
    "    (torch.cos, torch.arccos),\n",
    "    (torch.tan, torch.arctan),\n",
    "    (torch.exp, torch.log),\n",
    "]\n",
    "for a, b in inverses:\n",
    "    add_inverse(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的策略是 backward walk graph，将每个节点变换为它的逆(inverse)节点。\n",
    "\n",
    "为此，我们交换函数的输出和输入，然后在 `invert_mapping` 中查找它的逆函数。注意，此变换假设所有运算只接受一个输入并返回一个输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, output):\n",
      "    log = torch.log(output);  output = None\n",
      "    arctan = torch.arctan(log);  log = None\n",
      "    return arctan\n",
      "    \n",
      "tensor([1., 2., 3., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "def invert(model: torch.nn.Module) -> torch.nn.Module:\n",
    "    fx_model = fx.symbolic_trace(model)\n",
    "    new_graph = fx.Graph()  # 建立新的 graph\n",
    "    env = {}\n",
    "    for node in reversed(fx_model.graph.nodes):\n",
    "        if node.op == 'call_function':\n",
    "            # 在新 graph 中创建具有逆函数的节点，并传递 `env[node.name]` (即之前的输出节点)作为输入。\n",
    "            new_node = new_graph.call_function(invert_mapping[node.target], \n",
    "                                               (env[node.name],))\n",
    "            env[node.args[0].name] = new_node\n",
    "        elif node.op == 'output':\n",
    "            # 将 output 转换为输入 placeholder\n",
    "            new_node = new_graph.placeholder(node.name)\n",
    "            env[node.args[0].name] = new_node\n",
    "        elif node.op == 'placeholder':\n",
    "            # 将输入 placeholder 转换为 output\n",
    "            new_graph.output(env[node.name])\n",
    "        else:\n",
    "            raise RuntimeError(\"Not implemented\")\n",
    "\n",
    "    new_graph.lint()\n",
    "    return fx.GraphModule(fx_model, new_graph)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return torch.exp(torch.tan(x))\n",
    "\n",
    "res = invert(f)\n",
    "print(res.code)\n",
    "\n",
    "print(f(res((torch.arange(5) + 1))))  # [1., 2., 3., 4, 5.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
