{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, fx\n",
    "from torch.ao.quantization.observer import HistogramObserver\n",
    "from torch_book.data.simple_vision import load_data_fashion_mnist\n",
    "from torch_book.tools import train, try_gpu\n",
    "from observer import histogram_observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, activation=nn.Sigmoid):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, padding=2)\n",
    "        self.sigmoid1 = activation()\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.sigmoid2 = activation()\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.sigmoid3 = activation()\n",
    "        self.linear2 = nn.Linear(120, 84)\n",
    "        self.sigmoid4 = activation()\n",
    "        self.linear3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 256\n",
    "# lr, num_epochs = 0.9, 320\n",
    "\n",
    "# train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)\n",
    "# train(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模块变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = LeNet()\n",
    "# mod = fx.symbolic_trace(m)\n",
    "# graph = fx.Graph()\n",
    "# new_mod = fx.GraphModule(m, graph)\n",
    "# for node in mod.graph.nodes:\n",
    "#     if node.op == 'call_module' and \"sigmoid\" in node.target:\n",
    "#         break\n",
    "# mod.graph.lint() # 做一些检查，以确保 Graph 是格式良好的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_linear(x):\n",
    "    return x * (x>0)\n",
    "\n",
    "m = LeNet()\n",
    "mod = fx.symbolic_trace(m)\n",
    "# 遍历 Graph 中全部节点\n",
    "for node in mod.graph.nodes:\n",
    "    # 如果匹配目标\n",
    "    if node.op == \"call_module\":\n",
    "        if \"sigmoid\" in node.target:\n",
    "            # 设置插入点，添加新节点，用新节点替换所有 `node` 的用法\n",
    "            with mod.graph.inserting_after(node):\n",
    "                new_node = mod.graph.call_function(sigmoid_linear, node.args, node.kwargs)\n",
    "                node.replace_all_uses_with(new_node)\n",
    "            # 移除 graph 中旧的节点\n",
    "            mod.graph.erase_node(node)\n",
    "# 不用忘记 recompile!\n",
    "new_code = mod.recompile()\n",
    "new_mod = fx.GraphModule(m, mod.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (sigmoid1): Sigmoid()\n",
      "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (sigmoid2): Sigmoid()\n",
      "  (pool2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (sigmoid3): Sigmoid()\n",
      "  (linear2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (sigmoid4): Sigmoid()\n",
      "  (linear3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    sigmoid_linear = __main___sigmoid_linear(conv1);  conv1 = None\n",
      "    pool1 = self.pool1(sigmoid_linear);  sigmoid_linear = None\n",
      "    conv2 = self.conv2(pool1);  pool1 = None\n",
      "    sigmoid_linear_1 = __main___sigmoid_linear(conv2);  conv2 = None\n",
      "    pool2 = self.pool2(sigmoid_linear_1);  sigmoid_linear_1 = None\n",
      "    flatten = self.flatten(pool2);  pool2 = None\n",
      "    linear1 = self.linear1(flatten);  flatten = None\n",
      "    sigmoid_linear_2 = __main___sigmoid_linear(linear1);  linear1 = None\n",
      "    linear2 = self.linear2(sigmoid_linear_2);  sigmoid_linear_2 = None\n",
      "    sigmoid_linear_3 = __main___sigmoid_linear(linear2);  linear2 = None\n",
      "    linear3 = self.linear3(sigmoid_linear_3);  sigmoid_linear_3 = None\n",
      "    return linear3\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_linear(x):\n",
    "    return x * (x>0)\n",
    "\n",
    "env = {}\n",
    "# xs = torch.tensor([-6, -5, -3, -2, -1, 0, 1, 2, 3, 5, 6])\n",
    "# ys = torch.sigmoid(xs)\n",
    "new_graph = fx.Graph()\n",
    "decomposition_rules = {\"sigmoid\": sigmoid_linear}\n",
    "\n",
    "m = LeNet()\n",
    "mod = fx.symbolic_trace(m)\n",
    "graph = mod.graph\n",
    "tracer = fx.proxy.GraphAppendingTracer(graph)\n",
    "\n",
    "for node in graph.nodes:\n",
    "    if node.op == 'call_module' and \"sigmoid\" in node.target:\n",
    "        print(node)\n",
    "        # 通过使用代理包装参数，可以分派到适当的分解规则，\n",
    "        # 并通过符号跟踪隐式地将其添加到 Graph 中。\n",
    "        proxy_args = [fx.Proxy(env[x.name], tracer) \n",
    "                      if isinstance(x, fx.Node) else x for x in node.args]\n",
    "        output_proxy = decomposition_rules[\"sigmoid\"](*proxy_args)\n",
    "            \n",
    "        # 对 `Proxy` 的运算总是产生新的 `Proxy`，分解规则的返回值也不例外。\n",
    "        # 需要从 `Proxy` 中提取底层的 `Node`，以便在此变换的后续迭代中使用它。\n",
    "        new_node = output_proxy.node\n",
    "        env[node.target] = new_node\n",
    "        # break\n",
    "    else:\n",
    "        # 默认情况：没有此节点的分解规则，所以只需要将它复制到新的 Graph 中。\n",
    "        new_node = new_graph.node_copy(node, lambda x: env[x.name])\n",
    "        env[node.name] = new_node\n",
    "        \n",
    "new_mod = fx.GraphModule(m, new_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_mod.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
