{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 量化设计方案\n",
    "\n",
    "参考：[torch_quantization_design_proposal](https://github.com/pytorch/pytorch/wiki/torch_quantization_design_proposal)\n",
    "\n",
    "## 量化的数学描述\n",
    "\n",
    "定义缩放函数（scaling function） $sc: \\mathbb{R}^n \\to \\mathbf{Q}^n$，将来自任意范围的值归一化到 $[\\mathbf{Q}_{\\min}, \\mathbf{Q}_{\\max}]$。给定这样的函数，量化函数 {eq}`Q0`。\n",
    "\n",
    "其中 $\\mathbf{Q}_{\\min}$, $\\mathbf{Q}_{\\max}$ 表示量化级别的数据类型的最大最小值。比如，对于对称量化，int8 对应 $-128$，$127$。\n",
    "\n",
    "## 量化的表示\n",
    "\n",
    "方案：逐张量（per tensor） **非对称线性量化** （asymmetric linear quantization），这意味着张量内的所有值都以相同的方式缩放，输入数据的最小值和最大值线性映射到量化数据类型的最小值和最大值，从而表示零，没有量化误差。\n",
    "\n",
    "映射是通过\n",
    "\n",
    "$$\n",
    "Q(x, \\text{scale}, \\text{zero_point}) = \\operatorname{round}(\\cfrac{x}{\\text{scale}} + \\text{zero_point})\n",
    "$$\n",
    "\n",
    "```{tip}\n",
    "$\\text{zero_point}$ 确保浮点数中的 $0$ 在量化后没有错误表示，从而确保 padding 等运算不会导致额外的量化误差。\n",
    "```\n",
    "\n",
    "变换浮点张量的。\n",
    "\n",
    "```{note}\n",
    "对于算子，限制为：\n",
    "\n",
    "1. 8 位权重（`data_type = qint8`）\n",
    "2. 8 位激活（`data_type = quint8`）\n",
    "3. 32 位，对称量化 bias（`zero_point = 0, data_type = qint32`）\n",
    "```\n",
    "\n",
    "(QuantizedTensor)=\n",
    "## 量化张量\n",
    "\n",
    "参考：[量化张量](https://github.com/pytorch/pytorch/wiki/Introducing-Quantized-Tensor)\n",
    "\n",
    "为了在 PyTorch 中进行量化，需要能够在张量中表示量化的数据。量化张量允许存储量化数据（表示为 int8/uint8/int32）以及量化参数，如 `scale` 和 `zero_point`。量化张量允许许多有用的算子，使得量化算术变得容易，此外还允许以量化格式序列化数据。\n",
    "\n",
    "关于量化张量的有用函数的简短列表：\n",
    "\n",
    "- torch.quantize_linear(x: torch.tensor, scale: float, zero_point: int, dtype: torch.dtype)：使用 `scale` 和 `zero_point` 将输入 `x` 量化为 `dtype`\n",
    "- `qx.dequantize()`：将张量 `qx` 反量化为浮点数\n",
    "- `qx.__repr__()`：打印量化张量 `qx` 的反量化值\n",
    "- `qx.int_repr()`：打印量化张量 `qx` 中的原始值\n",
    "\n",
    "### 创建量化张量\n",
    "\n",
    "- 通过量化非量化的浮点张量得到量化张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2883, -0.4246, -1.7423],\n",
       "         [-0.4073,  0.4799, -0.6273]],\n",
       "\n",
       "        [[-0.1805,  0.8215, -1.5591],\n",
       "         [-1.7428, -0.6705,  0.0260]]], size=(2, 2, 3), dtype=torch.qint32,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.0001, zero_point=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "float_tensor = torch.randn(2, 2, 3)\n",
    "\n",
    "scale, zero_point = 1e-4, 2\n",
    "dtype = torch.qint32\n",
    "q_per_tensor = torch.quantize_per_tensor(float_tensor, scale, zero_point, dtype)\n",
    "q_per_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还支持逐通道量化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3000, -0.4200, -1.7420],\n",
       "         [-0.4000,  0.4800, -0.6270]],\n",
       "\n",
       "        [[-0.2000,  0.8200, -1.5590],\n",
       "         [-1.7000, -0.6700,  0.0260]]], size=(2, 2, 3), dtype=torch.qint32,\n",
       "       quantization_scheme=torch.per_channel_affine,\n",
       "       scale=tensor([0.1000, 0.0100, 0.0010], dtype=torch.float64),\n",
       "       zero_point=tensor([-1,  0,  1]), axis=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales = torch.tensor([1e-1, 1e-2, 1e-3])\n",
    "zero_points = torch.tensor([-1, 0, 1])\n",
    "channel_axis = 2\n",
    "q_per_channel = torch.quantize_per_channel(float_tensor,\n",
    "                                           scales,\n",
    "                                           zero_points,\n",
    "                                           axis=channel_axis,\n",
    "                                           dtype=dtype)\n",
    "q_per_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 直接从 `empty_quantized` 函数创建量化张量\n",
    "\n",
    "注意，`_empty_affine_quantized` 是一个私有 API，我们将用类似 torch 的方式替换它。将来使用 `empty_quantized_tensor(sizes, quantizer)`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000e-04, -3.0000e-04, -2.0000e-04,  2.0000e-04,  4.4000e-03,\n",
       "        -2.0000e-04,  7.9000e-03, -2.0000e-04, -1.3377e+05,  3.2649e+00],\n",
       "       size=(10,), dtype=torch.qint32,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.0001, zero_point=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch._empty_affine_quantized([10],\n",
    "                                  scale=scale,\n",
    "                                  zero_point=zero_point,\n",
    "                                  dtype=dtype)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过集合 int 张量和量化参数来创建量化张量\n",
    "\n",
    "```{note}\n",
    "注意，`_per_tensor_affine_qtensor` 是私有 API，我们将用类似 torch 的东西 `torch.form_tensor(int_tensor, quantizer)` 替换它\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_tensor = torch.randint(0, 100, size=(10,), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据类型为 `torch.quint8`，即对应的 `torch.uint8`，我们有以下对应的 torch int 类型和 torch 量化 int 类型：\n",
    "\n",
    "- `torch.uint8` -> `torch.quint8`\n",
    "- `torch.int8` -> `torch.qint8`\n",
    "- `torch.int32` -> `torch.qint32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0055, 0.0067, 0.0039, 0.0042, 0.0077, 0.0003, 0.0036, 0.0007, 0.0090,\n",
       "        0.0022], size=(10,), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.0001, zero_point=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch._make_per_tensor_quantized_tensor(int_tensor, scale, zero_point)  # Note no `dtype`\n",
    "q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在当前的 API 中，我们必须专一每个量化方案的函数，例如，如果我们想量化张量，我们将有 `quantize_per_tensor` 和 `quantize_per_channel`。类似地，对于 `q_scale` 和 `q_zero_point`，我们应该有以 `Quantizer` 作为参数的单一量化函数。为了检查量化参数，我们应该让量化张量返回 `Quantizer` 对象，这样我们就可以在 `Quantizer` 对象上检查量化参数，而不是把所有东西都放到张量 API 中。当前的基础设施还没有为这种支持做好准备，目前正在开发中。\n",
    "\n",
    "### 量化张量的运算\n",
    "\n",
    "```{rubric} 反量化\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0055, 0.0067, 0.0039, 0.0042, 0.0077, 0.0003, 0.0036, 0.0007, 0.0090,\n",
       "        0.0022])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dequantized_tensor = q.dequantize()\n",
    "dequantized_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 支持切片\n",
    "```\n",
    "\n",
    "量化张量像通常的张量一样支持切片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039, size=(), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.0001, zero_point=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = q[2]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "尺度（scale）和零点（zero_point）相同的量化张量，它包含与 `q_made_per_tensor[2, :]` 相同的原始量化张量的第二行值。\n",
    "```\n",
    "\n",
    "```{rubric} 赋值\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q[0] = 3.5 # 量化 3.5 并将 int 值存储在量化张量中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 拷贝\n",
    "```\n",
    "\n",
    "我们可以从量化张量复制相同大小和 dtype 但不同尺度和零点的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6000,  2.6000,  2.4000],\n",
       "        [ 5.8000, -8.8000,  8.5000]], size=(2, 3), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale1, zero_point1 = 1e-1, 0\n",
    "scale2, zero_point2 = 1, -1\n",
    "q1 = torch._empty_affine_quantized([2, 3],\n",
    "                                   scale=scale1,\n",
    "                                   zero_point=zero_point1,\n",
    "                                   dtype=torch.qint8)\n",
    "q2 = torch._empty_affine_quantized([2, 3],\n",
    "                                   scale=scale2,\n",
    "                                   zero_point=zero_point2,\n",
    "                                   dtype=torch.qint8)\n",
    "q2.copy_(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6000,  2.6000,  2.4000],\n",
       "        [ 5.8000, -8.8000,  8.5000]], size=(2, 3), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.transpose(0, 1)  # see https://pytorch.org/docs/stable/torch.html#torch.transpose\n",
    "q1.permute([1, 0])  # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute\n",
    "q1.contiguous()  # Convert to contiguous Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile() as f:\n",
    "    torch.save(q2, f)\n",
    "    f.seek(0)\n",
    "    q3 = torch.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查量化张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, torch.Size([10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check size of Tensor\n",
    "q.numel(), q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether the tensor is quantized\n",
    "q.is_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the scale of the quantized Tensor, only works for affine quantized tensor\n",
    "q.q_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the zero_point of quantized Tensor\n",
    "q.q_zero_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([255,  69,  41,  44,  79,   5,  38,   9,  92,  24], dtype=torch.uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the underlying integer representation of the quantized Tensor\n",
    "# int_repr() returns a Tensor of the corresponding data type of the quantized data type\n",
    "# e.g.for quint8 Tensor it returns a uint8 Tensor while preserving the MemoryFormat when possible\n",
    "q.int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If a quantized Tensor is a scalar we can print the value:\n",
    "# item() will dequantize the current tensor and return a Scalar of float\n",
    "q[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0253, 0.0067, 0.0039, 0.0042, 0.0077, 0.0003, 0.0036, 0.0007, 0.0090,\n",
      "        0.0022], size=(10,), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0001, zero_point=2)\n"
     ]
    }
   ],
   "source": [
    "# printing\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0253, size=(), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.0001, zero_point=2)\n"
     ]
    }
   ],
   "source": [
    "# indexing\n",
    "print(q[0]) # q[0] is a quantized Tensor with one value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化的算子/内核\n",
    "\n",
    "量化算子，如量化 `QRelu`、`QAdd`、`QCat`、`QLinear`、`QConv` 等。要么使用简单的算子实现，要么在算子符中封装 fbgemm 实现。所有的运算都是在 C10 中注册的，而且现在只在 CPU 中。也有关于[如何写量化算子/内核的说明](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/README.md)。\n",
    "\n",
    "## 量化模型\n",
    "\n",
    "还有量化的模块，它们封装了这些内核实现，这些内核实现位于 `torch.nn.quantized` 命名空间中，将在模型开发中使用。提供实用函数来将 `torch.nn.Module` 替换为 `torch.nn.quantized.Module`，但用户也可以自由地直接使用它们。尽量将量化模块的 api 与 `torch.nn.Module` 中的对应 api 匹配。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
