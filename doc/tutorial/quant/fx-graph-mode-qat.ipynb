{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch FX QAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import copy\n",
    "\n",
    "model_fp = UserModel()\n",
    "\n",
    "#\n",
    "# post training dynamic/weight_only quantization\n",
    "#\n",
    "\n",
    "# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "model_to_quantize.eval()\n",
    "qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n",
    "# a tuple of one or more example inputs are needed to trace the model\n",
    "example_inputs = (input_fp32)\n",
    "# prepare\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "# no calibration needed when we only have dynamic/weight_only quantization\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "#\n",
    "# post training static quantization\n",
    "#\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n",
    "model_to_quantize.eval()\n",
    "# prepare\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "# calibrate (not shown)\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "#\n",
    "# quantization aware training for static quantization\n",
    "#\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "qconfig_mapping = get_default_qat_qconfig_mapping(\"qnnpack\")\n",
    "model_to_quantize.train()\n",
    "# prepare\n",
    "model_prepared = quantize_fx.prepare_qat_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "# training loop (not shown)\n",
    "# quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "#\n",
    "# fusion\n",
    "#\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "model_fused = quantize_fx.fuse_fx(model_to_quantize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
