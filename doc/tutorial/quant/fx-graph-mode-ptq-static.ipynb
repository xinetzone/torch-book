{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FX 图模式后训练静态量化\n",
    "\n",
    "[教程](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html) 介绍了基于 {mod}`torch.fx` 在图模式下进行静态量化的步骤。\n",
    "\n",
    "FX 图模式量化的优点：可以完全自动地对模型进行量化。\n",
    "\n",
    "````{admonition} 大致流程\n",
    "```python\n",
    "import torch\n",
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping\n",
    "float_model.eval()\n",
    "# 旧的 'fbgemm' 仍然可用，但 'x86' 是推荐的默认值。\n",
    "qconfig = get_default_qconfig(\"x86\")\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            model(image)\n",
    "example_inputs = (next(iter(data_loader))[0]) # 获取样本输入\n",
    "prepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)  # 融合模块并插入观测器\n",
    "calibrate(prepared_model, data_loader_test)  # 在样本数据上进行校准\n",
    "quantized_model = convert_fx(prepared_model)  # 将校准模型转换为量化模型\n",
    "```\n",
    "````\n",
    "\n",
    "## FX 图模式量化的动机\n",
    "\n",
    "目前，PyTorch 只有 eager 模式量化作为替代：[静态量化](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)。\n",
    "\n",
    "eager 模式量化过程中涉及到多个手动步骤，包括：\n",
    "\n",
    "- 显式量化和反量化激活——当模型中时同时存在浮点运算和量化运算，这很耗时。\n",
    "- 显式地融合模块——这需要手动识别卷积序列、batch norms 和 relus 以及其他融合模式。\n",
    "- Pytorch 张量运算（如 `add`，`concat` 等）需要特殊处理。\n",
    "- 函数没有 first class 的支持（`functional.conv2d` 和 `functional.linear` 不会被量化）\n",
    "\n",
    "这些需要修改的大部分都来自 eager 模式量化的潜在局限性。Eager 模式工作在模块级别，因为它不能检查实际运行的代码（{func}`forward` 函数），量化是通过模块交换来实现的，不知道 {func}`forward` 函数中的模块是如何使用的，所以它需要用户手动插入 QuantStub 和 DeQuantStub 来标记想要量化或反量化的点。在图模式中，可以检查在 {func}`forward` 函数中执行的实际代码（例如，{func}`aten` 函数调用），量化是通过模块和图操作实现的。由于图模式对正在运行的代码具有完全可见性，这样能够自动地找出哪些模块需要融合，在哪里插入观测器调用，量化/反量化函数等，能够自动化整个量化过程。\n",
    "\n",
    "FX 图模式量化的优点是：\n",
    "\n",
    "- 简化量化流程，最小的手工步骤\n",
    "- 解锁了进行更高级别优化的可能性，如自动精度选择\n",
    "\n",
    "## 定义辅助函数并准备数据集"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义辅助函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.ao.quantization import get_default_qconfig, QConfigMapping\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, fuse_fx\n",
    "from torch import nn\n",
    "\n",
    "# 为可重复的结果指定随机种子\n",
    "_ = torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AverageMeter:\n",
    "    \"\"\"计算并存储平均值和当前值\"\"\"\n",
    "    name: str\n",
    "    fmt: str = \":f\"\n",
    "    def __post_init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            # criterion(output, target) # 计算损失\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "    return top1, top5\n",
    "\n",
    "def size_of_model(model):\n",
    "    \"\"\"返回模型大小\"\"\"\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size = os.path.getsize('temp.p')\n",
    "    os.remove(\"temp.p\")\n",
    "    return size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from torchvision.datasets.folder import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    root: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.root = Path(self.root)\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    @property\n",
    "    def trainset(self):\n",
    "        return ImageFolder(self.root/\"train\", transform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize]))\n",
    "\n",
    "    @property\n",
    "    def testset(self):\n",
    "        return ImageFolder(self.root/\"val\", transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize]))\n",
    "\n",
    "    def trainloader(self, batch_size):\n",
    "        train_sampler = torch.utils.data.RandomSampler(self.trainset)\n",
    "        return torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    def testloader(self, batch_size):\n",
    "        test_sampler = torch.utils.data.SequentialSampler(self.testset)\n",
    "        return torch.utils.data.DataLoader(self.testset, batch_size=batch_size, sampler=test_sampler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备浮点模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "saved_model_dir = 'data/' # 模型存储路径\n",
    "train_batch_size = 30 # 训练样本批量大小\n",
    "eval_batch_size = 50 # 测试样本批量大小\n",
    "dataset = Dataset('/media/pc/data/lxw/home/data/datasets/ILSVRC')\n",
    "data_loader = dataset.trainloader(train_batch_size)\n",
    "data_loader_test = dataset.testloader(eval_batch_size)\n",
    "example_inputs = (next(iter(data_loader))[0])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "float_model = float_model.to(\"cpu\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度复制模型，因为需要保持原始模型\n",
    "model_to_quantize = deepcopy(float_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置模型为评估模式\n",
    "\n",
    "对于训练后量化，需要将模型设置为评估模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_quantize.eval();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 QConfigMapping 指定如何量化模型\n",
    "\n",
    "```python\n",
    "qconfig_mapping = QConfigMapping.set_global(default_qconfig)\n",
    "```\n",
    "\n",
    "使用与 eager 模式量化相同的 `qconfig`, `qconfig` 只是用于激活和权重的观测器的命名元组。`QConfigMapping` 包含 `ops` 到 `qconfigs` 的映射信息："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model_to_quantize.eval()\n",
    "qconfig_mapping = (QConfigMapping()\n",
    "    .set_global(qconfig_opt)  # qconfig_opt 是可选的 qconfig，可以是有效的 qconfig，也可以是 None\n",
    "    .set_object_type(torch.nn.Conv2d, qconfig_opt) # 可以是可调用的\n",
    "    .set_object_type(\"torch.nn.functional.add\", qconfig_opt) # 或者类名的字符串\n",
    "    .set_module_name_regex(\"foo.*bar.*conv[0-9]+\", qconfig_opt) # 按顺序匹配，第一个匹配优先\n",
    "    .set_module_name(\"foo.bar\", qconfig_opt)\n",
    "    .set_module_name_object_type_order()\n",
    ")\n",
    "```\n",
    "```{note}\n",
    "- 优先级（按递增顺序）：global、object_type、module_name_regex、module_name\n",
    "- `qconfig == None` 表示任何事情都应该跳过融合和量化\n",
    "- 匹配规则（除非找到更高优先级的匹配）\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与 `qconfig` 相关的实用函数可以在 [`qconfig`](https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/qconfig.py) 文件中找到，而与 `QConfigMapping` 相关的实用函数可以在 [`qconfig_mapping`](https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/fx/qconfig_mapping.py) 中找到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 旧的 'fbgemm' 仍然可用，但 'x86' 是推荐的默认值。\n",
    "qconfig = get_default_qconfig(\"x86\")\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为训练后静态量化准备模型\n",
    "\n",
    "```python\n",
    "prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "```\n",
    "\n",
    "`prepare_fx` 将 `BatchNorm` 模块折叠到其前面的 Conv2d 模块中，并在模型的适当位置插入观测器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : torch.Tensor [#users=1] = placeholder[target=x]\n",
      "    %activation_post_process_0 : [#users=1] = call_module[target=activation_post_process_0](args = (%x,), kwargs = {})\n",
      "    %conv1 : [#users=1] = call_module[target=conv1](args = (%activation_post_process_0,), kwargs = {})\n",
      "    %activation_post_process_1 : [#users=1] = call_module[target=activation_post_process_1](args = (%conv1,), kwargs = {})\n",
      "    %maxpool : [#users=1] = call_module[target=maxpool](args = (%activation_post_process_1,), kwargs = {})\n",
      "    %activation_post_process_2 : [#users=2] = call_module[target=activation_post_process_2](args = (%maxpool,), kwargs = {})\n",
      "    %layer1_0_conv1 : [#users=1] = call_module[target=layer1.0.conv1](args = (%activation_post_process_2,), kwargs = {})\n",
      "    %activation_post_process_3 : [#users=1] = call_module[target=activation_post_process_3](args = (%layer1_0_conv1,), kwargs = {})\n",
      "    %layer1_0_conv2 : [#users=1] = call_module[target=layer1.0.conv2](args = (%activation_post_process_3,), kwargs = {})\n",
      "    %activation_post_process_4 : [#users=1] = call_module[target=activation_post_process_4](args = (%layer1_0_conv2,), kwargs = {})\n",
      "    %add : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_4, %activation_post_process_2), kwargs = {})\n",
      "    %layer1_0_relu_1 : [#users=1] = call_module[target=layer1.0.relu](args = (%add,), kwargs = {})\n",
      "    %activation_post_process_5 : [#users=2] = call_module[target=activation_post_process_5](args = (%layer1_0_relu_1,), kwargs = {})\n",
      "    %layer1_1_conv1 : [#users=1] = call_module[target=layer1.1.conv1](args = (%activation_post_process_5,), kwargs = {})\n",
      "    %activation_post_process_6 : [#users=1] = call_module[target=activation_post_process_6](args = (%layer1_1_conv1,), kwargs = {})\n",
      "    %layer1_1_conv2 : [#users=1] = call_module[target=layer1.1.conv2](args = (%activation_post_process_6,), kwargs = {})\n",
      "    %activation_post_process_7 : [#users=1] = call_module[target=activation_post_process_7](args = (%layer1_1_conv2,), kwargs = {})\n",
      "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_7, %activation_post_process_5), kwargs = {})\n",
      "    %layer1_1_relu_1 : [#users=1] = call_module[target=layer1.1.relu](args = (%add_1,), kwargs = {})\n",
      "    %activation_post_process_8 : [#users=2] = call_module[target=activation_post_process_8](args = (%layer1_1_relu_1,), kwargs = {})\n",
      "    %layer2_0_conv1 : [#users=1] = call_module[target=layer2.0.conv1](args = (%activation_post_process_8,), kwargs = {})\n",
      "    %activation_post_process_9 : [#users=1] = call_module[target=activation_post_process_9](args = (%layer2_0_conv1,), kwargs = {})\n",
      "    %layer2_0_conv2 : [#users=1] = call_module[target=layer2.0.conv2](args = (%activation_post_process_9,), kwargs = {})\n",
      "    %activation_post_process_10 : [#users=1] = call_module[target=activation_post_process_10](args = (%layer2_0_conv2,), kwargs = {})\n",
      "    %layer2_0_downsample_0 : [#users=1] = call_module[target=layer2.0.downsample.0](args = (%activation_post_process_8,), kwargs = {})\n",
      "    %activation_post_process_11 : [#users=1] = call_module[target=activation_post_process_11](args = (%layer2_0_downsample_0,), kwargs = {})\n",
      "    %add_2 : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_10, %activation_post_process_11), kwargs = {})\n",
      "    %layer2_0_relu_1 : [#users=1] = call_module[target=layer2.0.relu](args = (%add_2,), kwargs = {})\n",
      "    %activation_post_process_12 : [#users=2] = call_module[target=activation_post_process_12](args = (%layer2_0_relu_1,), kwargs = {})\n",
      "    %layer2_1_conv1 : [#users=1] = call_module[target=layer2.1.conv1](args = (%activation_post_process_12,), kwargs = {})\n",
      "    %activation_post_process_13 : [#users=1] = call_module[target=activation_post_process_13](args = (%layer2_1_conv1,), kwargs = {})\n",
      "    %layer2_1_conv2 : [#users=1] = call_module[target=layer2.1.conv2](args = (%activation_post_process_13,), kwargs = {})\n",
      "    %activation_post_process_14 : [#users=1] = call_module[target=activation_post_process_14](args = (%layer2_1_conv2,), kwargs = {})\n",
      "    %add_3 : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_14, %activation_post_process_12), kwargs = {})\n",
      "    %layer2_1_relu_1 : [#users=1] = call_module[target=layer2.1.relu](args = (%add_3,), kwargs = {})\n",
      "    %activation_post_process_15 : [#users=2] = call_module[target=activation_post_process_15](args = (%layer2_1_relu_1,), kwargs = {})\n",
      "    %layer3_0_conv1 : [#users=1] = call_module[target=layer3.0.conv1](args = (%activation_post_process_15,), kwargs = {})\n",
      "    %activation_post_process_16 : [#users=1] = call_module[target=activation_post_process_16](args = (%layer3_0_conv1,), kwargs = {})\n",
      "    %layer3_0_conv2 : [#users=1] = call_module[target=layer3.0.conv2](args = (%activation_post_process_16,), kwargs = {})\n",
      "    %activation_post_process_17 : [#users=1] = call_module[target=activation_post_process_17](args = (%layer3_0_conv2,), kwargs = {})\n",
      "    %layer3_0_downsample_0 : [#users=1] = call_module[target=layer3.0.downsample.0](args = (%activation_post_process_15,), kwargs = {})\n",
      "    %activation_post_process_18 : [#users=1] = call_module[target=activation_post_process_18](args = (%layer3_0_downsample_0,), kwargs = {})\n",
      "    %add_4 : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_17, %activation_post_process_18), kwargs = {})\n",
      "    %layer3_0_relu_1 : [#users=1] = call_module[target=layer3.0.relu](args = (%add_4,), kwargs = {})\n",
      "    %activation_post_process_19 : [#users=2] = call_module[target=activation_post_process_19](args = (%layer3_0_relu_1,), kwargs = {})\n",
      "    %layer3_1_conv1 : [#users=1] = call_module[target=layer3.1.conv1](args = (%activation_post_process_19,), kwargs = {})\n",
      "    %activation_post_process_20 : [#users=1] = call_module[target=activation_post_process_20](args = (%layer3_1_conv1,), kwargs = {})\n",
      "    %layer3_1_conv2 : [#users=1] = call_module[target=layer3.1.conv2](args = (%activation_post_process_20,), kwargs = {})\n",
      "    %activation_post_process_21 : [#users=1] = call_module[target=activation_post_process_21](args = (%layer3_1_conv2,), kwargs = {})\n",
      "    %add_5 : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_21, %activation_post_process_19), kwargs = {})\n",
      "    %layer3_1_relu_1 : [#users=1] = call_module[target=layer3.1.relu](args = (%add_5,), kwargs = {})\n",
      "    %activation_post_process_22 : [#users=2] = call_module[target=activation_post_process_22](args = (%layer3_1_relu_1,), kwargs = {})\n",
      "    %layer4_0_conv1 : [#users=1] = call_module[target=layer4.0.conv1](args = (%activation_post_process_22,), kwargs = {})\n",
      "    %activation_post_process_23 : [#users=1] = call_module[target=activation_post_process_23](args = (%layer4_0_conv1,), kwargs = {})\n",
      "    %layer4_0_conv2 : [#users=1] = call_module[target=layer4.0.conv2](args = (%activation_post_process_23,), kwargs = {})\n",
      "    %activation_post_process_24 : [#users=1] = call_module[target=activation_post_process_24](args = (%layer4_0_conv2,), kwargs = {})\n",
      "    %layer4_0_downsample_0 : [#users=1] = call_module[target=layer4.0.downsample.0](args = (%activation_post_process_22,), kwargs = {})\n",
      "    %activation_post_process_25 : [#users=1] = call_module[target=activation_post_process_25](args = (%layer4_0_downsample_0,), kwargs = {})\n",
      "    %add_6 : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_24, %activation_post_process_25), kwargs = {})\n",
      "    %layer4_0_relu_1 : [#users=1] = call_module[target=layer4.0.relu](args = (%add_6,), kwargs = {})\n",
      "    %activation_post_process_26 : [#users=2] = call_module[target=activation_post_process_26](args = (%layer4_0_relu_1,), kwargs = {})\n",
      "    %layer4_1_conv1 : [#users=1] = call_module[target=layer4.1.conv1](args = (%activation_post_process_26,), kwargs = {})\n",
      "    %activation_post_process_27 : [#users=1] = call_module[target=activation_post_process_27](args = (%layer4_1_conv1,), kwargs = {})\n",
      "    %layer4_1_conv2 : [#users=1] = call_module[target=layer4.1.conv2](args = (%activation_post_process_27,), kwargs = {})\n",
      "    %activation_post_process_28 : [#users=1] = call_module[target=activation_post_process_28](args = (%layer4_1_conv2,), kwargs = {})\n",
      "    %add_7 : [#users=1] = call_function[target=operator.add](args = (%activation_post_process_28, %activation_post_process_26), kwargs = {})\n",
      "    %layer4_1_relu_1 : [#users=1] = call_module[target=layer4.1.relu](args = (%add_7,), kwargs = {})\n",
      "    %activation_post_process_29 : [#users=1] = call_module[target=activation_post_process_29](args = (%layer4_1_relu_1,), kwargs = {})\n",
      "    %avgpool : [#users=1] = call_module[target=avgpool](args = (%activation_post_process_29,), kwargs = {})\n",
      "    %activation_post_process_30 : [#users=1] = call_module[target=activation_post_process_30](args = (%avgpool,), kwargs = {})\n",
      "    %flatten : [#users=1] = call_function[target=torch.flatten](args = (%activation_post_process_30, 1), kwargs = {})\n",
      "    %activation_post_process_31 : [#users=1] = call_module[target=activation_post_process_31](args = (%flatten,), kwargs = {})\n",
      "    %fc : [#users=1] = call_module[target=fc](args = (%activation_post_process_31,), kwargs = {})\n",
      "    %activation_post_process_32 : [#users=1] = call_module[target=activation_post_process_32](args = (%fc,), kwargs = {})\n",
      "    return activation_post_process_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "print(prepared_model.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 校准\n",
    "\n",
    "在将观测器插入模型后运行校准函数。校准的目的是运行一些代表工作负载的样本示例（例如训练数据集的样本），以便模型中的观测器能够观察到张量的统计信息，然后使用这些信息来计算量化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(model, data_loader, num=200):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for k, (image, _) in enumerate(data_loader):\n",
    "            if k > num:\n",
    "                break\n",
    "            model(image)\n",
    "calibrate(prepared_model, data_loader)  # 在样本数据上运行校准"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将模型转换为量化模型\n",
    "\n",
    "`convert_fx` 采用校准模型并生成量化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : torch.Tensor [#users=1] = placeholder[target=x]\n",
      "    %conv1_input_scale_0 : [#users=1] = get_attr[target=conv1_input_scale_0]\n",
      "    %conv1_input_zero_point_0 : [#users=1] = get_attr[target=conv1_input_zero_point_0]\n",
      "    %quantize_per_tensor : [#users=1] = call_function[target=torch.quantize_per_tensor](args = (%x, %conv1_input_scale_0, %conv1_input_zero_point_0, torch.quint8), kwargs = {})\n",
      "    %conv1 : [#users=1] = call_module[target=conv1](args = (%quantize_per_tensor,), kwargs = {})\n",
      "    %maxpool : [#users=2] = call_module[target=maxpool](args = (%conv1,), kwargs = {})\n",
      "    %layer1_0_conv1 : [#users=1] = call_module[target=layer1.0.conv1](args = (%maxpool,), kwargs = {})\n",
      "    %layer1_0_conv2 : [#users=1] = call_module[target=layer1.0.conv2](args = (%layer1_0_conv1,), kwargs = {})\n",
      "    %layer1_0_relu_scale_0 : [#users=1] = get_attr[target=layer1_0_relu_scale_0]\n",
      "    %layer1_0_relu_zero_point_0 : [#users=1] = get_attr[target=layer1_0_relu_zero_point_0]\n",
      "    %add_relu : [#users=2] = call_function[target=torch.ops.quantized.add_relu](args = (%layer1_0_conv2, %maxpool, %layer1_0_relu_scale_0, %layer1_0_relu_zero_point_0), kwargs = {})\n",
      "    %layer1_1_conv1 : [#users=1] = call_module[target=layer1.1.conv1](args = (%add_relu,), kwargs = {})\n",
      "    %layer1_1_conv2 : [#users=1] = call_module[target=layer1.1.conv2](args = (%layer1_1_conv1,), kwargs = {})\n",
      "    %layer1_1_relu_scale_0 : [#users=1] = get_attr[target=layer1_1_relu_scale_0]\n",
      "    %layer1_1_relu_zero_point_0 : [#users=1] = get_attr[target=layer1_1_relu_zero_point_0]\n",
      "    %add_relu_1 : [#users=2] = call_function[target=torch.ops.quantized.add_relu](args = (%layer1_1_conv2, %add_relu, %layer1_1_relu_scale_0, %layer1_1_relu_zero_point_0), kwargs = {})\n",
      "    %layer2_0_conv1 : [#users=1] = call_module[target=layer2.0.conv1](args = (%add_relu_1,), kwargs = {})\n",
      "    %layer2_0_conv2 : [#users=1] = call_module[target=layer2.0.conv2](args = (%layer2_0_conv1,), kwargs = {})\n",
      "    %layer2_0_downsample_0 : [#users=1] = call_module[target=layer2.0.downsample.0](args = (%add_relu_1,), kwargs = {})\n",
      "    %layer2_0_relu_scale_0 : [#users=1] = get_attr[target=layer2_0_relu_scale_0]\n",
      "    %layer2_0_relu_zero_point_0 : [#users=1] = get_attr[target=layer2_0_relu_zero_point_0]\n",
      "    %add_relu_2 : [#users=2] = call_function[target=torch.ops.quantized.add_relu](args = (%layer2_0_conv2, %layer2_0_downsample_0, %layer2_0_relu_scale_0, %layer2_0_relu_zero_point_0), kwargs = {})\n",
      "    %layer2_1_conv1 : [#users=1] = call_module[target=layer2.1.conv1](args = (%add_relu_2,), kwargs = {})\n",
      "    %layer2_1_conv2 : [#users=1] = call_module[target=layer2.1.conv2](args = (%layer2_1_conv1,), kwargs = {})\n",
      "    %layer2_1_relu_scale_0 : [#users=1] = get_attr[target=layer2_1_relu_scale_0]\n",
      "    %layer2_1_relu_zero_point_0 : [#users=1] = get_attr[target=layer2_1_relu_zero_point_0]\n",
      "    %add_relu_3 : [#users=2] = call_function[target=torch.ops.quantized.add_relu](args = (%layer2_1_conv2, %add_relu_2, %layer2_1_relu_scale_0, %layer2_1_relu_zero_point_0), kwargs = {})\n",
      "    %layer3_0_conv1 : [#users=1] = call_module[target=layer3.0.conv1](args = (%add_relu_3,), kwargs = {})\n",
      "    %layer3_0_conv2 : [#users=1] = call_module[target=layer3.0.conv2](args = (%layer3_0_conv1,), kwargs = {})\n",
      "    %layer3_0_downsample_0 : [#users=1] = call_module[target=layer3.0.downsample.0](args = (%add_relu_3,), kwargs = {})\n",
      "    %layer3_0_relu_scale_0 : [#users=1] = get_attr[target=layer3_0_relu_scale_0]\n",
      "    %layer3_0_relu_zero_point_0 : [#users=1] = get_attr[target=layer3_0_relu_zero_point_0]\n",
      "    %add_relu_4 : [#users=2] = call_function[target=torch.ops.quantized.add_relu](args = (%layer3_0_conv2, %layer3_0_downsample_0, %layer3_0_relu_scale_0, %layer3_0_relu_zero_point_0), kwargs = {})\n",
      "    %layer3_1_conv1 : [#users=1] = call_module[target=layer3.1.conv1](args = (%add_relu_4,), kwargs = {})\n",
      "    %layer3_1_conv2 : [#users=1] = call_module[target=layer3.1.conv2](args = (%layer3_1_conv1,), kwargs = {})\n",
      "    %layer3_1_relu_scale_0 : [#users=1] = get_attr[target=layer3_1_relu_scale_0]\n",
      "    %layer3_1_relu_zero_point_0 : [#users=1] = get_attr[target=layer3_1_relu_zero_point_0]\n",
      "    %add_relu_5 : [#users=2] = call_function[target=torch.ops.quantized.add_relu](args = (%layer3_1_conv2, %add_relu_4, %layer3_1_relu_scale_0, %layer3_1_relu_zero_point_0), kwargs = {})\n",
      "    %layer4_0_conv1 : [#users=1] = call_module[target=layer4.0.conv1](args = (%add_relu_5,), kwargs = {})\n",
      "    %layer4_0_conv2 : [#users=1] = call_module[target=layer4.0.conv2](args = (%layer4_0_conv1,), kwargs = {})\n",
      "    %layer4_0_downsample_0 : [#users=1] = call_module[target=layer4.0.downsample.0](args = (%add_relu_5,), kwargs = {})\n",
      "    %layer4_0_relu_scale_0 : [#users=1] = get_attr[target=layer4_0_relu_scale_0]\n",
      "    %layer4_0_relu_zero_point_0 : [#users=1] = get_attr[target=layer4_0_relu_zero_point_0]\n",
      "    %add_relu_6 : [#users=2] = call_function[target=torch.ops.quantized.add_relu](args = (%layer4_0_conv2, %layer4_0_downsample_0, %layer4_0_relu_scale_0, %layer4_0_relu_zero_point_0), kwargs = {})\n",
      "    %layer4_1_conv1 : [#users=1] = call_module[target=layer4.1.conv1](args = (%add_relu_6,), kwargs = {})\n",
      "    %layer4_1_conv2 : [#users=1] = call_module[target=layer4.1.conv2](args = (%layer4_1_conv1,), kwargs = {})\n",
      "    %layer4_1_relu_scale_0 : [#users=1] = get_attr[target=layer4_1_relu_scale_0]\n",
      "    %layer4_1_relu_zero_point_0 : [#users=1] = get_attr[target=layer4_1_relu_zero_point_0]\n",
      "    %add_relu_7 : [#users=1] = call_function[target=torch.ops.quantized.add_relu](args = (%layer4_1_conv2, %add_relu_6, %layer4_1_relu_scale_0, %layer4_1_relu_zero_point_0), kwargs = {})\n",
      "    %avgpool : [#users=1] = call_module[target=avgpool](args = (%add_relu_7,), kwargs = {})\n",
      "    %flatten : [#users=1] = call_function[target=torch.flatten](args = (%avgpool, 1), kwargs = {})\n",
      "    %fc : [#users=1] = call_module[target=fc](args = (%flatten,), kwargs = {})\n",
      "    %dequantize_32 : [#users=1] = call_method[target=dequantize](args = (%fc,), kwargs = {})\n",
      "    return dequantize_32\n"
     ]
    }
   ],
   "source": [
    "quantized_model = convert_fx(prepared_model)\n",
    "print(quantized_model.graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "\n",
    "打印量化模型的大小和精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "量化前模型大小: 44.65853214263916 MB\n",
      "量化后模型大小: 11.283549308776855 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"量化前模型大小: {size_of_model(float_model)/(1<<20)} MB\")\n",
    "print(f\"量化后模型大小: {size_of_model(quantized_model)/(1<<20)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[序列化前] 测试数据集的准确性评估:  69,  89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[序列化后] 测试数据集的准确性评估:  69.464,  88.936\n"
     ]
    }
   ],
   "source": [
    "top1, top5 = evaluate(quantized_model, data_loader_test)\n",
    "print(f\"[序列化前] 测试数据集的准确性评估: {top1.avg: 2.2g}, {top5.avg: 2.2g}\")\n",
    "fx_graph_mode_model_file_path = saved_model_dir + \"resnet18_fx_graph_mode_quantized.pth\"\n",
    "# this does not run due to some erros loading convrelu module:\n",
    "# ModuleAttributeError: 'ConvReLU2d' object has no attribute '_modules'\n",
    "# save the whole model directly\n",
    "# torch.save(quantized_model, fx_graph_mode_model_file_path)\n",
    "# loaded_quantized_model = torch.load(fx_graph_mode_model_file_path)\n",
    "\n",
    "# save with state_dict\n",
    "# torch.save(quantized_model.state_dict(), fx_graph_mode_model_file_path)\n",
    "# import copy\n",
    "# model_to_quantize = copy.deepcopy(float_model)\n",
    "# prepared_model = prepare_fx(model_to_quantize, {\"\": qconfig})\n",
    "# loaded_quantized_model = convert_fx(prepared_model)\n",
    "# loaded_quantized_model.load_state_dict(torch.load(fx_graph_mode_model_file_path))\n",
    "\n",
    "# 保存 script 模型\n",
    "torch.jit.save(torch.jit.script(quantized_model), fx_graph_mode_model_file_path)\n",
    "loaded_quantized_model = torch.jit.load(fx_graph_mode_model_file_path)\n",
    "top1, top5 = evaluate(loaded_quantized_model, data_loader_test)\n",
    "print(f\"[序列化后] 测试数据集的准确性评估: {top1.avg: 2.5g}, {top5.avg: 2.5g}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想获得更好的精度或性能，请尝试更改 `qconfig_mapping`。\n",
    "\n",
    "## 调试量化模型\n",
    "\n",
    "还可以打印量化的非量化卷积运算的权值来查看差异，首先显式调用 `fuse` 来融合卷积和模型中的批处理范数：注意 `fuse_fx` 只在 eval 模式下工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0007, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "fused = fuse_fx(float_model)\n",
    "conv1_weight_after_fuse = fused.conv1[0].weight[0]\n",
    "conv1_weight_after_quant = quantized_model.conv1.weight().dequantize()[0]\n",
    "print(torch.max(abs(conv1_weight_after_fuse - conv1_weight_after_quant)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与基线浮点模型和 eager 模式量化的比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 模型大小: 44.65853214263916 MB\n",
      "baseline 浮点模型评估精度:  69.758,  89.078\n"
     ]
    }
   ],
   "source": [
    "scripted_float_model_file = \"resnet18_scripted.pth\"\n",
    "print(f\"baseline 模型大小: {size_of_model(float_model)/(1<<20)} MB\")\n",
    "top1, top5 = evaluate(float_model, data_loader_test)\n",
    "print(f\"baseline 浮点模型评估精度: {top1.avg: 2.5g}, {top5.avg: 2.5g}\")\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节中，比较了用 FX 图模式量化的模型和用 eager 模式量化的模型。FX 图模式和 eager 模式产生非常相似的量化模型，因此期望精度和加速也是相似的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FX 图模式量化模型大小: 11.283549308776855 MB\n",
      "FX 图模式量化评估精度:  69.464,  88.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_QuantizedWeights.IMAGENET1K_FBGEMM_V1`. You can also use `weights=ResNet18_QuantizedWeights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/_utils.py:320: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  scales = torch.tensor(scales, dtype=torch.double, device=storage.device)\n",
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/_utils.py:322: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  zero_points, dtype=torch.long, device=storage.device\n",
      "/media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/torch/_utils.py:335: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of eager mode quantized model\n",
      "eager 模式量化模型大小: 0.0035047531127929688 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"FX 图模式量化模型大小: {size_of_model(quantized_model)/(1<<20)} MB\")\n",
    "top1, top5 = evaluate(quantized_model, data_loader_test)\n",
    "print(f\"FX 图模式量化评估精度: {top1.avg: 2.5g}, {top5.avg: 2.5g}\")\n",
    "\n",
    "from torchvision.models.quantization.resnet import resnet18\n",
    "eager_quantized_model = resnet18(pretrained=True, quantize=True).eval()\n",
    "print(\"Size of eager mode quantized model\")\n",
    "eager_quantized_model = torch.jit.script(eager_quantized_model)\n",
    "print(f\"eager 模式量化模型大小: {size_of_model(eager_quantized_model)/(1<<20)} MB\")\n",
    "top1, top5 = evaluate(eager_quantized_model, data_loader_test)\n",
    "print(f\"eager 模式量化评估精度: {top1.avg: 2.5g}, {top5.avg: 2.5g}\")\n",
    "eager_mode_model_file = \"resnet18_eager_mode_quantized.pth\"\n",
    "torch.jit.save(eager_quantized_model, saved_model_dir + eager_mode_model_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，FX 图模式和 eager 模式量化模型的模型大小和精度非常相似。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们所看到的，对于 resnet18, FX 图模式和 eager 模式量化模型都比浮点模型得到了相似的加速，比浮点模型快 2-4 倍左右。但是浮点模型的实际加速可能会根据模型、设备、构建、输入批大小、线程等而有所不同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
