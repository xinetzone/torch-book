{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FX 量化快速入门\n",
    "\n",
    "参考：\n",
    "\n",
    "1. [量化实践](https://pytorch.org/blog/quantization-in-practice/)\n",
    "2. [fx graph 模式 POST TRAINING STATIC QUANTIZATION](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html)\n",
    "\n",
    "本教程介绍基于 {mod}`torch.fx` 在 graph 模式下进行训练后静态量化的步骤。FX Graph 模式量化的优点：可以在模型上完全自动地执行量化，尽管可能需要一些努力使模型与 FX Graph 模式量化兼容（象征性地用 {mod}`torch.fx` 跟踪），将有单独的教程来展示如何使我们想量化的模型的一部分与 FX Graph 模式量化兼容。也有 [FX Graph 模式后训练动态量化](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html) 教程。FX Graph 模式 API 如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/ao/quantization/observer.py:176: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, fx\n",
    "from torch.quantization import get_default_qconfig\n",
    "from torch.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "\n",
    "class M(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "float_model = M()\n",
    "float_model.eval()\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qconfig_dict = {\"\": qconfig}\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            model(image)\n",
    "\n",
    "valset = []\n",
    "prepared_model = prepare_fx(float_model, qconfig_dict)  # 融合模块并插入观测器\n",
    "calibrate(prepared_model, valset)  # 在代表数据上运行校准\n",
    "quantized_model = convert_fx(prepared_model)  # 转化校准后的模型为量化模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FX Graph 模式量化的动机\n",
    "\n",
    "目前 PyTorch 存在 eager 模式量化：[Static Quantization with Eager Mode in PyTorch](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)。\n",
    "\n",
    "可以看到，该过程涉及到多个手动步骤，包括：\n",
    "\n",
    "- 显式地 quantize 和 dequantize activations，当浮点和量化运算混合在模型中时，这是非常耗时的。\n",
    "- 显式融合模块，这需要手动识别卷积序列、 batch norms 以及 relus 和其他融合模式。\n",
    "- PyTorch 张量运算需要特殊处理（如 `add`、`concat` 等）。\n",
    "- 函数式没有  first class 支持（`functional.conv2d` 和 `functional.linear` 不会被量化）\n",
    "\n",
    "这些需要的修改大多来自于 Eager 模式量化的潜在限制。Eager 模式在模块级工作，因为它不能检查实际运行的代码（在 `forward` 函数中），量化是通过模块交换实现的，不知道在 Eager 模式下 `forward` 函数中模块是如何使用的。因此，它需要用户手动插入 `QuantStub` 和 `DeQuantStub`，以标记他们想要 quantize 或 dequantize 的点。在图模式中，可以检查在 `forward` 函数中执行的实际代码（例如 `aten` 函数调用），量化是通过模块和 graph 操作实现的。由于图模式对运行的代码具有完全的可见性，能够自动地找出要融合哪些模块，在哪里插入 observer 调用，quantize/dequantize 函数等，能够自动化整个量化过程。\n",
    "\n",
    "FX Graph 模式量化的优点是：\n",
    "\n",
    "- 简化量化流程，最小化手动步骤\n",
    "- 开启了进行更高级别优化的可能性，如自动精度选择（automatic precision selection）\n",
    "\n",
    "## 定义辅助函数和 Prepare Dataset\n",
    "\n",
    "首先进行必要的导入，定义一些辅助函数并准备数据。这些步骤与 PyTorch 中 [使用 Eager 模式的静态量化](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html) 相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要使用整个 ImageNet 数据集运行本教程中的代码，首先按照 [ImageNet Data](http://www.image-net.org/download) 中的说明下载 ImageNet。将下载的文件解压缩到 `data_path` 文件夹中。\n",
    "\n",
    "下载 {mod}`torchvision resnet18 模型 <torchvision.models.resnet>` 并将其重命名为 `models/resnet18_pretrained_float.pth`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2857142857142857"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LRScheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Decays the learning rate of each parameter group by gamma every\u001b[0m\n",
      "\u001b[0;34m    step_size epochs. Notice that such decay can happen simultaneously with\u001b[0m\n",
      "\u001b[0;34m    other changes to the learning rate from outside this scheduler. When\u001b[0m\n",
      "\u001b[0;34m    last_epoch=-1, sets initial lr as lr.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m        optimizer (Optimizer): Wrapped optimizer.\u001b[0m\n",
      "\u001b[0;34m        step_size (int): Period of learning rate decay.\u001b[0m\n",
      "\u001b[0;34m        gamma (float): Multiplicative factor of learning rate decay.\u001b[0m\n",
      "\u001b[0;34m            Default: 0.1.\u001b[0m\n",
      "\u001b[0;34m        last_epoch (int): The index of last epoch. Default: -1.\u001b[0m\n",
      "\u001b[0;34m        verbose (bool): If ``True``, prints a message to stdout for\u001b[0m\n",
      "\u001b[0;34m            each update. Default: ``False``.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Example:\u001b[0m\n",
      "\u001b[0;34m        >>> # Assuming optimizer uses lr = 0.05 for all groups\u001b[0m\n",
      "\u001b[0;34m        >>> # lr = 0.05     if epoch < 30\u001b[0m\n",
      "\u001b[0;34m        >>> # lr = 0.005    if 30 <= epoch < 60\u001b[0m\n",
      "\u001b[0;34m        >>> # lr = 0.0005   if 60 <= epoch < 90\u001b[0m\n",
      "\u001b[0;34m        >>> # ...\u001b[0m\n",
      "\u001b[0;34m        >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\u001b[0m\n",
      "\u001b[0;34m        >>> for epoch in range(100):\u001b[0m\n",
      "\u001b[0;34m        >>>     train(...)\u001b[0m\n",
      "\u001b[0;34m        >>>     validate(...)\u001b[0m\n",
      "\u001b[0;34m        >>>     scheduler.step()\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lr_called_within_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To get the last learning rate computed by the scheduler, \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                          \u001b[0;34m\"please use `get_last_lr()`.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_closed_form_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbase_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mbase_lr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           /media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "torch.optim.lr_scheduler.StepLR??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = 0.857142\n",
    "x = 0.857142\n",
    "L = []\n",
    "for k in range(100):\n",
    "    x = x * s\n",
    "    L.append(x)\n",
    "y = np.array(L)\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageNet' object has no attribute 'loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/quant/fx.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/quant/fx.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m saved_model_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/quant/fx.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m dataset \u001b[39m=\u001b[39m ImageNet(root)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/quant/fx.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m trainset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mloader(batch_size\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bxin/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/quant/fx.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m valset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mloader(batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageNet' object has no attribute 'loader'"
     ]
    }
   ],
   "source": [
    "from torch_book.data import ImageNet\n",
    "\n",
    "\n",
    "root = \"/media/pc/data/4tb/lxw/datasets/ILSVRC\"\n",
    "saved_model_dir = 'models/'\n",
    "\n",
    "dataset = ImageNet(root)\n",
    "trainset = dataset.loader(batch_size=30, split=\"train\")\n",
    "valset = dataset.loader(batch_size=50, split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torchvision import models\n",
    "\n",
    "model_name = \"resnet18\"\n",
    "float_model = getattr(models, model_name)(pretrained=True)\n",
    "float_model.eval()\n",
    "\n",
    "# deepcopy the model since we need to keep the original model around\n",
    "model_to_quantize = copy.deepcopy(float_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模式的模型\n",
    "\n",
    "对于训练后量化，需要将模型设置为评估模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_quantize.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 `qconfig_dict` 指定如何量化模型\n",
    "\n",
    "```python\n",
    "qconfig_dict = {\"\" : default_qconfig}\n",
    "```\n",
    "\n",
    "使用与 Eager 模式量化中相同的 `qconfig`, `qconfig` 只是用于激活和权重的 observers 的命名元组。`qconfig_dict` 是具有以下配置的字典：\n",
    "\n",
    "```python\n",
    "qconfig = {\n",
    "    \" : qconfig_global,\n",
    "    \"sub\" : qconfig_sub,\n",
    "    \"sub.fc\" : qconfig_fc,\n",
    "    \"sub.conv\": None\n",
    "}\n",
    "qconfig_dict = {\n",
    "    # qconfig? means either a valid qconfig or None\n",
    "    # optional, global config\n",
    "    \"\": qconfig?,\n",
    "    # optional, used for module and function types\n",
    "    # could also be split into module_types and function_types if we prefer\n",
    "    \"object_type\": [\n",
    "        (torch.nn.Conv2d, qconfig?),\n",
    "        (torch.nn.functional.add, qconfig?),\n",
    "        ...,\n",
    "    ],\n",
    "    # optional, used for module names\n",
    "    \"module_name\": [\n",
    "        (\"foo.bar\", qconfig?)\n",
    "        ...,\n",
    "    ],\n",
    "    # optional, matched in order, first match takes precedence\n",
    "    \"module_name_regex\": [\n",
    "        (\"foo.*bar.*conv[0-9]+\", qconfig?)\n",
    "        ...,\n",
    "    ],\n",
    "    # priority (in increasing order): global, object_type, module_name_regex, module_name\n",
    "    # qconfig == None means fusion and quantization should be skipped for anything\n",
    "    # matching the rule\n",
    "\n",
    "    # **api subject to change**\n",
    "    # optional: specify the path for standalone modules\n",
    "    # These modules are symbolically traced and quantized as one unit\n",
    "    # so that the call to the submodule appears as one call_module\n",
    "    # node in the forward graph of the GraphModule\n",
    "    \"standalone_module_name\": [\n",
    "        \"submodule.standalone\"\n",
    "    ],\n",
    "    \"standalone_module_class\": [\n",
    "        StandaloneModuleClass\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "可以在 [`qconfig` 文件](https://github.com/pytorch/pytorch/blob/master/torch/quantization/qconfig.py) 中找到与 `qconfig` 相关的实用函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import get_default_qconfig, quantize_jit\n",
    "\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qconfig_dict = {\"\": qconfig}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为静态后训练量化模型做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from torch.quantization.quantize_fx import prepare_fx\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "prepared_model = prepare_fx(model_to_quantize, qconfig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prepare_fx` 将 BatchNorm 模块折叠到前面的 Conv2d 模块中，并在模型中的适当位置插入 observers。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prepared_model.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 校准\n",
    "\n",
    "将 observers 插入模型后，运行校准函数。校准的目的就是通过一些样本运行代表性的工作负载（例如样本的训练数据集）以便 observers 在模型中能够观测到张量的统计数据，以后使用这些信息来计算量化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def calibrate(model, data_loader, samples=500):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        k = 0\n",
    "        for image, _ in data_loader:\n",
    "            if k > samples:\n",
    "                break\n",
    "            model(image)\n",
    "            k += len(image)\n",
    "\n",
    "calibrate(prepared_model, trainset)  # run calibration on sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将模型转换为量化模型\n",
    "\n",
    "`convert_fx` 采用校准模型并产生量化模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization.quantize_fx import convert_fx\n",
    "\n",
    "quantized_model = convert_fx(prepared_model)\n",
    "print(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估\n",
    "\n",
    "现在可以打印量化模型的大小和精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_book.contrib.helper import evaluate, print_size_of_model\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Size of model before quantization\")\n",
    "print_size_of_model(float_model)\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(quantized_model)\n",
    "top1, top5 = evaluate(quantized_model, criterion, valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[before serilaization] Evaluation accuracy on test dataset: {top1.avg:2.2f}, {top5.avg:2.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_graph_mode_model_file_path = saved_model_dir + f\"{model_name}_fx_graph_mode_quantized.pth\"\n",
    "\n",
    "# this does not run due to some erros loading convrelu module:\n",
    "# ModuleAttributeError: 'ConvReLU2d' object has no attribute '_modules'\n",
    "# save the whole model directly\n",
    "# torch.save(quantized_model, fx_graph_mode_model_file_path)\n",
    "# loaded_quantized_model = torch.load(fx_graph_mode_model_file_path)\n",
    "\n",
    "# save with state_dict\n",
    "# torch.save(quantized_model.state_dict(), fx_graph_mode_model_file_path)\n",
    "# import copy\n",
    "# model_to_quantize = copy.deepcopy(float_model)\n",
    "# prepared_model = prepare_fx(model_to_quantize, {\"\": qconfig})\n",
    "# loaded_quantized_model = convert_fx(prepared_model)\n",
    "# loaded_quantized_model.load_state_dict(torch.load(fx_graph_mode_model_file_path))\n",
    "\n",
    "# save with script\n",
    "torch.jit.save(torch.jit.script(quantized_model), fx_graph_mode_model_file_path)\n",
    "loaded_quantized_model = torch.jit.load(fx_graph_mode_model_file_path)\n",
    "\n",
    "top1, top5 = evaluate(loaded_quantized_model, criterion, valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[after serialization/deserialization] Evaluation accuracy on test dataset: {top1.avg:2.2f}, {top5.avg:2.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果希望获得更好的精度或性能，请尝试更改 `qconfig_dict`。\n",
    "\n",
    "## 调试量化模型\n",
    "\n",
    "还可以打印量化的 un-quantized conv 的权重来查看区别，首先显式地调用 `fuse` 来融合模型中的 conv 和 bn：注意，`fuse_fx` 只在 `eval` 模式下工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization.quantize_fx import fuse_fx\n",
    "\n",
    "fused = fuse_fx(float_model)\n",
    "\n",
    "conv1_weight_after_fuse = fused.conv1[0].weight[0]\n",
    "conv1_weight_after_quant = quantized_model.conv1.weight().dequantize()[0]\n",
    "\n",
    "print(torch.max(abs(conv1_weight_after_fuse - conv1_weight_after_quant)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基线浮点模型和 Eager 模式量化的比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_float_model_file = \"resnet18_scripted.pth\"\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "top1, top5 = evaluate(float_model, criterion, valset)\n",
    "print(\"Baseline Float Model Evaluation accuracy: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节中，将量化模型与 FX Graph 模式的量化模型与在 Eager 模式下量化的模型进行比较。FX Graph 模式和 Eager 模式产生的量化模型非常相似，因此期望精度和 speedup 也很相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Fx graph mode quantized model\")\n",
    "print_size_of_model(quantized_model)\n",
    "top1, top5 = evaluate(quantized_model, criterion, valset)\n",
    "print(\"FX graph mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n",
    "\n",
    "from torchvision.models.quantization.resnet import resnet18\n",
    "eager_quantized_model = resnet18(pretrained=True, quantize=True).eval()\n",
    "print(\"Size of eager mode quantized model\")\n",
    "eager_quantized_model = torch.jit.script(eager_quantized_model)\n",
    "print_size_of_model(eager_quantized_model)\n",
    "top1, top5 = evaluate(eager_quantized_model, criterion, valset)\n",
    "print(\"eager mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n",
    "eager_mode_model_file = \"resnet18_eager_mode_quantized.pth\"\n",
    "torch.jit.save(eager_quantized_model, saved_model_dir + eager_mode_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 FX Graph 模式和 Eager 模式量化模型的模型大小和精度是非常相似的。\n",
    "\n",
    "在 AIBench 中运行模型（单线程）会得到如下结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```log\n",
    "Scripted Float Model:\n",
    "Self CPU time total: 192.48ms\n",
    "\n",
    "Scripted Eager Mode Quantized Model:\n",
    "Self CPU time total: 50.76ms\n",
    "\n",
    "Scripted FX Graph Mode Quantized Model:\n",
    "Self CPU time total: 50.63ms\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，对于 resnet18, FX Graph 模式和 Eager 模式量化模型都比浮点模型获得了相似的速度，大约比浮点模型快 2-4 倍。但是浮点模型上的实际加速可能会因模型、设备、构建、输入批大小、线程等而不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
