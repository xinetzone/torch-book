{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化实践\n",
    "\n",
    "参考：[Practical Quantization in PyTorch](https://pytorch.org/blog/quantization-in-practice/)\n",
    "\n",
    "{guilabel}`NN 量化目标`：运行更快、内存需求更低。\n",
    "\n",
    "- 量化源于信息压缩；在深度神经网络中，它指的是降低其权重和/或激活的数值精度。\n",
    "- 过度参数化的 DNN 有更多的 **自由度**，这使它们成为信息压缩的良好候选对象 {cite:ps}`gholami2021survey`。\n",
    "\n",
    "当量化模型时，通常会发生两件事——模型变得更小，运行效率更高。硬件供应商明确地允许更快地处理 8 位数据（而不是 32 位数据），从而获得更高的 **吞吐量** （throughput）。更小的模型具有更低的内存占用和功耗 {cite:ps}`krishnamoorthi2018quantizing`，这对于边缘部署至关重要。\n",
    "\n",
    "## 量化背景知识\n",
    "\n",
    "### 映射函数\n",
    "\n",
    "映射函数：将值从浮点数映射到整数空间的函数。常用的映射函数是由 $Q(r) = \\operatorname{round}(r/S + Z)$ 给出的线性变换，其中为 $r$ 为输入，$S, Z$ 为量化参数（quantization parameters）。为了重新变换为浮点空间，反函数由 $\\overline{r} = (Q(r) - Z) \\cdot S$ 给出（被称为 **反量化**，即 dequantization）。\n",
    "\n",
    "```{note}\n",
    "$\\overline{r} \\neq r$，它们之间的差异构成了量化误差。\n",
    "```\n",
    "\n",
    "### 量化参数\n",
    "\n",
    "映射函数由缩放因子 $S$ 和零点 $Z$ 所参数化。$S$ 仅仅是输入范围与输出范围的比值 $S = \\frac {\\beta - \\alpha}{\\beta_q - \\alpha_q}$。这里 $[\\alpha, \\beta]$ 是输入的裁剪（clipping）范围，即允许输入的边界。$[\\alpha_q, \\beta_q]$ 是它被映射到的量化输出空间的范围。对于 8 位量化，输出范围 $0 \\leq \\beta_q - \\alpha_q \\leq 2^8 -1$。$Z = -(\\frac {\\alpha}{S} - \\alpha_q)$ 作为偏置，以确保输入空间中的 $0$ 完全映射到量化空间中的 $0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 校准\n",
    "\n",
    "选择输入裁剪范围的过程称为 **校准** （calibration）。最简单的方法（也是 PyTorch 中的默认方法）是记录正在运行的最小值和最大值，并将它们赋值给 $\\alpha$ 和 $\\beta$。[TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/calib.html) 使用熵最小化（KL 散度），均方误差最小化，或输入范围的百分位数。\n",
    "\n",
    "在 PyTorch 中，{class}`Observer <torch.ao.quantization.observer.ObserverBase>` 模块收集关于输入值的统计信息并计算 `qparams` $S, Z$。不同的校准方案会产生不同的量化输出，最好通过经验验证哪种方案最适合您的应用程序和体系结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} 观测器基类\n",
    "{class}`~torch.ao.quantization.observer.ObserverBase` 是观测器（observer）模块的基类。在 {func}`forward` 中，将更新观测张量的统计信息。观测器应该提供用于收集的统计信息并计算量化参数的 `calculate_qparams` 函数。\n",
    "```\n",
    "\n",
    "### 均匀量化\n",
    "\n",
    "````{admonition} 均匀量化的观测器基类\n",
    "{class}`~torch.ao.quantization.observer.UniformQuantizationObserverBase` 是所有使用均匀量化（uniform quantization，{cite:p}`UniformQuantizers21`）来计算 `scale` 和 `zero_point` 的观测器基类。\n",
    "\n",
    "参数：\n",
    "- `dtype`：量化后的数据类型。\n",
    "- `qscheme`：量化方案。\n",
    "- `reduce_range`：使用 1 bit 归约量化数据类型的范围。这有时是为了避免指令溢出所必需的。\n",
    "- `quant_min`：最小量化值。如果未指定，它将遵循 8 bit 设置。\n",
    "- `quant_max`：最大量化值。如果未指定，它将遵循 8 bit 设置。\n",
    "- `eps`：float32 的 $\\epsilon$ 值，默认为 {data}`torch.finfo(torch.float32).eps`。\n",
    "\n",
    "```{warning}\n",
    "1. {attr}`dtype` 仅支持 ``torch.qint8`` 或者 ``torch.quint8``。\n",
    "2. {attr}`qscheme` 有：\n",
    "\n",
    "    - ``torch.per_tensor_affine``\n",
    "    - ``torch.per_tensor_symmetric``\n",
    "    - ``torch.per_channel_affine``\n",
    "    - ``torch.per_channel_symmetric``\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`````{admonition} MinMaxObserver\n",
    "{class}`~torch.ao.quantization.observer.MinMaxObserver` 观测器模块，根据运行的最小值和最大值计算量化参数。\n",
    "\n",
    "此观测器使用张量最小/最大统计量来计算量化参数。该模块记录传入张量的运行最小值和最大值，并利用该统计量计算量化参数。\n",
    "\n",
    "记作 running min/max 为 {math}`x_{\\min}` 和 {math}`x_{\\max}`；`scale` 和零点为 {math}`s`，{math}`z`，则：\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "x_\\text{min} &= \\begin{cases}\n",
    "    \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\\n",
    "    \\min\\left(x_\\text{min}, \\min(X)\\right) & \\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "x_\\text{max} &= \\begin{cases}\n",
    "    \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\\n",
    "    \\max\\left(x_\\text{max}, \\max(X)\\right) & \\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "其中 $X$ 是被观测张量。\n",
    "\n",
    "为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{if 对称:}&\\\\\n",
    "&s = 2 \\max(|x_\\text{min}|, x_\\text{max}) /\n",
    "    \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\\n",
    "&z = \\begin{cases}\n",
    "    0 & \\text{if dtype is qint8} \\\\\n",
    "    128 & \\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "\\text{else:}&\\\\\n",
    "    &s = \\left( x_\\text{max} - x_\\text{min}  \\right ) /\n",
    "        \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\\n",
    "    &z = Q_\\text{min} - \\text{round}(x_\\text{min} / s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这里 $Q_{\\min}$ 和 $Q_{\\max}$ 量化后的数据的最小最大值。\n",
    "\n",
    "如果 running 最小值等于 running 最大值，则 `scale` 和 `zero_point` 设置为 $1.0$ 和 $0$。\n",
    "\n",
    "````{admonition} MovingAverageMinMaxObserver\n",
    "{class}`~torch.ao.quantization.observer.MovingAverageMinMaxObserver` 是 {class}`~torch.ao.quantization.observer.MinMaxObserver` 的子类，它是基于最小值和最大值的滑动平均值计算量化参数的观测器模块。\n",
    "\n",
    "此观测器根据传入张量的极小值和极大值的滑动\n",
    "\n",
    "平均值计算量化参数。该模块记录传入张量的平均最小值和最大值，并利用该统计量计算量化参数。\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    x_\\text{min} = \\begin{cases}\n",
    "        \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\\n",
    "        (1 - c) x_\\text{min} + c \\min(X) & \\text{otherwise}\n",
    "    \\end{cases}\\\\\n",
    "    x_\\text{max} = \\begin{cases}\n",
    "        \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\\n",
    "        (1 - c) x_\\text{max} + c \\max(X) & \\text{otherwise}\n",
    "    \\end{cases}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "其中 $c$ 是 ``averaging_constant``。\n",
    "\n",
    "```{note}\n",
    "仅用于 ``torch.per_tensor_affine`` 量化方案。\n",
    "```\n",
    "````\n",
    "`````\n",
    "\n",
    "`````{admonition} PerChannelMinMaxObserver\n",
    "{class}`~torch.ao.quantization.observer.PerChannelMinMaxObserver` 观测器模块，根据逐通道运行的最小值和最大值计算量化参数。\n",
    "\n",
    "此观测器使用张量最小/最大统计量来逐通道计算量化参数。该模块记录传入张量的运行最小值和最大值，并使用该统计量计算量化\n",
    "参数。\n",
    "\n",
    "参数：\n",
    "\n",
    "- `ch_axis`：通道的轴。\n",
    "\n",
    "量化参数的计算方法与 {class}`~torch.ao.quantization.observer.MinMaxObserver` 相同，区别在于运行的最小/最大值存储在每个通道上。\n",
    "\n",
    "此观测器 存在子类 {class}`~torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver`。\n",
    "`````\n",
    "\n",
    "````{admonition} HistogramObserver\n",
    "{class}`~torch.ao.quantization.observer.HistogramObserver` 记录张量值的运行直方图（histogram）以及最小/最大值。\n",
    "\n",
    "参数：\n",
    "\n",
    "- `bins`：用于直方图的箱的数量。\n",
    "- `upsample_rate`：直方图上采样的因子，这用于插值直方图与不同的观测范围。\n",
    "\n",
    "`scale` 和零点计算方式如下：\n",
    "\n",
    "1. 创建传入输入的直方图：直方图的计算是连续的，每个 bin 的范围随着观测到的每个新张量而变化。\n",
    "2. 搜索直方图中的分布，寻找最佳的最小/最大值：对最小/最大值的搜索确保了相对于浮点模型的量化误差的最小化。\n",
    "3. 计算`scale` 和零点的方法同 {class}`~torch.ao.quantization.observer.MinMaxObserver`。\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.6990, 0.6236],\n",
       "         [1.9239, 0.0646],\n",
       "         [0.5055, 0.2862]]),\n",
       " tensor([[ 0.2722,  0.0734],\n",
       "         [ 1.7291,  1.0023],\n",
       "         [-1.0072, -0.1741]])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
    "\n",
    "# 设置输入\n",
    "C, L = 3, 2\n",
    "normal = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "inputs = [normal.sample((C, L)),\n",
    "          normal.sample((C, L))]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置观测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "observers = [MinMaxObserver(),\n",
    "             MovingAverageMinMaxObserver(),\n",
    "             HistogramObserver()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算并查看量化参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxObserver (tensor([0.0115]), tensor([88], dtype=torch.int32))\n",
      "MovingAverageMinMaxObserver (tensor([0.0075]), tensor([0], dtype=torch.int32))\n",
      "HistogramObserver (tensor([0.0053]), tensor([0], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "for obs in observers:\n",
    "    for x in inputs:\n",
    "        obs(x)\n",
    "    print(obs.__class__.__name__, obs.calculate_qparams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仿射和对称量化方案\n",
    "\n",
    "仿射（affine）或非对称量化（asymmetric quantization）方案分配输入范围的最小和最大观测值。仿射方案通常提供更小的剪切范围，并且对于量化非负激活非常有用（如果你的输入张量永远都不是负的，你就不需要输入范围包含负值）。计算范围为 $\\alpha=\\min(r), \\beta = \\max(r)$。当用于权值张量 {cite:ps}`wu2020integer` 时，仿射量化会导致更昂贵的计算推理。\n",
    "\n",
    "对称量化（Symmetric quantization）方案将输入范围集中在 $0$ 附近，消除了计算零点偏置的需要。计算范围为 $-\\alpha=\\beta=\\max(|\\max(r)|,|\\min(r)|)$。\n",
    "\n",
    "对于倾斜的信号（如非负激活），这可能会导致糟糕的量化分辨率（quantization resolution），因为剪辑范围包括从未在输入中出现的值（参见下面的 pyplot）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_symmetric_range(x):\n",
    "    '''获取对称范围'''\n",
    "    beta = torch.max(x.max(), x.min().abs())\n",
    "    return -beta.item(), beta.item()\n",
    "\n",
    "\n",
    "def get_affine_range(x):\n",
    "    '''获取仿射范围'''\n",
    "    return x.min().item(), x.max().item()\n",
    "\n",
    "\n",
    "def plot(plt, data, scheme):\n",
    "    '''画出不同方案的分布'''\n",
    "    boundaries = get_affine_range(data) if scheme == 'affine' \\\n",
    "        else get_symmetric_range(data)\n",
    "    a, _, _ = plt.hist(data, density=True, bins=100)\n",
    "    ymin, ymax = np.quantile(a[a > 0], [0.25, 0.95])\n",
    "    plt.vlines(x=boundaries, ls='--', colors='purple', ymin=ymin, ymax=ymax)\n",
    "\n",
    "\n",
    "# 模拟激活和权重\n",
    "act = torch.distributions.pareto.Pareto(1, 10).sample((1, 1024))\n",
    "weights = torch.distributions.normal.Normal(0, 0.12).sample((3, 8, 3, 3)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "plot(axs[0, 0], act, 'affine')\n",
    "axs[0, 0].set_title(\"Activation, Affine-Quantized\")\n",
    "plot(axs[0, 1], act, 'symmetric')\n",
    "axs[0, 1].set_title(\"Activation, Symmetric-Quantized\")\n",
    "plot(axs[1, 0], weights, 'affine')\n",
    "axs[1, 0].set_title(\"Weights, Affine-Quantized\")\n",
    "plot(axs[1, 1], weights, 'symmetric')\n",
    "axs[1, 1].set_title(\"Weights, Symmetric-Quantized\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，你可以在初始化 `Observer` 时指定仿射或对称模式。注意，并非所有 `observer` 都支持这两种方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qscheme: torch.per_tensor_affine | (tensor([0.0075]), tensor([0], dtype=torch.int32))\n",
      "Qscheme: torch.per_tensor_symmetric | (tensor([0.0151]), tensor([128]))\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
    "    obs = MovingAverageMinMaxObserver(qscheme=qscheme)\n",
    "    for x in inputs:\n",
    "        obs(x)\n",
    "    print(f\"Qscheme: {qscheme} | {obs.calculate_qparams()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逐张量和逐通道量化方案\n",
    "\n",
    "量化参数可以作为整体计算层的整个权值张量，也可以单独计算每个通道的权值张量。在每张量中，对层中的所有通道应用相同的剪切范围：\n",
    "\n",
    "![](images/tensor-quantization.png)\n",
    "\n",
    "对于权值量化，逐通道（Per-Channel）对称量化提供了更好的精度；逐张量（Per-Tensor）量化的性能很差，这可能是由于不同通道之间的转换权值与批量范数折叠（batchnorm folding） {cite:ps}`wu2020integer` 差异很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0027, 0.0075, 0.0020]), tensor([0, 0, 0], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
    "# 计算全部 `C` 通道的 qparams\n",
    "obs = MovingAveragePerChannelMinMaxObserver(ch_axis=0)\n",
    "for x in inputs:\n",
    "    obs(x)\n",
    "print(obs.calculate_qparams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0075, 0.0025]), tensor([0, 0], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "obs = MovingAveragePerChannelMinMaxObserver(ch_axis=1)\n",
    "for x in inputs:\n",
    "    obs(x)\n",
    "print(obs.calculate_qparams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后端引擎\n",
    "\n",
    "目前，量化算子通过 [FBGEMM 后端](https://github.com/pytorch/FBGEMM) 在 x86 机器上运行，或者在 ARM 机器上使用 [QNNPACK](https://github.com/pytorch/QNNPACK) 原语。服务器 GPU 的后端支持（通过 TensorRT 和 cuDNN）即将推出。了解更多关于将量化扩展到自定义后端：[RFC-0019](https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "backend = 'fbgemm'  # if x86 else 'qnnpack'\n",
    "qconfig = get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `QConfig`\n",
    "\n",
    "{class}`~torch.ao.quantization.qconfig.QConfig` NamedTuple 存储用于量化激活和权重的 Observer 和量化方案。\n",
    "\n",
    "一定要传递 `Observer` 类（而不是实例），或者可以返回 `Observer` 实例的可调用对象。使用 {func}`with_args` 覆盖默认参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_affine){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.qint8){})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization.qconfig import QConfig\n",
    "\n",
    "my_qconfig = QConfig(\n",
    "    activation=MovingAverageMinMaxObserver.with_args(\n",
    "        qscheme=torch.per_tensor_affine),\n",
    "    weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8)\n",
    ")\n",
    "my_qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 PyTorch 中\n",
    "\n",
    "PyTorch 允许您使用几种不同的方式来量化您的模型：\n",
    "\n",
    "- Eager 模式 v/s FX Graph 模式：如果你更喜欢灵活但手动的，或受限的自动过程\n",
    "- 静态 v/s 动态：如果量化激活（层的输出）的 `qparams` 为所有输入预先计算，或对每个输入重新计算，\n",
    "- 量化感知训练（quantization-aware training） v/s 训练后量化（post-training quantization）：如果 `qparams` 是在有或没有重新训练的情况下计算的\n",
    "\n",
    "FX Graph Mode 自动融合符合条件的模块，插入 Quant/DeQuant stub，校准模型并返回量化模块——所有这些都是在两个方法调用中进行的——但仅适用于 [可符号跟踪](https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace) 的网络。 \n",
    "\n",
    "在 DNN 中，量化的合适候选对象是 FP32 权值（层参数）和激活（层输出）。量化权值可以减少模型的大小。量化激活通常会导致更快的推理。\n",
    "\n",
    "例如，50 层 ResNet 网络有近 2600 万个权值参数，在正向传程中计算近 1600 万个激活。\n",
    "\n",
    "### Post-Training Dynamic/Weight-only Quantization\n",
    "\n",
    "这里模型的权值是预量化的；在推理期间，激活是动态量化的。这是所有方法中最简单的一种，它在 {func}`~torch.ao.quantization.quantize.quantize_dynamic` 中有一行 API 调用。目前只支持线性和循环（LSTM、GRU、RNN）层进行动态量化。\n",
    "\n",
    "- 可以导致更高的精度，因为每个输入的裁剪范围是精确校准的\n",
    "- 对于像 LSTM 和 Transformer 这样的模型，动态量化是首选的，因为从内存中写入/检索模型的权值会受制于带宽\n",
    "- 在运行时对每个层的激活进行校准和量化会增加计算开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 小 model\n",
    "def create_model():\n",
    "    m = nn.Sequential(\n",
    "        nn.Conv2d(2, 64, (8,)),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 10),\n",
    "        nn.LSTM(10, 10))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} eager 模式\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 64, kernel_size=(8,), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): DynamicQuantizedLinear(in_features=16, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (3): DynamicQuantizedLSTM(10, 10)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization.quantize import quantize_dynamic\n",
    "\n",
    "m = create_model()\n",
    "m.eval()\n",
    "model_quantized = quantize_dynamic(\n",
    "    model=m, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False\n",
    ")\n",
    "model_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} FX 模式\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import quantize_fx\n",
    "from torch.ao.quantization.qconfig import default_dynamic_qconfig\n",
    "\n",
    "m = create_model()\n",
    "m.eval()\n",
    "# 空键表示应用于所有模块的默认值\n",
    "qconfig_dict = {\"\": default_dynamic_qconfig}\n",
    "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict)\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Training Static Quantization (PTQ)\n",
    "\n",
    "PTQ 也预量化模型权重，但不是动态校准激活，而是使用验证数据对剪切范围进行预校准和固定（“静态”）。在推理过程中，激活在运算之间保持量化精度。大约 100 个小批次的代表性数据就足以校准观测者。为了方便起见，下面的例子在校准中使用了随机数据——在应用程序中使用随机数据将导致错误的 `qparams`。\n",
    "\n",
    "![](images/ptq-flowchart.svg)\n",
    "\n",
    "[模块融合](https://pytorch.org/tutorials/recipes/fuse.html) 将多个顺序模块（如：`[Conv2d, BatchNorm, ReLU]`）组合成一个。融合模块意味着编译器只需要运行一个内核而不是多个；这可以通过减少量化误差来提高速度和准确性。\n",
    "\n",
    "- 静态量化比动态量化具有更快的推理速度，因为它消除了层之间的 float<->int 转换成本。\n",
    "- 静态量化模型可能需要定期重新校准，以保持对分布漂移的鲁棒性。\n",
    "\n",
    "静态量化模型包括以下步骤：\n",
    "\n",
    "- 融合模块\n",
    "- 插入 Quant/DeQuant 存根\n",
    "- 准备融合模块（在层前和层后插入观察者）\n",
    "- 校准准备好的模块（传递代表数据）\n",
    "- 转换校准模块（替换为量化版本）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_fx\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 如果在ARM上运行，使用 `qnnpack`。\n",
    "backend = \"fbgemm\"  # 运行在 x86 CPU 上。\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    m = nn.Sequential(\n",
    "        nn.Conv2d(2, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, 3),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 急切的模式\n",
    "\n",
    "**融合**：就地融合用所述融合模块替换所述序列中的第一个模块，其余用相同模块替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvReLU2d(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Identity()\n",
       "  (2): ConvReLU2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_model()\n",
    "# fuse first Conv-ReLU pair\n",
    "torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)\n",
    "# fuse second Conv-ReLU pair\n",
    "torch.quantization.fuse_modules(m, ['2', '3'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "插入 stub："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub()\n",
       "  (1): ConvReLU2d(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Identity()\n",
       "  (3): ConvReLU2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): Identity()\n",
       "  (5): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sequential(torch.quantization.QuantStub(),\n",
    "                  *m,\n",
    "                  torch.quantization.DeQuantStub())\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/ao/quantization/observer.py:176: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (1): ConvReLU2d(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (2): Identity()\n",
       "  (3): ConvReLU2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (4): Identity()\n",
       "  (5): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "from torch.ao.quantization.quantize import prepare\n",
    "\n",
    "m.qconfig = get_default_qconfig(backend)\n",
    "prepare(m, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**校准**：为了方便起见，这个例子使用了随机数据。使用代表性（验证）数据代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode(): # PyTorch 1.9\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        x = torch.rand(1, 2, 28, 28)\n",
    "        m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0080]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (1): QuantizedConvReLU2d(2, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.008235916495323181, zero_point=0)\n",
       "  (2): Identity()\n",
       "  (3): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0039798179641366005, zero_point=0)\n",
       "  (4): Identity()\n",
       "  (5): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantization.convert(m, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 1 字节，而不是 FP32 的 4 字节\n",
    "print(m[1].weight().element_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FX GRAPH 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "from torch.ao.quantization import quantize_fx\n",
    "\n",
    "\n",
    "def calibrate(model, data_loader):\n",
    "    '''使用代表性（验证）数据来校准'''\n",
    "    model.eval()\n",
    "    # with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "        for image, _ in data_loader:\n",
    "            model(image)\n",
    "\n",
    "\n",
    "def ptq(float_model, sample_inference_data, backend='fbgemm'):\n",
    "    qconfig = get_default_qconfig(backend)\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "    float_model.eval()\n",
    "\n",
    "    prepared_model = quantize_fx.prepare_fx(float_model, qconfig_dict)\n",
    "    # 运行校准\n",
    "    calibrate(prepared_model, sample_inference_data)\n",
    "    return prepared_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "from torch.ao.quantization import quantize_fx\n",
    "\n",
    "def data_iter():\n",
    "    for _ in range(10):\n",
    "        yield torch.rand(1, 2, 28, 28), _\n",
    "\n",
    "m = create_model()\n",
    "model_prepared = ptq(m, data_iter(), backend='fbgemm')\n",
    "# 量化\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization-aware Training (QAT)\n",
    "\n",
    "![](images/qat-flowchart.svg)\n",
    "\n",
    "PTQ 方法对于大型模型非常好，但在较小的模型中准确性会受到影响。当然，这是由于将 FP32 的模型调整到 INT8 域时的数值精度损失。\n",
    "\n",
    "QAT 通过在训练损失中包含量化误差来解决这个问题，因此训练一个 INT8-first 模型。\n",
    "\n",
    "![](images/ptq-qat.png)\n",
    "\n",
    "所有的权重和偏置都存储在 FP32 中，反向传播照常发生。然而在正向传递中，量化是通过 `FakeQuantize` 模块进行内部模拟的。它们之所以被称为假的，是因为它们对数据进行量化和立即反量化，并添加与量化推理过程中可能遇到的类似的量化噪声。因此，最终的损失可以解释任何预期的量化误差。在此基础上进行优化，可以使模型在损失函数中识别出更宽的区域，并识别出 FP32 参数，这样量化到 INT8 不会显著影响精度。\n",
    "\n",
    "[![](images/qat-fake-quantization.png)](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt)\n",
    "\n",
    "- QAT 比 PTQ 具有更高的精度。\n",
    "- Qparams 可以在模型训练期间学习，以获得更细粒度的准确性（参见 [LearnableFakeQuantize](https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/_learnable_fake_quantize.py)）。\n",
    "- 在 QAT 中，重新训练一个模型的计算成本可以达到几百个 epoch。{cite:ps}`gholami2021survey`\n",
    "\n",
    "除了在将模型实际转换为量化版本之前的训练循环之外，QAT 遵循与 PTQ 相同的步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 运行在 x86 CPU 上。如果在 ARM 上运行，使用 \"qnnpack\"。\n",
    "backend = \"fbgemm\"\n",
    "\n",
    "m = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "融合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvReLU2d(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Identity()\n",
       "  (2): ConvReLU2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)  # 融合第一对 Conv-ReLU\n",
    "torch.quantization.fuse_modules(m, ['2', '3'], inplace=True)  # 融合第二对 Conv-ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "插入存根（打桩）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(torch.quantization.QuantStub(),\n",
    "                  *m,\n",
    "                  torch.quantization.DeQuantStub())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (1): ConvReLU2d(\n",
       "    2, 64, kernel_size=(3, 3), stride=(1, 1)\n",
       "    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (2): Identity()\n",
       "  (3): ConvReLU2d(\n",
       "    64, 128, kernel_size=(3, 3), stride=(1, 1)\n",
       "    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (4): Identity()\n",
       "  (5): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.train()\n",
    "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare_qat(m, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
    "def loss_fn(out, tgt): return torch.pow(tgt-out, 2).mean()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    x = torch.rand(10, 2, 24, 24)\n",
    "    out = m(x)\n",
    "    loss = loss_fn(out, torch.rand_like(out))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0080]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (1): QuantizedConvReLU2d(2, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.01154174655675888, zero_point=0)\n",
       "  (2): Identity()\n",
       "  (3): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004559975583106279, zero_point=0)\n",
       "  (4): Identity()\n",
       "  (5): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.eval()\n",
    "torch.quantization.convert(m, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 敏感性分析\n",
    "\n",
    "并不是所有层对量化的响应都是一样的，有些层对精度下降比其他层更敏感。确定最优的层组合以最小化精度下降是非常耗时的，因此 {cite:ps}`wu2020integer` 建议进行一次一次的灵敏度分析，以确定哪些层最敏感，并在这些层上保持 FP32 的精度。在他们的实验中，跳过 2 个 conv 层（在 MobileNet v1 的 28 个 conv 层中）使他们接近 FP32 的精度。使用 FX Graph 模式，可以创建自定义 `qconfigs` 来轻松做到这一点。\n",
    "\n",
    "```python\n",
    "# ONE-AT-A-TIME SENSITIVITY ANALYSIS \n",
    "\n",
    "for quantized_layer, _ in model.named_modules():\n",
    "  print(\"Only quantizing layer: \", quantized_layer)\n",
    "\n",
    "  # The module_name key allows module-specific qconfigs. \n",
    "  qconfig_dict = {\"\": None, \n",
    "  \"module_name\":[(quantized_layer, torch.quantization.get_default_qconfig(backend))]}\n",
    "\n",
    "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict)\n",
    "  # calibrate\n",
    "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "  # evaluate(model)\n",
    "```\n",
    "\n",
    "另一种方法是比较 FP32 和 INT8 层的统计数据；常用的度量有 SQNR（信号量化噪声比，即 Signal to Quantized Noise Ratio）和均方误差（Mean-Squre-Error）。这种比较分析也有助于指导进一步的优化。\n",
    "\n",
    "![](images/compare_output_ns.png)\n",
    "\n",
    "PyTorch 在数值套件下提供了帮助进行此分析的工具。从完整的教程中了解更多关于使用 [Numeric Suite](https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html) 的信息。\n",
    "\n",
    "```python\n",
    "# extract from https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html\n",
    "import torch.quantization._numeric_suite as ns\n",
    "\n",
    "def SQNR(x, y):\n",
    "    # Higher is better\n",
    "    Ps = torch.norm(x)\n",
    "    Pn = torch.norm(x-y)\n",
    "    return 20*torch.log10(Ps/Pn)\n",
    "\n",
    "wt_compare_dict = ns.compare_weights(fp32_model.state_dict(), int8_model.state_dict())\n",
    "for key in wt_compare_dict:\n",
    "    print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))\n",
    "\n",
    "act_compare_dict = ns.compare_model_outputs(fp32_model, int8_model, input_data)\n",
    "for key in act_compare_dict:\n",
    "    print(key, compute_error(act_compare_dict[key]['float'][0], act_compare_dict[key]['quantized'][0].dequantize()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对您工作流程的建议\n",
    "\n",
    "![](images/quantization-flowchart2.png)\n",
    "\n",
    "要点：\n",
    "\n",
    "- 大（10M+ 参数）模型对量化误差更具鲁棒性。\n",
    "- 从 FP32 检查点量化模型比从零开始训练 INT8 模型提供了更好的 accuracy。\n",
    "- 分析模型运行时是可选的，但它可以帮助识别阻碍推理的层。\n",
    "- 动态量化是一个简单的第一步，特别是当您的模型有许多线性或递归层时。\n",
    "- 使用逐通道对称量化借由 `MinMax` 观测者量化权重。使用逐张量仿射量化借由 `MovingAverageMinMax` 观测者量化激活。\n",
    "- 使用诸如 SQNR 之类的度量来确定哪些层最容易受到量化误差的影响。关闭这些层上的量化。\n",
    "- 使用 QAT 对原始训练调度的大约 $10\\%$ 进行微调，退火学习率（annealing learning rate）调度从初始训练学习率的 $1\\%$ 开始。\n",
    "- 如果上面的工作流程不适合你，我们想知道更多。发布一个包含你的代码细节的帖子（模型架构，准确性指标，尝试过的技术）。请抄送 [@suraj.pt](https://discuss.pytorch.org/u/suraj.pt/)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
