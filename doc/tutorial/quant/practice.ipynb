{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化实践\n",
    "\n",
    "参考：[Practical Quantization in PyTorch](https://pytorch.org/blog/quantization-in-practice/)\n",
    "\n",
    "{guilabel}`NN 量化目标`：运行更快、内存需求更低。\n",
    "\n",
    "- 量化源于信息压缩；在深度神经网络中，它指的是降低其权重和/或激活的数值精度。\n",
    "- 过度参数化的 DNN 有更多的 **自由度**，这使它们成为信息压缩的良好候选对象 {cite:ps}`gholami2021survey`。\n",
    "\n",
    "当量化模型时，通常会发生两件事——模型变得更小，运行效率更高。硬件供应商明确地允许更快地处理 8 位数据（而不是 32 位数据），从而获得更高的 **吞吐量** （throughput）。更小的模型具有更低的内存占用和功耗 {cite:ps}`krishnamoorthi2018quantizing`，这对于边缘部署至关重要。\n",
    "\n",
    "## 量化背景知识\n",
    "\n",
    "### 映射函数\n",
    "\n",
    "映射函数：将值从浮点数映射到整数空间的函数。常用的映射函数是由 $Q(r) = \\operatorname{round}(r/S + Z)$ 给出的线性变换，其中为 $r$ 为输入，$S, Z$ 为量化参数（quantization parameters）。为了重新变换为浮点空间，反函数由 $\\overline{r} = (Q(r) - Z) \\cdot S$ 给出（被称为 **反量化**，即 dequantization）。\n",
    "\n",
    "```{note}\n",
    "$\\overline{r} \\neq r$，它们之间的差异构成了量化误差。\n",
    "```\n",
    "\n",
    "### 量化参数\n",
    "\n",
    "映射函数由缩放因子 $S$ 和零点 $Z$ 所参数化。$S$ 仅仅是输入范围与输出范围的比值 $S = \\frac {\\beta - \\alpha}{\\beta_q - \\alpha_q}$。这里 $[\\alpha, \\beta]$ 是输入的裁剪（clipping）范围，即允许输入的边界。$[\\alpha_q, \\beta_q]$ 是它被映射到的量化输出空间的范围。对于 8 位量化，输出范围 $0 \\leq \\beta_q - \\alpha_q \\leq 2^8 -1$。$Z = -(\\frac {\\alpha}{S} - \\alpha_q)$ 作为偏置，以确保输入空间中的 $0$ 完全映射到量化空间中的 $0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 校准\n",
    "\n",
    "选择输入裁剪范围的过程称为 **校准** （calibration）。最简单的方法（也是 PyTorch 中的默认方法）是记录正在运行的最小值和最大值，并将它们赋值给 $\\alpha$ 和 $\\beta$。[TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/calib.html) 使用熵最小化（KL 散度），均方误差最小化，或输入范围的百分位数。\n",
    "\n",
    "在 PyTorch 中，{class}`Observer <torch.ao.quantization.observer.ObserverBase>` 模块收集关于输入值的统计信息并计算 `qparams` $S, Z$。不同的校准方案会产生不同的量化输出，最好通过经验验证哪种方案最适合您的应用程序和体系结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} 观测器基类\n",
    "{class}`~torch.ao.quantization.observer.ObserverBase` 是观测器（observer）模块的基类。在 {func}`forward` 中，将更新观测张量的统计信息。观测器应该提供用于收集的统计信息并计算量化参数的 `calculate_qparams` 函数。\n",
    "```\n",
    "\n",
    "### 均匀量化\n",
    "\n",
    "````{admonition} 均匀量化的观测器基类\n",
    "{class}`~torch.ao.quantization.observer.UniformQuantizationObserverBase` 是所有使用均匀量化（uniform quantization，{cite:p}`UniformQuantizers21`）来计算 `scale` 和 `zero_point` 的观测器基类。\n",
    "\n",
    "参数：\n",
    "- `dtype`：量化后的数据类型。\n",
    "- `qscheme`：量化方案。\n",
    "- `reduce_range`：使用 1 bit 归约量化数据类型的范围。这有时是为了避免指令溢出所必需的。\n",
    "- `quant_min`：最小量化值。如果未指定，它将遵循 8 bit 设置。\n",
    "- `quant_max`：最大量化值。如果未指定，它将遵循 8 bit 设置。\n",
    "- `eps`：float32 的 $\\epsilon$ 值，默认为 {data}`torch.finfo(torch.float32).eps`。\n",
    "\n",
    "```{warning}\n",
    "1. {attr}`dtype` 仅支持 ``torch.qint8`` 或者 ``torch.quint8``。\n",
    "2. {attr}`qscheme` 有：\n",
    "\n",
    "    - ``torch.per_tensor_affine``\n",
    "    - ``torch.per_tensor_symmetric``\n",
    "    - ``torch.per_channel_affine``\n",
    "    - ``torch.per_channel_symmetric``\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`````{admonition} MinMaxObserver\n",
    "{class}`~torch.ao.quantization.observer.MinMaxObserver` 观测器模块，根据运行的最小值和最大值计算量化参数。\n",
    "\n",
    "此观测器使用张量最小/最大统计量来计算量化参数。该模块记录传入张量的运行最小值和最大值，并利用该统计量计算量化参数。\n",
    "\n",
    "记作 running min/max 为 {math}`x_{\\min}` 和 {math}`x_{\\max}`；`scale` 和零点为 {math}`s`，{math}`z`，则：\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "x_\\text{min} &= \\begin{cases}\n",
    "    \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\\n",
    "    \\min\\left(x_\\text{min}, \\min(X)\\right) & \\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "x_\\text{max} &= \\begin{cases}\n",
    "    \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\\n",
    "    \\max\\left(x_\\text{max}, \\max(X)\\right) & \\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "其中 $X$ 是被观测张量。\n",
    "\n",
    "为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{if 对称:}&\\\\\n",
    "&s = 2 \\max(|x_\\text{min}|, x_\\text{max}) /\n",
    "    \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\\n",
    "&z = \\begin{cases}\n",
    "    0 & \\text{if dtype is qint8} \\\\\n",
    "    128 & \\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "\\text{else:}&\\\\\n",
    "    &s = \\left( x_\\text{max} - x_\\text{min}  \\right ) /\n",
    "        \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\\n",
    "    &z = Q_\\text{min} - \\text{round}(x_\\text{min} / s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "这里 $Q_{\\min}$ 和 $Q_{\\max}$ 量化的数据类型的最小最大值。\n",
    "\n",
    "如果 running 最小值等于 running 最大值，则 `scale` 和 `zero_point` 设置为 $1.0$ 和 $0$。\n",
    "\n",
    "````{admonition} MovingAverageMinMaxObserver\n",
    "{class}`~torch.ao.quantization.observer.MovingAverageMinMaxObserver` 是 {class}`~torch.ao.quantization.observer.MinMaxObserver` 的子类，它是基于最小值和最大值的滑动平均值计算量化参数的观测器模块。\n",
    "\n",
    "此观测器根据传入张量的极小值和极大值的滑动\n",
    "\n",
    "平均值计算量化参数。该模块记录传入张量的平均最小值和最大值，并利用该统计量计算量化参数。\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    x_\\text{min} = \\begin{cases}\n",
    "        \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\\n",
    "        (1 - c) x_\\text{min} + c \\min(X) & \\text{otherwise}\n",
    "    \\end{cases}\\\\\n",
    "    x_\\text{max} = \\begin{cases}\n",
    "        \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\\n",
    "        (1 - c) x_\\text{max} + c \\max(X) & \\text{otherwise}\n",
    "    \\end{cases}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "其中 $c$ 是 ``averaging_constant``。\n",
    "\n",
    "```{note}\n",
    "仅用于 ``torch.per_tensor_affine`` 量化方案。\n",
    "```\n",
    "````\n",
    "`````\n",
    "\n",
    "`````{admonition} PerChannelMinMaxObserver\n",
    "{class}`~torch.ao.quantization.observer.PerChannelMinMaxObserver` 观测器模块，根据逐通道运行的最小值和最大值计算量化参数。\n",
    "\n",
    "此观测器使用张量最小/最大统计量来逐通道计算量化参数。该模块记录传入张量的运行最小值和最大值，并使用该统计量计算量化\n",
    "参数。\n",
    "\n",
    "参数：\n",
    "\n",
    "- `ch_axis`：通道的轴。\n",
    "\n",
    "量化参数的计算方法与 {class}`~torch.ao.quantization.observer.MinMaxObserver` 相同，区别在于运行的最小/最大值存储在每个通道上。\n",
    "\n",
    "此观测器 存在子类 {class}`~torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver`。\n",
    "`````\n",
    "\n",
    "````{admonition} HistogramObserver\n",
    "{class}`~torch.ao.quantization.observer.HistogramObserver` 记录张量值的运行直方图（histogram）以及最小/最大值。\n",
    "\n",
    "参数：\n",
    "\n",
    "- `bins`：用于直方图的箱的数量。\n",
    "- `upsample_rate`：直方图上采样的因子，这用于插值直方图与不同的观测范围。\n",
    "\n",
    "`scale` 和零点计算方式如下：\n",
    "\n",
    "1. 创建传入输入的直方图：直方图的计算是连续的，每个 bin 的范围随着观测到的每个新张量而变化。\n",
    "2. 搜索直方图中的分布，寻找最佳的最小/最大值：对最小/最大值的搜索确保了相对于浮点模型的量化误差的最小化。\n",
    "3. 计算`scale` 和零点的方法同 {class}`~torch.ao.quantization.observer.MinMaxObserver`。\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.8635,  1.4422],\n",
       "         [-0.1421, -1.5490],\n",
       "         [ 0.6654, -0.6494]]),\n",
       " tensor([[ 0.0805, -0.4340],\n",
       "         [ 0.2766,  0.5731],\n",
       "         [ 1.1772, -0.7429]])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.ao.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
    "\n",
    "# 设置输入\n",
    "C, L = 3, 2\n",
    "normal = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "inputs = [normal.sample((C, L)),\n",
    "          normal.sample((C, L))]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置观测："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "observers = [MinMaxObserver(),\n",
    "             MovingAverageMinMaxObserver(),\n",
    "             HistogramObserver()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算并查看量化参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxObserver (tensor([0.0117]), tensor([132], dtype=torch.int32))\n",
      "MovingAverageMinMaxObserver (tensor([0.0117]), tensor([132], dtype=torch.int32))\n",
      "HistogramObserver (tensor([0.0090]), tensor([150], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "for obs in observers:\n",
    "    for x in inputs:\n",
    "        obs(x)\n",
    "    print(obs.__class__.__name__, obs.calculate_qparams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 仿射和对称量化方案\n",
    "\n",
    "仿射（affine）或非对称量化（asymmetric quantization）方案分配输入范围的最小和最大观测值。仿射方案通常提供更小的剪切范围，并且对于量化非负激活非常有用（如果你的输入张量永远都不是负的，你就不需要输入范围包含负值）。计算范围为 $\\alpha=\\min(r), \\beta = \\max(r)$。当用于权值张量 {cite:ps}`wu2020integer` 时，仿射量化会导致更昂贵的计算推理。\n",
    "\n",
    "对称量化（Symmetric quantization）方案将输入范围集中在 $0$ 附近，消除了计算零点偏置的需要。计算范围为 $-\\alpha=\\beta=\\max(|\\max(r)|,|\\min(r)|)$。\n",
    "\n",
    "对于倾斜的信号（如非负激活），这可能会导致糟糕的量化分辨率（quantization resolution），因为剪辑范围包括从未在输入中出现的值（参见下面的 pyplot）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_symmetric_range(x):\n",
    "    '''获取对称范围'''\n",
    "    beta = torch.max(x.max(), x.min().abs())\n",
    "    return -beta.item(), beta.item()\n",
    "\n",
    "\n",
    "def get_affine_range(x):\n",
    "    '''获取仿射范围'''\n",
    "    return x.min().item(), x.max().item()\n",
    "\n",
    "\n",
    "def plot(plt, data, scheme):\n",
    "    '''画出不同方案的分布'''\n",
    "    boundaries = get_affine_range(data) if scheme == 'affine' \\\n",
    "        else get_symmetric_range(data)\n",
    "    a, _, _ = plt.hist(data, density=True, bins=100, histtype='stepfilled')\n",
    "    ymin, ymax = np.quantile(a[a > 0], [0.25, 0.95])\n",
    "    plt.vlines(x=boundaries, ls='--', colors='purple', ymin=ymin, ymax=ymax)\n",
    "\n",
    "\n",
    "# 模拟激活和权重\n",
    "act = torch.distributions.pareto.Pareto(1, 10).sample((1, 1024))\n",
    "weights = torch.distributions.normal.Normal(0, 0.14).sample((3, 64, 7, 7)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "plot(axs[0, 0], act, 'affine')\n",
    "axs[0, 0].set_title(\"Activation, Affine-Quantized\")\n",
    "plot(axs[0, 1], act, 'symmetric')\n",
    "axs[0, 1].set_title(\"Activation, Symmetric-Quantized\")\n",
    "plot(axs[1, 0], weights, 'affine')\n",
    "axs[1, 0].set_title(\"Weights, Affine-Quantized\")\n",
    "plot(axs[1, 1], weights, 'symmetric')\n",
    "axs[1, 1].set_title(\"Weights, Symmetric-Quantized\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，你可以在初始化 `Observer` 时指定仿射或对称模式。注意，并非所有 `observer` 都支持这两种方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qscheme: torch.per_tensor_affine | (tensor([0.0117]), tensor([132], dtype=torch.int32))\n",
      "Qscheme: torch.per_tensor_symmetric | (tensor([0.0121]), tensor([128]))\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
    "    obs = MovingAverageMinMaxObserver(qscheme=qscheme)\n",
    "    for x in inputs:\n",
    "        obs(x)\n",
    "    print(f\"Qscheme: {qscheme} | {obs.calculate_qparams()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逐张量和逐通道量化方案\n",
    "\n",
    "量化参数可以作为整体计算层的整个权值张量，也可以单独计算每个通道的权值张量。在每张量中，对层中的所有通道应用相同的剪切范围：\n",
    "\n",
    "![](images/tensor-quantization.png)\n",
    "\n",
    "对于权值量化，逐通道（Per-Channel）对称量化提供了更好的精度；逐张量（Per-Tensor）量化的性能很差，这可能是由于不同通道之间的转换权值与批量范数折叠（batchnorm folding） {cite:ps}`wu2020integer` 差异很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0090, 0.0060, 0.0052]), tensor([ 96, 255, 126], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
    "# 计算全部 `C` 通道的 qparams\n",
    "obs = MovingAveragePerChannelMinMaxObserver(ch_axis=0)\n",
    "for x in inputs:\n",
    "    obs(x)\n",
    "print(obs.calculate_qparams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0060, 0.0117]), tensor([143, 132], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "obs = MovingAveragePerChannelMinMaxObserver(ch_axis=1)\n",
    "for x in inputs:\n",
    "    obs(x)\n",
    "print(obs.calculate_qparams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后端引擎\n",
    "\n",
    "目前，量化算子通过 [FBGEMM 后端](https://github.com/pytorch/FBGEMM) 在 x86 机器上运行，或者在 ARM 机器上使用 [QNNPACK](https://github.com/pytorch/QNNPACK) 原语。服务器 GPU 的后端支持（通过 TensorRT 和 cuDNN）即将推出。了解更多关于将量化扩展到自定义后端：[RFC-0019](https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "backend = 'fbgemm'  # if x86 else 'qnnpack'\n",
    "qconfig = get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `QConfig`\n",
    "\n",
    "{class}`~torch.ao.quantization.qconfig.QConfig` NamedTuple 存储用于量化激活和权重的 Observer 和量化方案。\n",
    "\n",
    "一定要传递 `Observer` 类（而不是实例），或者可以返回 `Observer` 实例的可调用对象。使用 {func}`with_args` 覆盖默认参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, qscheme=torch.per_tensor_affine){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, qscheme=torch.qint8){})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization.qconfig import QConfig\n",
    "\n",
    "my_qconfig = QConfig(\n",
    "    activation=MovingAverageMinMaxObserver.with_args(\n",
    "        qscheme=torch.per_tensor_affine),\n",
    "    weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8)\n",
    ")\n",
    "my_qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 PyTorch 中\n",
    "\n",
    "PyTorch 允许您使用几种不同的方式来量化您的模型：\n",
    "\n",
    "- Eager 模式 v/s FX Graph 模式：如果你更喜欢灵活但手动的，或受限的自动过程\n",
    "- 静态 v/s 动态：如果量化激活（层的输出）的 `qparams` 为所有输入预先计算，或对每个输入重新计算，\n",
    "- 量化感知训练（quantization-aware training） v/s 训练后量化（post-training quantization）：如果 `qparams` 是在有或没有重新训练的情况下计算的\n",
    "\n",
    "FX Graph Mode 自动融合符合条件的模块，插入 Quant/DeQuant stub，校准模型并返回量化模块——所有这些都是在两个方法调用中进行的——但仅适用于 [可符号跟踪](https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace) 的网络。 \n",
    "\n",
    "在 DNN 中，量化的合适候选对象是 FP32 权值（层参数）和激活（层输出）。量化权值可以减少模型的大小。量化激活通常会导致更快的推理。\n",
    "\n",
    "例如，50 层 ResNet 网络有近 2600 万个权值参数，在正向传程中计算近 1600 万个激活。\n",
    "\n",
    "### Post-Training Dynamic/Weight-only Quantization\n",
    "\n",
    "这里模型的权值是预量化的；在推理期间，激活是动态量化的。这是所有方法中最简单的一种，它在 {func}`~torch.ao.quantization.quantize.quantize_dynamic` 中有一行 API 调用。目前只支持线性和循环（LSTM、GRU、RNN）层进行动态量化。\n",
    "\n",
    "- 可以导致更高的精度，因为每个输入的裁剪范围是精确校准的\n",
    "- 对于像 LSTM 和 Transformer 这样的模型，动态量化是首选的，因为从内存中写入/检索模型的权值会受制于带宽\n",
    "- 在运行时对每个层的激活进行校准和量化会增加计算开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 小 model\n",
    "def create_model():\n",
    "    m = nn.Sequential(\n",
    "        nn.Conv2d(2, 64, (8,)),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 10),\n",
    "        nn.LSTM(10, 10))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} eager 模式\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 64, kernel_size=(8,), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): DynamicQuantizedLinear(in_features=16, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (3): DynamicQuantizedLSTM(10, 10)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization.quantize import quantize_dynamic\n",
    "\n",
    "m = create_model()\n",
    "m.eval()\n",
    "model_quantized = quantize_dynamic(\n",
    "    model=m, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False\n",
    ")\n",
    "model_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} FX 模式\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/nn/quantized/_reference/modules/utils.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"scale\"], dtype=torch.float, device=device))\n",
      "/media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/nn/quantized/_reference/modules/utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"zero_point\"], dtype=zero_point_dtype, device=device))\n",
      "/media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/nn/quantized/_reference/modules/rnn.py:302: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"scale\"], dtype=torch.float, device=device))\n",
      "/media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/nn/quantized/_reference/modules/rnn.py:305: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(weight_qparams[\"zero_point\"], dtype=torch.int, device=device))\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization import quantize_fx\n",
    "from torch.ao.quantization.qconfig import default_dynamic_qconfig\n",
    "\n",
    "m = create_model()\n",
    "m.eval()\n",
    "# 空键表示应用于所有模块的默认值\n",
    "qconfig_dict = {\"\": default_dynamic_qconfig}\n",
    "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict)\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Training Static Quantization (PTQ)\n",
    "\n",
    "PTQ 也预量化模型权重，但不是动态校准激活，而是使用验证数据对剪切范围进行预校准和固定（“静态”）。在推理过程中，激活在运算之间保持量化精度。大约 100 个小批次的代表性数据就足以校准观测者。为了方便起见，下面的例子在校准中使用了随机数据——在应用程序中使用随机数据将导致错误的 `qparams`。\n",
    "\n",
    "![](images/ptq-flowchart.svg)\n",
    "\n",
    "[模块融合](https://pytorch.org/tutorials/recipes/fuse.html) 将多个顺序模块（如：`[Conv2d, BatchNorm, ReLU]`）组合成一个。融合模块意味着编译器只需要运行一个内核而不是多个；这可以通过减少量化误差来提高速度和准确性。\n",
    "\n",
    "- 静态量化比动态量化具有更快的推理速度，因为它消除了层之间的 float<->int 转换成本。\n",
    "- 静态量化模型可能需要定期重新校准，以保持对分布漂移的鲁棒性。\n",
    "\n",
    "静态量化模型包括以下步骤：\n",
    "\n",
    "- 融合模块\n",
    "- 插入 Quant/DeQuant 存根\n",
    "- 准备融合模块（在层前和层后插入观察者）\n",
    "- 校准准备好的模块（传递代表数据）\n",
    "- 转换校准模块（替换为量化版本）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_fx\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 如果在ARM上运行，使用 `qnnpack`。\n",
    "backend = \"fbgemm\"  # 运行在 x86 CPU 上。\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    m = nn.Sequential(\n",
    "        nn.Conv2d(2, 64, 3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 128, 3),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 急切的模式\n",
    "\n",
    "**融合**：就地融合用所述融合模块替换所述序列中的第一个模块，其余用相同模块替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvReLU2d(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Identity()\n",
       "  (2): ConvReLU2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_model()\n",
    "# fuse first Conv-ReLU pair\n",
    "torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)\n",
    "# fuse second Conv-ReLU pair\n",
    "torch.quantization.fuse_modules(m, ['2', '3'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "插入 stub："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub()\n",
       "  (1): ConvReLU2d(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (2): Identity()\n",
       "  (3): ConvReLU2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (4): Identity()\n",
       "  (5): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sequential(torch.quantization.QuantStub(),\n",
    "                  *m,\n",
    "                  torch.quantization.DeQuantStub())\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/ao/quantization/observer.py:176: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (1): ConvReLU2d(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (2): Identity()\n",
       "  (3): ConvReLU2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (4): Identity()\n",
       "  (5): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "from torch.ao.quantization.quantize import prepare\n",
    "\n",
    "m.qconfig = get_default_qconfig(backend)\n",
    "prepare(m, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**校准**：为了方便起见，这个例子使用了随机数据。使用代表性（验证）数据代替。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.inference_mode(): # PyTorch 1.9\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        x = torch.rand(1, 2, 28, 28)\n",
    "        m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0080]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (1): QuantizedConvReLU2d(2, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.01072006206959486, zero_point=0)\n",
       "  (2): Identity()\n",
       "  (3): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0029677667189389467, zero_point=0)\n",
       "  (4): Identity()\n",
       "  (5): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantization.convert(m, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 1 字节，而不是 FP32 的 4 字节\n",
    "print(m[1].weight().element_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FX GRAPH 模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "from torch.ao.quantization import quantize_fx\n",
    "\n",
    "\n",
    "def calibrate(model, data_loader):\n",
    "    '''使用代表性（验证）数据来校准'''\n",
    "    model.eval()\n",
    "    # with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "        for image, _ in data_loader:\n",
    "            model(image)\n",
    "\n",
    "\n",
    "def ptq(float_model, sample_inference_data, backend='fbgemm'):\n",
    "    qconfig = get_default_qconfig(backend)\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "    float_model.eval()\n",
    "\n",
    "    prepared_model = quantize_fx.prepare_fx(float_model, qconfig_dict)\n",
    "    # 运行校准\n",
    "    calibrate(prepared_model, sample_inference_data)\n",
    "    return prepared_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "from torch.ao.quantization import quantize_fx\n",
    "\n",
    "def data_iter():\n",
    "    for _ in range(10):\n",
    "        yield torch.rand(1, 2, 28, 28), _\n",
    "\n",
    "m = create_model()\n",
    "model_prepared = ptq(m, data_iter(), backend='fbgemm')\n",
    "# 量化\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization-aware Training (QAT)\n",
    "\n",
    "![](images/qat-flowchart.svg)\n",
    "\n",
    "PTQ 方法对于大型模型非常好，但在较小的模型中准确性会受到影响。当然，这是由于将 FP32 的模型调整到 INT8 域时的数值精度损失。\n",
    "\n",
    "QAT 通过在训练损失中包含量化误差来解决这个问题，因此训练一个 INT8-first 模型。\n",
    "\n",
    "![](images/ptq-qat.png)\n",
    "\n",
    "所有的权重和偏置都存储在 FP32 中，反向传播照常发生。然而在正向传递中，量化是通过 `FakeQuantize` 模块进行内部模拟的。它们之所以被称为假的，是因为它们对数据进行量化和立即反量化，并添加与量化推理过程中可能遇到的类似的量化噪声。因此，最终的损失可以解释任何预期的量化误差。在此基础上进行优化，可以使模型在损失函数中识别出更宽的区域，并识别出 FP32 参数，这样量化到 INT8 不会显著影响精度。\n",
    "\n",
    "[![](images/qat-fake-quantization.png)](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt)\n",
    "\n",
    "- QAT 比 PTQ 具有更高的精度。\n",
    "- Qparams 可以在模型训练期间学习，以获得更细粒度的准确性（参见 [LearnableFakeQuantize](https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/_learnable_fake_quantize.py)）。\n",
    "- 在 QAT 中，重新训练一个模型的计算成本可以达到几百个 epoch。{cite:ps}`gholami2021survey`\n",
    "\n",
    "除了在将模型实际转换为量化版本之前的训练循环之外，QAT 遵循与 PTQ 相同的步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 运行在 x86 CPU 上。如果在 ARM 上运行，使用 \"qnnpack\"。\n",
    "backend = \"fbgemm\"\n",
    "\n",
    "m = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "融合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvReLU2d(\n",
       "    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (1): Identity()\n",
       "  (2): ConvReLU2d(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (3): Identity()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)  # 融合第一对 Conv-ReLU\n",
    "torch.quantization.fuse_modules(m, ['2', '3'], inplace=True)  # 融合第二对 Conv-ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "插入存根（打桩）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(torch.quantization.QuantStub(),\n",
    "                  *m,\n",
    "                  torch.quantization.DeQuantStub())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (1): ConvReLU2d(\n",
       "    2, 64, kernel_size=(3, 3), stride=(1, 1)\n",
       "    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (2): Identity()\n",
       "  (3): ConvReLU2d(\n",
       "    64, 128, kernel_size=(3, 3), stride=(1, 1)\n",
       "    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (4): Identity()\n",
       "  (5): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.train()\n",
    "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare_qat(m, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
    "def loss_fn(out, tgt): return torch.pow(tgt-out, 2).mean()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    x = torch.rand(10, 2, 24, 24)\n",
    "    out = m(x)\n",
    "    loss = loss_fn(out, torch.rand_like(out))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (1): QuantizedConvReLU2d(2, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.009738738648593426, zero_point=0)\n",
       "  (2): Identity()\n",
       "  (3): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004433792550116777, zero_point=0)\n",
       "  (4): Identity()\n",
       "  (5): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.eval()\n",
    "torch.quantization.convert(m, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 敏感性分析\n",
    "\n",
    "并不是所有层对量化的响应都是一样的，有些层对精度下降比其他层更敏感。确定最优的层组合以最小化精度下降是非常耗时的，因此 {cite:ps}`wu2020integer` 建议进行一次一次的灵敏度分析，以确定哪些层最敏感，并在这些层上保持 FP32 的精度。在他们的实验中，跳过 2 个 conv 层（在 MobileNet v1 的 28 个 conv 层中）使他们接近 FP32 的精度。使用 FX Graph 模式，可以创建自定义 `qconfigs` 来轻松做到这一点。\n",
    "\n",
    "```python\n",
    "# ONE-AT-A-TIME SENSITIVITY ANALYSIS \n",
    "\n",
    "for quantized_layer, _ in model.named_modules():\n",
    "  print(\"Only quantizing layer: \", quantized_layer)\n",
    "\n",
    "  # The module_name key allows module-specific qconfigs. \n",
    "  qconfig_dict = {\"\": None, \n",
    "  \"module_name\":[(quantized_layer, torch.quantization.get_default_qconfig(backend))]}\n",
    "\n",
    "  model_prepared = quantize_fx.prepare_fx(model, qconfig_dict)\n",
    "  # calibrate\n",
    "  model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "  # evaluate(model)\n",
    "```\n",
    "\n",
    "另一种方法是比较 FP32 和 INT8 层的统计数据；常用的度量有 SQNR（信号量化噪声比，即 Signal to Quantized Noise Ratio）和均方误差（Mean-Squre-Error）。这种比较分析也有助于指导进一步的优化。\n",
    "\n",
    "![](images/compare_output_ns.png)\n",
    "\n",
    "PyTorch 在数值套件下提供了帮助进行此分析的工具。从完整的教程中了解更多关于使用 [Numeric Suite](https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html) 的信息。\n",
    "\n",
    "```python\n",
    "# extract from https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html\n",
    "import torch.quantization._numeric_suite as ns\n",
    "\n",
    "def SQNR(x, y):\n",
    "    # Higher is better\n",
    "    Ps = torch.norm(x)\n",
    "    Pn = torch.norm(x-y)\n",
    "    return 20*torch.log10(Ps/Pn)\n",
    "\n",
    "wt_compare_dict = ns.compare_weights(fp32_model.state_dict(), int8_model.state_dict())\n",
    "for key in wt_compare_dict:\n",
    "    print(key, compute_error(wt_compare_dict[key]['float'], wt_compare_dict[key]['quantized'].dequantize()))\n",
    "\n",
    "act_compare_dict = ns.compare_model_outputs(fp32_model, int8_model, input_data)\n",
    "for key in act_compare_dict:\n",
    "    print(key, compute_error(act_compare_dict[key]['float'][0], act_compare_dict[key]['quantized'][0].dequantize()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对您工作流程的建议\n",
    "\n",
    "![](images/quantization-flowchart2.png)\n",
    "\n",
    "要点：\n",
    "\n",
    "- 大（10M+ 参数）模型对量化误差更具鲁棒性。\n",
    "- 从 FP32 检查点量化模型比从零开始训练 INT8 模型提供了更好的 accuracy。\n",
    "- 分析模型运行时是可选的，但它可以帮助识别阻碍推理的层。\n",
    "- 动态量化是一个简单的第一步，特别是当您的模型有许多线性或递归层时。\n",
    "- 使用逐通道对称量化借由 `MinMax` 观测者量化权重。使用逐张量仿射量化借由 `MovingAverageMinMax` 观测者量化激活。\n",
    "- 使用诸如 SQNR 之类的度量来确定哪些层最容易受到量化误差的影响。关闭这些层上的量化。\n",
    "- 使用 QAT 对原始训练调度的大约 $10\\%$ 进行微调，退火学习率（annealing learning rate）调度从初始训练学习率的 $1\\%$ 开始。\n",
    "- 如果上面的工作流程不适合你，我们想知道更多。发布一个包含你的代码细节的帖子（模型架构，准确性指标，尝试过的技术）。请抄送 [@suraj.pt](https://discuss.pytorch.org/u/suraj.pt/)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
