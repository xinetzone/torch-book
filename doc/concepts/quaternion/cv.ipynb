{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四元数 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import v2 \n",
    "from torchvision import datasets, models\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from pathlib import Path\n",
    "from taolib.utils.logger import config_logging\n",
    "\n",
    "temp_dir = Path(\".\").resolve() / \".temp\"\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "config_logging(f'{temp_dir}/0-compile.log', \"root\", maxBytes=5000000, backupCount=7)\n",
    "torch.cuda.empty_cache() # 清空 GPU 缓存\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CIFAR10:\n",
    "    root_dir: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.root_dir = Path(self.root_dir)\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        self.train_transform = v2.Compose([ # 在高度和宽度上将图像放大到40像素的正方形\n",
    "            v2.Resize(40),\n",
    "            # 随机裁剪出一个高度和宽度均为40像素的正方形图像，\n",
    "            # 生成一个面积为原始图像面积0.64到1倍的小正方形，\n",
    "            # 然后将其缩放为高度和宽度均为32像素的正方形\n",
    "            v2.RandomResizedCrop(32, scale=(0.64, 1.0), ratio=(1.0, 1.0)),\n",
    "            v2.RandomHorizontalFlip(),\n",
    "            v2.ToImage(),\n",
    "        ])\n",
    "        self.val_transform = v2.ToImage()\n",
    "        self.train = datasets.CIFAR10(\n",
    "            root=self.root_dir, \n",
    "            train=True, download=True, \n",
    "            transform=self.train_transform,\n",
    "        )\n",
    "        self.val = datasets.CIFAR10(\n",
    "            root=self.root_dir, \n",
    "            train=False, download=True,\n",
    "            transform=self.val_transform,\n",
    "        )\n",
    "        self.normalize = nn.Sequential(\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(self.mean, self.std)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10(temp_dir/\"data\")\n",
    "batch_size = 16\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(dataset.train, batch_size=batch_size, shuffle=True),\n",
    "    \"val\": DataLoader(dataset.val, batch_size=batch_size, shuffle=False),\n",
    "}\n",
    "dataset_sizes = {\n",
    "    \"train\": len(dataset.train),\n",
    "    \"val\": len(dataset.val),\n",
    "}\n",
    "class_names = dataset.train.classes\n",
    "# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
    "# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taolib.plot.image import show_images\n",
    "classe_names = dataset.train.classes\n",
    "idx_to_class = {v:k for k, v in dataset.train.class_to_idx.items()}\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders[\"val\"]))\n",
    "inputs, classes = inputs[:8], classes[:8]\n",
    "inputs = [v2.ToPILImage()(inp) for inp in inputs]\n",
    "# inputs = inputs.numpy().transpose((0, 2, 3, 1))\n",
    "show_images(inputs, 2, 4, scale=2, titles=[idx_to_class[x.item()] for x in classes]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "from core_qnn.quaternion_layers import QuaternionConv\n",
    "\n",
    "def qconv(f_conv, in_channels, out_channels, groups):\n",
    "    return QuaternionConv(\n",
    "        in_channels, out_channels,\n",
    "        f_conv.kernel_size, f_conv.stride, \n",
    "        f_conv.dilation, f_conv.padding, groups, f_conv.bias,\n",
    "        init_criterion='glorot',\n",
    "        weight_init='quaternion', seed=None, operation='convolution2d', \n",
    "        rotation=False, quaternion_format=True, scale=False\n",
    "    )\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, transform: nn.Module, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        model = models.mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "        model.features[0] = Conv2dNormActivation(3, 32, stride=1, norm_layer=nn.BatchNorm2d, activation_layer=nn.ReLU6)\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, len(class_names))\n",
    "        # model.features[18][0] = qconv(model.features[18][0], model.features[18][0].groups)\n",
    "        for index, blk in enumerate(model.features[1:18]):\n",
    "            if index < 2:\n",
    "                continue\n",
    "            # if index==0:\n",
    "            #     # k = 1\n",
    "            #     # model.features[index+1].conv[k] = qconv(blk.conv[k], blk.in_channels, blk.out_channels, blk.conv[k].groups)\n",
    "            #     continue\n",
    "            # elif index<13:\n",
    "            #     # for param in blk.parameters():\n",
    "            #     #     param.requires_grad = False # 冻结参数\n",
    "            #     continue\n",
    "            else:\n",
    "                # model.features[index+1].conv[0][0] = qconv(blk.conv[0][0], blk.in_channels, blk.out_channels, 1)\n",
    "                k = 2\n",
    "                model.features[index+1].conv[k] = qconv(blk.conv[k], blk.conv[k].in_channels, blk.conv[k].out_channels, blk.conv[k].groups)\n",
    "                # k = 1\n",
    "                # model.features[index+1].conv[k][0] = qconv(blk.conv[k][0], blk.in_channels, blk.out_channels, 1) # 普通模式\n",
    "                # model.features[index+1].conv[k][0] = qconv(\n",
    "                #     blk.conv[k][0], \n",
    "                #     4, \n",
    "                #     4, \n",
    "                #     4\n",
    "                # )\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transform(x)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "lr = 0.00142857\n",
    "lr_decay = 0.0857142\n",
    "weight_decay = 0.00857142\n",
    "momentum = 0.857142\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_ft = Model(dataset.normalize)\n",
    "optimizer_ft = optim.SGD(\n",
    "    model_ft.parameters(), lr=lr, momentum=momentum, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=4, gamma=lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, blk in enumerate(model_ft.model.features[1:18]):\n",
    "#     print(blk.conv)\n",
    "#     # model_ft.model.features[index+1].conv[0][0] = blk.conv[0][0]\n",
    "#     # if index==2:\n",
    "#     #     break\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import fx\n",
    "from tvm.relax.frontend.torch import from_fx\n",
    "from torch import _dynamo as dynamo\n",
    "\n",
    "input_info = [((1, 3, 32, 32), \"float32\")]\n",
    "# graph_module = fx.symbolic_trace(model_ft.eval())\n",
    "# \n",
    "scripted_model = torch.jit.trace(model_ft.model.eval(), torch.randn((1, 3, 32, 32))).eval()\n",
    "# 保存模型\n",
    "torch.jit.save(scripted_model, temp_dir/'test.pt')\n",
    "# scripted_model = torch.jit.load(temp_dir/'test.pt')\n",
    "# scripted_model = torch.jit.script(scripted_model)\n",
    "# mod = from_fx(scripted_model, input_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和评估模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taolib.utils.timer import Timer\n",
    "from torch_book.vision.classifier import Classifier\n",
    "timer = Timer()\n",
    "classifier = Classifier(\n",
    "    model_ft, criterion, optimizer_ft, \n",
    "    exp_lr_scheduler, \n",
    "    dataloaders[\"train\"], \n",
    "    dataloaders[\"val\"],\n",
    "    device, \n",
    "    timer)\n",
    "classifier.fit(20, ylim=[0, 2], checkpoint_dir=temp_dir/'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
