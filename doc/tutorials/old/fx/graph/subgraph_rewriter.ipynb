{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {func}`~torch.fx.replace_pattern` 重写子图\n",
    "\n",
    "对于仅由替换组成的简单变换，还可以使用 {mod}`~torch.fx.subgraph_rewriter`。\n",
    "\n",
    "FX 在直接 {class}`~torch.fx.Graph` 操作的基础上还提供了另一个自动化级别。{func}`~torch.fx.replace_pattern` API 本质上是编辑 {class}`~torch.fx.Graph` 的“查找/替换”工具。它允许您指定 `pattern` 和 `replacement`，它将跟踪这些函数，在 `pattern` graph 中查找运算组的实例，并用 `replacement` graph 的副本替换这些实例。随着变换变得更加复杂，这些代码可能会变得笨拙，这有助于极大地自动化繁琐的 graph 操作代码。\n",
    "\n",
    "在 {class}`~torch.fx.GraphModule` （`gm`）的 graph 中匹配所有可能不重叠的算子集及其数据依赖关系（`pattern`），然后用另一个子图替换每个匹配的子图（`replacement`）。\n",
    "\n",
    "返回值是 `Match` 对象列表，表示与 `pattern` 相匹配的原始 graph 中的位置。如果没有相匹配的，则列表为空。匹配定义为：\n",
    "\n",
    "```python\n",
    "class Match(NamedTuple):\n",
    "    # 从中找到匹配的 Node\n",
    "    anchor: Node\n",
    "    # 将 pattern subgraph 中的节点映射到较大 graph 中的节点\n",
    "    nodes_map: Dict[Node, Node]\n",
    "```\n",
    "\n",
    "```{note}\n",
    "`pattern` 中的 `return` 语句只根据它的值进行匹配；它可能与较大图中的 `return` 语句匹配，也可能不匹配。换句话说，模式不必扩展到更大的图的末尾。\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match(anchor=max_1, nodes_map={output: max_1, sum_1: sum_1, cat: cat, w1: w1, w2: w2}),\n",
       " Match(anchor=max_2, nodes_map={output: max_2, sum_1: sum_2, cat: cat_1, w1: w1, w2: w2})]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn, fx\n",
    "import torch\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, w1, w2):\n",
    "        m1 = torch.cat([w1, w2]).sum()\n",
    "        m2 = torch.cat([w1, w2]).sum()\n",
    "        return x + torch.max(m1) + torch.max(m2)\n",
    "\n",
    "def pattern(w1, w2):\n",
    "    return torch.cat([w1, w2]).sum()\n",
    "\n",
    "def replacement(w1, w2):\n",
    "    return torch.stack([w1, w2])\n",
    "\n",
    "traced_module = fx.symbolic_trace(M())\n",
    "fx.subgraph_rewriter.replace_pattern(traced_module, pattern, replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码将首先匹配 `traced_module` 的 `forward` 方法中的 `pattern`。例如，如果 `p = torch.cat([a, b])` 在 `pattern` 中，你可以在原 `forward` 函数中匹配 `m = torch.cat([a, b])`，尽管变量名不同（`p` vs `m`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x, w1, w2):\n",
      "    stack = torch.stack([w1, w2])\n",
      "    max_1 = torch.max(stack);  stack = None\n",
      "    add = x + max_1;  x = max_1 = None\n",
      "    stack_1 = torch.stack([w1, w2]);  w1 = w2 = None\n",
      "    max_2 = torch.max(stack_1);  stack_1 = None\n",
      "    add_1 = add + max_2;  add = max_2 = None\n",
      "    return add_1\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "traced_module.graph.lint()\n",
    "print(traced_module.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面介绍一些使用案例。\n",
    "\n",
    "定义验证函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_result(traced, pattern, replacement, comparison, x):\n",
    "    comparison_fn = fx.symbolic_trace(comparison)\n",
    "    fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "    traced.graph.lint()\n",
    "    ref_output = comparison_fn(x)\n",
    "    test_output = traced.forward(x)\n",
    "    torch.testing.assert_close(ref_output, test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保留底层逻辑\n",
    "\n",
    "用相同的模式替换 pattern，不应该改变底层逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x):\n",
    "        val = torch.neg(x) + torch.relu(x)\n",
    "        return torch.add(val, val)\n",
    "\n",
    "def pattern(x):\n",
    "    return torch.neg(x) + torch.relu(x)\n",
    "\n",
    "def comparison(x):\n",
    "    val = torch.neg(x) + torch.relu(x)\n",
    "    return torch.add(val, val)\n",
    "\n",
    "\n",
    "x = torch.rand(1, 3)\n",
    "traced = fx.symbolic_trace(M())\n",
    "eval_result(traced, pattern, pattern, comparison, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 替换单个节点\n",
    "\n",
    "添加单个线性结构 relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x):\n",
    "        val = torch.neg(x)\n",
    "        return torch.add(val, val)\n",
    "\n",
    "def pattern(x):\n",
    "    return torch.neg(x)\n",
    "\n",
    "def replacement(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "def comparison(x):\n",
    "    val = torch.relu(x)\n",
    "    return torch.add(val, val)\n",
    "\n",
    "\n",
    "x = torch.rand(1, 3)\n",
    "traced = fx.symbolic_trace(M())\n",
    "eval_result(traced, pattern, replacement, comparison, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移除单个节点\n",
    "\n",
    "当 `pattern` 被匹配时，它将从更大的函数中删除，并被 `replacement` 替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x):\n",
    "        val = torch.neg(x) + torch.relu(x)\n",
    "        return torch.add(val, val)\n",
    "\n",
    "def pattern(x):\n",
    "    return torch.neg(x) + torch.relu(x)\n",
    "\n",
    "def replacement(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "def comparison(x):\n",
    "    val = torch.relu(x)\n",
    "    return torch.add(val, val)\n",
    "    \n",
    "x = torch.rand(1, 3)\n",
    "traced = fx.symbolic_trace(M())\n",
    "eval_result(traced, pattern, replacement, comparison, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多模式匹配\n",
    "\n",
    "如果在较大的函数中有多个 `pattern` 匹配，则每个不重叠的匹配将被替换。在匹配重叠的情况下，将替换重叠匹配集中第一个找到的匹配。（“第一”在这里被定义为节点使用-定义关系拓扑顺序中的第一。在大多数情况下，第一个 Node 是直接出现在 `self` 之后的参数，而最后一个 Node 是函数返回的任何值。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x, w1, w2):\n",
    "        m1 = torch.cat([w1, w2]).sum()\n",
    "        m2 = torch.cat([w1, w2]).sum()\n",
    "        return x + torch.max(m1) + torch.max(m2)\n",
    "\n",
    "def pattern(w1, w2):\n",
    "    return torch.cat([w1, w2]).sum()\n",
    "\n",
    "def replacement(w1, w2):\n",
    "    return torch.stack([w1, w2])\n",
    "\n",
    "def comparison(x, w1, w2):\n",
    "    m1 = torch.stack([w1, w2])\n",
    "    m2 = torch.stack([w1, w2])\n",
    "    return x + torch.max(m1) + torch.max(m2)\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "comparison_fn = fx.symbolic_trace(comparison)\n",
    "\n",
    "x = torch.rand(1, 3)\n",
    "w1 = torch.rand(1, 3)\n",
    "w2 = torch.rand(1, 3)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "\n",
    "traced.graph.lint()\n",
    "\n",
    "ref_outs = comparison_fn(x, w1, w2)\n",
    "test_outs = traced.forward(x, w1, w2)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{note}\n",
    "需要注意的一件重要的事情是，`pattern` Callable 的参数必须在 Callable 本身中使用，而 `replacement` Callable 的参数必须与 `pattern` 匹配。\n",
    "\n",
    "第一个规则是，为什么在上面的代码块中，`forward` 函数有参数 `x`, `w1`, `w2`，而 `pattern` 函数只有参数 `w1`, `w2`。`pattern` 不使用 `x`，因此它不应该指定 `x` 作为参数。\n",
    "\n",
    "作为第二条规则的例子，考虑使用\n",
    "```python\n",
    "def replacement(x, y):\n",
    "    return torch.relu(x)\n",
    "```\n",
    "替换\n",
    "```python\n",
    "def pattern(x, y):\n",
    "    return torch.neg(x) + torch.relu(y)\n",
    "```\n",
    "在本例中，`replacement` 需要与 `pattern` 相同数量的参数（`x` 和 `y`），即使 `replacement` 中没有使用参数 `y`。\n",
    "\n",
    "````\n",
    "\n",
    "可以正确识别参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        val = torch.neg(y) + torch.relu(x)\n",
    "        return torch.add(val, val)\n",
    "\n",
    "def pattern(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "def replacement(x):\n",
    "    return torch.neg(x)\n",
    "\n",
    "def comparison(x, y):\n",
    "    val = torch.neg(y) + torch.neg(x)\n",
    "    return torch.add(val, val)\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "comparison_fn = fx.symbolic_trace(comparison)\n",
    "\n",
    "x = torch.randn(4, 4)\n",
    "y = torch.randn(4, 4)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "traced.graph.lint()\n",
    "\n",
    "ref_outs = comparison_fn(x, y)\n",
    "test_outs = traced.forward(x, y)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 追踪可回调对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x):\n",
    "        val = torch.neg(x) + torch.relu(x)\n",
    "        return torch.add(val, val)\n",
    "\n",
    "class Pattern(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.neg(x) + torch.relu(x)\n",
    "\n",
    "class Replacement(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def comparison(x):\n",
    "    val = torch.sigmoid(x)\n",
    "    return torch.add(val, val)\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "traced_pattern = fx.symbolic_trace(Pattern())\n",
    "traced_replacement = fx.symbolic_trace(Replacement())\n",
    "comparison_fn = fx.symbolic_trace(comparison)\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, traced_pattern, traced_replacement)\n",
    "\n",
    "traced.graph.lint()\n",
    "\n",
    "ref_outs = comparison_fn(x)\n",
    "test_outs = traced.forward(x)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 替换整个计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        a = torch.neg(x)\n",
    "        return torch.add(a, a)\n",
    "\n",
    "def pattern(x):\n",
    "    a = torch.neg(x)\n",
    "    return torch.add(a, a)\n",
    "\n",
    "def replacement(x):\n",
    "    a = torch.sigmoid(x)\n",
    "    return torch.cat([a, a])\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "comparison_fn = fx.symbolic_trace(replacement)\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "\n",
    "traced.graph.lint()\n",
    "\n",
    "ref_outs = comparison_fn(x)\n",
    "test_outs = traced.forward(x)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "子图重写器模式输出模式节点可以有不匹配的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x):\n",
    "        y = torch.relu(x)\n",
    "        return torch.neg(y) - y\n",
    "\n",
    "def pattern(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "def replacement(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "def comparison(x):\n",
    "    y = torch.sigmoid(x)\n",
    "    return torch.neg(y) - y\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "comparison_fn = fx.symbolic_trace(comparison)\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "\n",
    "traced.graph.lint()\n",
    "\n",
    "ref_outs = comparison_fn(x)\n",
    "test_outs = traced.forward(x)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不匹配的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x, w1, w2, b1, b2):\n",
    "        m1 = torch.cat([w1, w2])\n",
    "        m2 = torch.cat([x, b2])\n",
    "        t0 = torch.addmm(b1, m1, m2.t())\n",
    "        t1 = torch.sum(w1, 1)\n",
    "        t2 = torch.addmm(b1, m1, m2.t())\n",
    "        return torch.sum(t1), torch.sum(t2)\n",
    "\n",
    "def pattern(x, w1, w2, b1, b2):\n",
    "    m1 = torch.cat([w1, w2])\n",
    "    m2 = torch.cat([x, b2])\n",
    "    return torch.addmm(b1, m1, m2.t())\n",
    "\n",
    "def replacement(x, w1, w2, b1, b2):\n",
    "    return torch.cat([x, w1, w2])\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "\n",
    "# Result should be [] since no matches can be found\n",
    "res = fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 匹配 `placeholder`\n",
    "\n",
    "这将测试 `placeholder` 节点是否可以与具有不同数量输入节点的节点相匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dtype = torch.float16\n",
    "\n",
    "    def forward(self, x):\n",
    "        x += 3\n",
    "        x = x.dequantize()\n",
    "        x = torch.sigmoid(x)\n",
    "        dtype = self.dtype\n",
    "        x = x.to(dtype)\n",
    "        return x\n",
    "\n",
    "def pattern(x):\n",
    "    x = x.dequantize()\n",
    "    x = torch.sigmoid(x)\n",
    "    x = x.to(torch.float16)\n",
    "    return x\n",
    "\n",
    "def replacement(x):\n",
    "    return x\n",
    "\n",
    "def comparison(x):\n",
    "    return x + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始的跟踪模块是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name        target                                                      args                      kwargs\n",
      "-------------  ----------  ----------------------------------------------------------  ------------------------  --------\n",
      "placeholder    x           x                                                           ()                        {}\n",
      "call_function  add         <built-in function add>                                     (x, 3)                    {}\n",
      "call_method    dequantize  dequantize                                                  (add,)                    {}\n",
      "call_function  sigmoid     <built-in method sigmoid of type object at 0x7f25e4da7200>  (dequantize,)             {}\n",
      "call_method    to          to                                                          (sigmoid, torch.float16)  {}\n",
      "output         output      output                                                      (to,)                     {}\n"
     ]
    }
   ],
   "source": [
    "traced = fx.symbolic_trace(M())\n",
    "traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而想要匹配的模式是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name    target                   args    kwargs\n",
      "-------------  ------  -----------------------  ------  --------\n",
      "placeholder    x       x                        ()      {}\n",
      "call_function  add     <built-in function add>  (x, 3)  {}\n",
      "output         output  output                   (add,)  {}\n"
     ]
    }
   ],
   "source": [
    "comparison_fn = fx.symbolic_trace(comparison)\n",
    "comparison_fn.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "traced.graph.lint()\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "ref_outs = comparison_fn(x)\n",
    "test_outs = traced.forward(x)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 替换被引用的子模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.submod = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + 1\n",
    "        return self.submod(self.sigmoid(x))\n",
    "\n",
    "class Pattern(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.submod = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.submod(self.sigmoid(x))\n",
    "\n",
    "class Replacement(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.submod = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.submod(self.tanh(x))\n",
    "\n",
    "class Comparison(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.submod = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + 1\n",
    "        return self.submod(self.tanh(x))\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "comparison = Comparison()\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, Pattern(), Replacement())\n",
    "\n",
    "traced.graph.lint()\n",
    "\n",
    "ref_outs = comparison(x)\n",
    "test_outs = traced.forward(x)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tanh()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced.get_submodule(\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "M has no attribute `sigmoid`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m traced\u001b[39m.\u001b[39;49mget_submodule(\u001b[39m\"\u001b[39;49m\u001b[39msigmoid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/media/pc/data/4tb/lxw/libs/anaconda3/envs/tvmx/lib/python3.10/site-packages/torch/nn/modules/module.py:456\u001b[0m, in \u001b[0;36mModule.get_submodule\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m atoms:\n\u001b[1;32m    455\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(mod, item):\n\u001b[0;32m--> 456\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(mod\u001b[39m.\u001b[39m_get_name() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m has no \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39mattribute `\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m item \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    459\u001b[0m     mod \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, item)\n\u001b[1;32m    461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(mod, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[0;31mAttributeError\u001b[0m: M has no attribute `sigmoid`"
     ]
    }
   ],
   "source": [
    "traced.get_submodule(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submod = traced.get_submodule(\"submod\")\n",
    "submod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注解整数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx.annotate import annotate\n",
    "from torch.fx.experimental.rewriter import RewritingTracer\n",
    "\n",
    "class M1(nn.Module):\n",
    "    def forward(self, x):\n",
    "        y: int = x\n",
    "        return torch.add(x, y)\n",
    "\n",
    "class M2(nn.Module):\n",
    "    def forward(self, x):\n",
    "        y = annotate(x, int)\n",
    "        return torch.add(x, y)\n",
    "\n",
    "ast_rewriter = RewritingTracer()\n",
    "graph = ast_rewriter.trace(M1())\n",
    "\n",
    "module = M2()\n",
    "symbolic_traced = fx.symbolic_trace(module)\n",
    "for n, m in zip(symbolic_traced.graph.nodes, graph.nodes):\n",
    "    if n.op == 'placeholder':\n",
    "        assert n.type == int\n",
    "        assert m.type == int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重写连续的子模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = torch.sigmoid(x)\n",
    "    x = torch.sigmoid(x)\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "def pattern(x):\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "def replacement(x):\n",
    "    return torch.exp(x)\n",
    "\n",
    "def comparison(x):\n",
    "    x = torch.exp(x)\n",
    "    x = torch.exp(x)\n",
    "    return torch.exp(x)\n",
    "\n",
    "traced = fx.symbolic_trace(f)\n",
    "comparison_fn = fx.symbolic_trace(comparison)\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "\n",
    "traced.graph.lint()\n",
    "\n",
    "ref_outs = comparison_fn(x)\n",
    "test_outs = traced.forward(x)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重叠的匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = torch.sigmoid(x)\n",
    "    x = torch.sigmoid(x)\n",
    "    x = torch.sigmoid(x)\n",
    "    return torch.sigmoid(x)\n",
    "\n",
    "def pattern(x):\n",
    "    x = torch.sigmoid(x)\n",
    "    x = torch.sigmoid(x)\n",
    "    return x\n",
    "\n",
    "def replacement(x):\n",
    "    return torch.neg(x)\n",
    "\n",
    "def comparison(x):\n",
    "    x = torch.neg(x)\n",
    "    return torch.neg(x)\n",
    "\n",
    "traced = fx.symbolic_trace(f)\n",
    "comparison_fn = fx.symbolic_trace(comparison)\n",
    "\n",
    "x = torch.randn(3, 4)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "\n",
    "traced.graph.lint()\n",
    "\n",
    "ref_outs = comparison_fn(x)\n",
    "test_outs = traced.forward(x)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移除未被使用的 args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x, y]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x, y, z):\n",
    "        return x + y\n",
    "\n",
    "def pattern(x, y):\n",
    "    return x + y\n",
    "\n",
    "def replacement(x, y):\n",
    "    return x - y\n",
    "\n",
    "def comparison(x1, x2, x3):\n",
    "    return x1 - x2\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "comparison_fn = fx.symbolic_trace(comparison)\n",
    "\n",
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "\n",
    "traced.graph.lint()\n",
    "\n",
    "placeholder_nodes = [n for n in traced.graph.nodes if n.op == \"placeholder\"]\n",
    "placeholder_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(3, 4)\n",
    "x2 = torch.randn(3, 4)\n",
    "x3 = torch.randn(3, 4)\n",
    "ref_outs = comparison_fn(x1, x2, x3)\n",
    "test_outs = traced.forward(x1, x2)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重写回调方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.dequantize()\n",
    "        x = x.sigmoid()\n",
    "        x = x.to(torch.float16)\n",
    "        return x\n",
    "\n",
    "def pattern(x):\n",
    "    x = x.dequantize()\n",
    "    x = x.sigmoid()\n",
    "    x = x.to(torch.float16)\n",
    "    return x\n",
    "\n",
    "def replacement(x):\n",
    "    return x\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "comparison_fn = fx.symbolic_trace(replacement)\n",
    "fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "traced.graph.lint()\n",
    "\n",
    "x1 = torch.randn(3, 4)\n",
    "ref_outs = comparison_fn(x1)\n",
    "test_outs = traced.forward(x1)\n",
    "torch.testing.assert_close(ref_outs, test_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过 kwargs 重写子图\n",
    "\n",
    "需要定义模块级方法：\n",
    "\n",
    "```python\n",
    "# custom_rewriter.py\n",
    "from torch import fx, nn\n",
    "\n",
    "@fx.wrap\n",
    "def wrapped_gemm_bias_mul(a, b, bias):\n",
    "    lin_res = nn.functional.linear(a, b, bias=bias)\n",
    "    mul_res = lin_res * a\n",
    "    return lin_res, mul_res\n",
    "\n",
    "@fx.wrap\n",
    "def wrapped_gemm_bias_mul_with_c(a, b, bias, c):\n",
    "    lin_res = nn.functional.linear(a, b, bias=bias)\n",
    "    mul_res = lin_res * c\n",
    "    return lin_res, mul_res\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_rewriter import wrapped_gemm_bias_mul, wrapped_gemm_bias_mul_with_c\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.w0 = nn.Parameter(torch.empty([128, 128]))\n",
    "        self.b0 = nn.Parameter(torch.empty([128]))\n",
    "\n",
    "    def forward(self, in0):\n",
    "        lin_res = nn.functional.linear(in0, self.w0, bias=self.b0)\n",
    "        mul_res = in0 * lin_res\n",
    "        sum_res = mul_res + in0\n",
    "        return sum_res\n",
    "\n",
    "def pattern(a, b, bias):\n",
    "    lin_res = nn.functional.linear(a, b, bias=bias)\n",
    "    mul_res = a * lin_res\n",
    "    return (lin_res, mul_res)\n",
    "\n",
    "def replacement(a, b, bias):\n",
    "    lin_res, mul_res = wrapped_gemm_bias_mul(a, b, bias)\n",
    "    return (lin_res, mul_res)\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "matches = fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_repalcement_node = False\n",
    "for node in traced.graph.nodes:\n",
    "    if node.target == wrapped_gemm_bias_mul:\n",
    "        found_repalcement_node = True\n",
    "        break\n",
    "\n",
    "found_repalcement_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重写 loca revert\n",
    "\n",
    "下面的模型将有 3 个锚（anchor）作为匹配候选者，锚 1 和锚 3 是真正的匹配，但锚 2 不是。子图重写器应该能够恢复在匹配锚点 2 时所做的更改。与三号锚的最后的匹配应该会成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following model will have 3 anchors as the matching candidate with the given pattern\n",
    "# Anchor 1 and 3 is a real match, but anchor 2 is not.\n",
    "# The subgraph rewriter should be able to revert the changes made while matching anchor 2.\n",
    "# Final match with anchor 3 should be successful.\n",
    "class M(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.w0 = nn.Parameter(torch.empty([128, 128]))\n",
    "        self.b0 = nn.Parameter(torch.empty([128]))\n",
    "        self.w1 = nn.Parameter(torch.empty([128, 128]))\n",
    "        self.b1 = nn.Parameter(torch.empty([128]))\n",
    "        self.w2 = nn.Parameter(torch.empty([128, 128]))\n",
    "        self.b2 = nn.Parameter(torch.empty([128]))\n",
    "        self.w3 = nn.Parameter(torch.empty([128, 128]))\n",
    "        self.b3 = nn.Parameter(torch.empty([128]))\n",
    "        self.w4 = nn.Parameter(torch.empty([128, 128]))\n",
    "        self.b4 = nn.Parameter(torch.empty([128]))\n",
    "\n",
    "    def forward(self, in0, in1):\n",
    "        lin_res_1 = nn.functional.linear(in1, self.w0, bias=self.b0)\n",
    "        lin_res_2 = nn.functional.linear(lin_res_1, self.w1, bias=self.b1)\n",
    "        # potential match at anchor 1\n",
    "        mul_res_1 = in1 * lin_res_2\n",
    "        sum_res_1 = mul_res_1 + in1\n",
    "        lin_res_3 = nn.functional.linear(\n",
    "            sum_res_1, self.w2, bias=self.b2\n",
    "        )\n",
    "        sigmoid_res_1 = torch.sigmoid(lin_res_3)\n",
    "        # potential match at anchor 2\n",
    "        mul_res_2 = lin_res_3 * sigmoid_res_1\n",
    "        lin_res_4 = nn.functional.linear(in0, self.w3, bias=self.b3)\n",
    "        lin_res_5 = nn.functional.linear(lin_res_4, self.w4, bias=self.b4)\n",
    "        # potential match at anchor 3\n",
    "        mul_res_3 = in0 * lin_res_5\n",
    "        sum_res_2 = mul_res_3 + in0\n",
    "        cat_res = torch.cat(\n",
    "            [mul_res_2, sum_res_2],\n",
    "            dim=1,\n",
    "        )\n",
    "        return cat_res\n",
    "\n",
    "def gemm_bias_mul_pattern_with_c(a, b, bias, c):\n",
    "    lin_res = nn.functional.linear(a, b, bias=bias)\n",
    "    mul_res = c * lin_res\n",
    "    return lin_res, mul_res\n",
    "\n",
    "def gemm_bias_mul_replacement_with_c(a, b, bias, c):\n",
    "    lin_res, mul_res = wrapped_gemm_bias_mul_with_c(a, b, bias, c)\n",
    "    return lin_res, mul_res\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "matches = fx.subgraph_rewriter.replace_pattern(\n",
    "    traced,\n",
    "    gemm_bias_mul_pattern_with_c,\n",
    "    gemm_bias_mul_replacement_with_c)\n",
    "\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repalcement_node_found = 0\n",
    "for node in traced.graph.nodes:\n",
    "    if node.target == wrapped_gemm_bias_mul_with_c:\n",
    "        repalcement_node_found += 1\n",
    "\n",
    "repalcement_node_found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过过滤器重写子图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, scale, zero_point):\n",
    "        # Match, second input to add is a scalar\n",
    "        x = x.dequantize()\n",
    "        x = torch.add(x, 2)\n",
    "        x = x.relu()\n",
    "        x = torch.quantize_per_tensor(x, scale, zero_point, torch.quint8)\n",
    "\n",
    "        y = x + 1\n",
    "        # NOT a match, second input to add is NOT a scalar\n",
    "        x = x.dequantize()\n",
    "        x = torch.add(x, y)\n",
    "        x = x.relu()\n",
    "        x = torch.quantize_per_tensor(x, scale, zero_point, torch.quint8)\n",
    "\n",
    "        return x\n",
    "\n",
    "def BinaryOpScalarReLUPattern(x, num, scale, zero_point):\n",
    "    x = x.dequantize()\n",
    "    x = torch.add(x, num)\n",
    "    x = x.relu()\n",
    "    x = torch.quantize_per_tensor(x, scale, zero_point, torch.quint8)\n",
    "    return x\n",
    "\n",
    "def BinaryOpScalarReLUReplacement(x, num, scale, zero_point):\n",
    "    x = torch.mul(x, num)\n",
    "    return x\n",
    "\n",
    "def second_input_is_scalar(match, original_graph, pattern_graph):\n",
    "    \"\"\" check the node that's matched to the second input of the pattern graph\n",
    "    is a scalar number\n",
    "    \"\"\"\n",
    "    input_idx = 0\n",
    "    for node in pattern_graph.nodes:\n",
    "        if node.op == \"placeholder\":\n",
    "            if input_idx == 1:\n",
    "                num_node = node\n",
    "            input_idx += 1\n",
    "    if not isinstance(match.nodes_map[num_node], (int, float)):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def num_repalcement_node_found(traced):\n",
    "    return sum(1 for node in traced.graph.nodes if node.target == torch.mul)\n",
    "\n",
    "# match without filter, should find 2 match\n",
    "traced = fx.symbolic_trace(M())\n",
    "matches = fx.subgraph_rewriter.replace_pattern(\n",
    "    traced,\n",
    "    BinaryOpScalarReLUPattern,\n",
    "    BinaryOpScalarReLUReplacement)\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_repalcement_node_found(traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.fx.subgraph_rewriter' has no attribute 'replace_pattern_with_filters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb Cell 58\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# match with filter, should find 1 match\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m traced \u001b[39m=\u001b[39m fx\u001b[39m.\u001b[39msymbolic_trace(M())\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m matches \u001b[39m=\u001b[39m fx\u001b[39m.\u001b[39;49msubgraph_rewriter\u001b[39m.\u001b[39;49mreplace_pattern_with_filters(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     traced,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     BinaryOpScalarReLUPattern,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     BinaryOpScalarReLUReplacement,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.16.11.3/media/pc/data/4tb/lxw/home/lxw/hub/torch-book/doc/tutorial/fx/graph/subgraph_rewriter.ipynb#Y123sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     [second_input_is_scalar])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.fx.subgraph_rewriter' has no attribute 'replace_pattern_with_filters'"
     ]
    }
   ],
   "source": [
    "# match with filter, should find 1 match\n",
    "traced = fx.symbolic_trace(M())\n",
    "matches = fx.subgraph_rewriter.replace_pattern_with_filters(\n",
    "    traced,\n",
    "    BinaryOpScalarReLUPattern,\n",
    "    BinaryOpScalarReLUReplacement,\n",
    "    [second_input_is_scalar])\n",
    "\n",
    "len(matches), num_repalcement_node_found(traced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_matching_pattern_with_list_type_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class M(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.ops.aten._reshape_alias_copy.default(x, [1, 2], [3, 4])\n",
    "\n",
    "def pattern(x, arg0, arg1):\n",
    "    return torch.ops.aten._reshape_alias_copy.default(x, arg0, arg1)\n",
    "\n",
    "def replacement(x, arg0, arg1):\n",
    "    return torch.ops.aten._reshape_alias_copy.default(x, arg1, arg0)\n",
    "\n",
    "traced = fx.symbolic_trace(M())\n",
    "matches = fx.subgraph_rewriter.replace_pattern(traced, pattern, replacement)\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self, x):\n",
      "    _reshape_alias_copy_default = torch.ops.aten._reshape_alias_copy.default(x, [1, 2], [3, 4]);  x = None\n",
      "    return _reshape_alias_copy_default\n"
     ]
    }
   ],
   "source": [
    "print(traced.code.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
