{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FX 流程控制\n",
    "\n",
    "先定义带有流程控制的抽象基类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractclassmethod\n",
    "from typing import Any, NamedTuple\n",
    "import torch\n",
    "from torch import Tensor, nn, fx\n",
    "\n",
    "\n",
    "class MyModuleBase(nn.Module, ABC):\n",
    "    def forward(self, x):\n",
    "        matrx = self.get_mul_matrix()\n",
    "        if self.no_relu():\n",
    "            return torch.mm(x, matrx)\n",
    "        else:\n",
    "            return torch.relu(torch.mm(x, matrx))\n",
    "\n",
    "    def get_mul_matrix(self):\n",
    "        return self.param\n",
    "    \n",
    "    @abstractclassmethod\n",
    "    def no_relu(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义简单的条件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModuleParamShape(MyModuleBase):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.param = nn.Parameter(torch.randn(in_channels, 3))\n",
    "\n",
    "    def no_relu(self):\n",
    "        return self.param.shape[0] < 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同条件的实例化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_only_mod = MyModuleParamShape(in_channels=5)\n",
    "relu_mod = MyModuleParamShape(in_channels=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证一个模块只执行 `mm` 运算，而另一个模块在级联（cascade）中执行 `mm` 和 `relu` 运算。\n",
    "\n",
    "验证仅仅执行 `mm_only_mod` 运算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 5)\n",
    "torch.testing.assert_close(mm_only_mod(x), \n",
    "                           torch.mm(x, mm_only_mod.get_mul_matrix()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证计算图模块计算结果是相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = fx.Tracer(param_shapes_constant=True)\n",
    "traced_graph = tracer.trace(mm_only_mod)\n",
    "graph_mod_mm = fx.GraphModule(mm_only_mod, traced_graph)\n",
    "torch.testing.assert_close(graph_mod_mm(x), \n",
    "                           torch.mm(x, mm_only_mod.get_mul_matrix()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建具有不同参数形状的新模块，以沿着不同的代码路径前进："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 15)\n",
    "torch.testing.assert_close(relu_mod(x), \n",
    "                           torch.relu(torch.mm(x, relu_mod.get_mul_matrix())))\n",
    "\n",
    "tracer2 = fx.Tracer(param_shapes_constant=True)\n",
    "traced_graph2 = tracer2.trace(relu_mod)\n",
    "\n",
    "# 验证计算图模块计算结果是相同\n",
    "graph_mod_relu = fx.GraphModule(relu_mod, traced_graph2)\n",
    "torch.testing.assert_close(graph_mod_relu(x), \n",
    "                            torch.relu(torch.mm(x, relu_mod.get_mul_matrix())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个 graph 有额外的 `relu` 函数调用节点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1_node_targets = [n.target for n in traced_graph.nodes]\n",
    "graph2_node_targets = [n.target for n in traced_graph2.nodes]\n",
    "assert torch.mm in graph1_node_targets and torch.mm in graph2_node_targets\n",
    "assert torch.relu not in graph1_node_targets and torch.relu in graph2_node_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述验证过程放入函数中以重用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_mm_relu_mods(mm_only_mod, relu_mod):\n",
    "    \"\"\"\n",
    "    验证一个模块只执行 `mm` 运算，\n",
    "    而另一个模块在级联（cascade）中执行 `mm` 和 `relu` 运算。\n",
    "    \"\"\"\n",
    "    x = torch.randn(10, 5)\n",
    "    torch.testing.assert_close(mm_only_mod(x), \n",
    "                               torch.mm(x, mm_only_mod.get_mul_matrix()))\n",
    "    tracer = fx.Tracer(param_shapes_constant=True)\n",
    "    traced_graph = tracer.trace(mm_only_mod)\n",
    "\n",
    "    # 验证计算图模块计算结果是相同\n",
    "    graph_mod_mm = fx.GraphModule(mm_only_mod, traced_graph)\n",
    "    torch.testing.assert_close(graph_mod_mm(x), \n",
    "                               torch.mm(x, mm_only_mod.get_mul_matrix()))\n",
    "\n",
    "\n",
    "    # 创建具有不同参数形状的新模块，以沿着不同的代码路径前进\n",
    "    x = torch.randn(10, 15)\n",
    "    torch.testing.assert_close(relu_mod(x), \n",
    "                               torch.relu(torch.mm(x, relu_mod.get_mul_matrix())))\n",
    "\n",
    "    tracer2 = fx.Tracer(param_shapes_constant=True)\n",
    "    traced_graph2 = tracer2.trace(relu_mod)\n",
    "\n",
    "    # 验证计算图模块计算结果是相同\n",
    "    graph_mod_relu = fx.GraphModule(relu_mod, traced_graph2)\n",
    "    torch.testing.assert_close(graph_mod_relu(x), \n",
    "                               torch.relu(torch.mm(x, relu_mod.get_mul_matrix())))\n",
    "\n",
    "\n",
    "    graph1_node_targets = [n.target for n in traced_graph.nodes]\n",
    "    graph2_node_targets = [n.target for n in traced_graph2.nodes]\n",
    "    # 第二个 graph 有额外的 `relu` 函数调用节点\n",
    "    assert torch.mm in graph1_node_targets and torch.mm in graph2_node_targets\n",
    "    assert torch.relu not in graph1_node_targets and torch.relu in graph2_node_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModuleParamSize(MyModuleBase):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.param = nn.Parameter(torch.randn(in_channels, 3))\n",
    "\n",
    "    def no_relu(self):\n",
    "        return self.param.size()[0] < 10\n",
    "\n",
    "class MyModuleParamDim(MyModuleBase):\n",
    "    def __init__(self, param):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def get_mul_matrix(self):\n",
    "        return self.param[0] if (self.param.dim() == 3) else self.param\n",
    "\n",
    "    def no_relu(self):\n",
    "        return self.param.dim() == 3\n",
    "\n",
    "class MyModuleParamNDim(MyModuleBase):\n",
    "    def __init__(self, param):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def get_mul_matrix(self):\n",
    "        return self.param[0] if (self.param.ndim == 3) else self.param\n",
    "\n",
    "    def no_relu(self):\n",
    "        return self.param.ndim == 3\n",
    "\n",
    "class MyModuleParamNumEl(MyModuleBase):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.randn(in_channels, 3))\n",
    "\n",
    "    def no_relu(self):\n",
    "        return self.param.numel() < 10 * 3\n",
    "\n",
    "class MyModuleParamNElement(MyModuleBase):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.randn(in_channels, 3))\n",
    "\n",
    "    def no_relu(self):\n",
    "        return self.param.nelement() < 10 * 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_param_size_const(self):\n",
    "    mymod = MyModuleParamSize(in_channels=5)\n",
    "    mymod2 = MyModuleParamSize(in_channels=15)\n",
    "    self.verify_mm_relu_mods(mymod, mymod2)\n",
    "\n",
    "def test_param_dim_const(self):\n",
    "    mymod = MyModuleParamDim(torch.nn.Parameter(torch.randn(2, 5, 3)))\n",
    "    mymod2 = MyModuleParamDim(torch.nn.Parameter(torch.randn(15, 3)))\n",
    "    self.verify_mm_relu_mods(mymod, mymod2)\n",
    "\n",
    "def test_param_ndim_const(self):\n",
    "    mymod = MyModuleParamNDim(torch.nn.Parameter(torch.randn(2, 5, 3)))\n",
    "    mymod2 = MyModuleParamNDim(torch.nn.Parameter(torch.randn(15, 3)))\n",
    "    self.verify_mm_relu_mods(mymod, mymod2)\n",
    "\n",
    "def test_param_numel_const(self):\n",
    "    mymod = MyModuleParamNumEl(in_channels=5)\n",
    "    mymod2 = MyModuleParamNumEl(in_channels=15)\n",
    "    self.verify_mm_relu_mods(mymod, mymod2)\n",
    "\n",
    "def test_param_nelement_const(self):\n",
    "    mymod = MyModuleParamNElement(in_channels=5)\n",
    "    mymod2 = MyModuleParamNElement(in_channels=15)\n",
    "    self.verify_mm_relu_mods(mymod, mymod2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
