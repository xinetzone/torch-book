{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原语库\n",
    "\n",
    "在这个例子中，将定义“复合”(composite)运算库。复合运算是定义为可调用函数的运算，这些函数在其实现中由多个其他运算组成。\n",
    "\n",
    "复合运算允许您选择在什么抽象级别上解释/运算代码。我们演示了可以提供一个函数来内联这些函数，也可以使用自定义 {class}`~torch.fx.Tracer` 来自动内联这些函数。\n",
    "\n",
    "组合运算对于向后端/变换公开更高级别的上下文，同时仍然保持在更细粒度级别检查内容的能力很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "def sigmoid_lowp(x: torch.Tensor):\n",
    "    x = x.float()\n",
    "    x = x.sigmoid()\n",
    "    return x.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{func}`wrap` 表示传入的函数应该始终被记录为 `call_function` 节点，而不是被跟踪。稍后，我们将看到如何做到:\n",
    "\n",
    "a. 内联这样一个函数的实现;\n",
    "b. 定义一个跟踪器，自动跟踪这样一个函数\n",
    "\n",
    "```python\n",
    "# primitive_library.py\n",
    "fx.wrap(sigmoid_lowp)\n",
    "```\n",
    "\n",
    "同样:\n",
    "\n",
    "```python\n",
    "# primitive_library.py\n",
    "def add_lowp(a: torch.Tensor, b: torch.Tensor):\n",
    "    a, b = a.float(), b.float()\n",
    "    c = a + b\n",
    "    return c.half()\n",
    "\n",
    "torch.fx.wrap(add_lowp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看看在使用这些函数的代码中进行符号跟踪时会发生什么"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primitive_library import sigmoid_lowp, add_lowp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x, y):\n",
      "    float_1 = x.float();  x = None\n",
      "    sigmoid = float_1.sigmoid();  float_1 = None\n",
      "    half = sigmoid.half();  sigmoid = None\n",
      "    float_2 = y.float();  y = None\n",
      "    sigmoid_1 = float_2.sigmoid();  float_2 = None\n",
      "    half_1 = sigmoid_1.half();  sigmoid_1 = None\n",
      "    float_3 = half.float();  half = None\n",
      "    float_4 = half_1.float();  half_1 = None\n",
      "    add = float_3 + float_4;  float_3 = float_4 = None\n",
      "    half_2 = add.half();  add = None\n",
      "    return half_2\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "class Foo(torch.nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        x = sigmoid_lowp(x)\n",
    "        y = sigmoid_lowp(y)\n",
    "        return add_lowp(x, y)\n",
    "\n",
    "\n",
    "traced = fx.symbolic_trace(Foo())\n",
    "print(traced.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意 `sigmoid_lowp` 和 `add_lowp` 的调用出现在跟踪中;他们自身没有被追踪.\n",
    "\n",
    "## 内联回调\n",
    "\n",
    "定义一个函数，允许在 graph 运算期间内联这些调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inline_lowp_func(n : fx.Node):\n",
    "    # If we find a call to a function in our \"lowp\" module, inline it\n",
    "    if n.op == 'call_function' and n.target.__module__ == inline_lowp_func.__module__:\n",
    "        # We want to insert the operations comprising the implementation of the\n",
    "        # function before the function itself. Then, we can swap the output value\n",
    "        # of the function call with the output value for its implementation nodes\n",
    "        tracer = fx.proxy.GraphAppendingTracer(n.graph)\n",
    "        with n.graph.inserting_before(n):\n",
    "            # We can inline code by using `fx.Proxy` instances.\n",
    "            # map_arg traverses all aggregate types and applies the given function\n",
    "            # to Node instances in the data structure. In this case, we are applying\n",
    "            # the fx.Proxy constructor.\n",
    "            proxy_args = torch.fx.node.map_arg(n.args, lambda x: torch.fx.Proxy(x, tracer))\n",
    "            proxy_kwargs = torch.fx.node.map_arg(n.kwargs, lambda x: torch.fx.Proxy(x, tracer))\n",
    "            # Call the function itself with proxy arguments. This will emit\n",
    "            # nodes in the graph corresponding to the operations in the im-\n",
    "            # plementation of the function\n",
    "            output_proxy = n.target(*proxy_args, **proxy_kwargs)\n",
    "            # Now replace the original node's uses with the output node of\n",
    "            # the implementation.\n",
    "            node.replace_all_uses_with(output_proxy.node)\n",
    "            # Delete the old node\n",
    "            node.graph.erase_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x, y):\n",
      "    float_1 = x.float();  x = None\n",
      "    sigmoid = float_1.sigmoid();  float_1 = None\n",
      "    half = sigmoid.half();  sigmoid = None\n",
      "    float_2 = y.float();  y = None\n",
      "    sigmoid_1 = float_2.sigmoid();  float_2 = None\n",
      "    half_1 = sigmoid_1.half();  sigmoid_1 = None\n",
      "    float_3 = half.float();  half = None\n",
      "    float_4 = half_1.float();  half_1 = None\n",
      "    add = float_3 + float_4;  float_3 = float_4 = None\n",
      "    half_2 = add.half();  add = None\n",
      "    return half_2\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for node in traced.graph.nodes:\n",
    "    if node.op == 'call_function' and node.target is sigmoid_lowp:\n",
    "        inline_lowp_func(node)\n",
    "\n",
    "# 不要忘记在 Graph 运算之后重新编译\n",
    "new_code = traced.recompile()\n",
    "print(traced.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，`sigmoid_lowp` 的实现已被替换为所有对该函数的调用。\n",
    "\n",
    "## 跟踪期间的内联调用\n",
    "\n",
    "现在将定义自定义跟踪器，它可以有选择地动态内联对某些组合运算的调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x, y):\n",
      "    float_1 = x.float();  x = None\n",
      "    sigmoid = float_1.sigmoid();  float_1 = None\n",
      "    half = sigmoid.half();  sigmoid = None\n",
      "    float_2 = y.float();  y = None\n",
      "    sigmoid_1 = float_2.sigmoid();  float_2 = None\n",
      "    half_1 = sigmoid_1.half();  sigmoid_1 = None\n",
      "    float_3 = half.float();  half = None\n",
      "    float_4 = half_1.float();  half_1 = None\n",
      "    add = float_3 + float_4;  float_3 = float_4 = None\n",
      "    half_2 = add.half();  add = None\n",
      "    return half_2\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "f = Foo()\n",
    "\n",
    "class InliningTracer(fx.Tracer):\n",
    "    FNS_TO_INLINE = [add_lowp]\n",
    "\n",
    "    def create_node(self, kind, target, args, kwargs, name=None, type_expr=None):\n",
    "        if kind == 'call_function' and target in self.FNS_TO_INLINE:\n",
    "            tracer = fx.proxy.GraphAppendingTracer(self.graph)\n",
    "            # Trace through the implementation of the function rather than\n",
    "            # create a node\n",
    "            proxy_args = fx.node.map_arg(args, lambda x: torch.fx.Proxy(x, tracer))\n",
    "            proxy_kwargs = fx.node.map_arg(kwargs, lambda x: torch.fx.Proxy(x, tracer))\n",
    "            return target(*proxy_args, **proxy_kwargs).node\n",
    "        else:\n",
    "            return super().create_node(kind, target, args, kwargs, name, type_expr)\n",
    "\n",
    "\n",
    "tracer = InliningTracer()\n",
    "graph = tracer.trace(f)\n",
    "module = torch.fx.GraphModule(f, graph)\n",
    "print(module.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如你所看到的，`add_lowp` 的实现已经在使用我们的 `InliningTracer` 进行跟踪的过程中内联了。例如，这样的功能可以用于实现后端，该后端希望看到某些运算的低级形式，但希望看到另一些运算的高级形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
