{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch编译器示例教程\n",
    "\n",
    "本教程旨在涵盖 PyTorch 编译器的以下几个方面：\n",
    "\n",
    "- 基本概念（即时(Just-In-Time)编译器，提前(Ahead-of-time)编译器）\n",
    "- Dynamo（图捕获，将用户的代码分为纯 Python 代码和纯 PyTorch 相关代码）\n",
    "- AOTAutograd（从正向计算图中生成反向计算图）\n",
    "- Inductor/其他后端（给定计算图，如何在不同的设备上更快地运行它）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些组件将根据不同的后端选项被调用：\n",
    "\n",
    "- 当只使用 Dynamo 时，使用 `torch.compile(backend=\"eager\")`。\n",
    "- 当使用 Dynamo 和 AOTAutograd 时，使用 `torch.compile(backend=\"aot_eager\")`。\n",
    "- 默认情况下，使用 `torch.compile(backend=\"inductor\")`，这意味着同时使用 Dynamo、AOTAutograd 以及 PyTorch 内置的图优化后端 Inductor。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 编译器是即时编译器\n",
    "\n",
    "首先需要了解的概念是，PyTorch 编译器是一种即时编译器(Just-In-Time)。那么，即时编译器是什么意思呢？来看例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class A(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.exp(2 * x)\n",
    "\n",
    "class B(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-x)\n",
    "\n",
    "def f(x, mod):\n",
    "    y = mod(x)\n",
    "    z = torch.log(y)\n",
    "    return z\n",
    "\n",
    "# users might use\n",
    "# mod = A()\n",
    "# x = torch.randn(5, 5, 5)\n",
    "# output = f(x, mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写了函数 `f`，它包含模块调用，该调用将执行 `mod.forward`，以及 `torch.log` 调用。由于众所周知的代数简化恒等式 $\\log(\\exp(a\\times x))=a\\times x$，迫不及待地想要优化代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, mod):\n",
    "    if isinstance(mod, A):\n",
    "        return 2 * x\n",
    "    elif isinstance(mod, B):\n",
    "        return -x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以将其称为我们的第一个编译器，尽管它是由我们的大脑而不是自动化程序编译的。\n",
    "\n",
    "如果希望更加严谨，那么编译器示例应该更新如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, mod):\n",
    "    if isinstance(x, torch.Tensor) and isinstance(mod, A):\n",
    "        return 2 * x\n",
    "    elif isinstance(x, torch.Tensor) and isinstance(mod, B):\n",
    "        return -x\n",
    "    else:\n",
    "        y = mod(x)\n",
    "        z = torch.log(y)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要检查每个参数，以确保优化条件是合理的，如果未能优化代码，还需要回退到原始代码。\n",
    "\n",
    "这引出了即时编译器中的两个基本概念：守卫和转换代码。**守卫**是函数可以被优化的条件，而 **转换代码** 则是在满足守卫条件下的函数优化版本。在上面简单的编译器示例中，`isinstance(mod, A)` 就是守卫，而 `return 2 * x` 则是相应的转换代码，它在守卫条件下与原始代码等效，但执行速度要快得多。\n",
    "\n",
    "上述例子是提前编译的编译器：检查所有可用的源代码，并在运行任何函数（即提前）之前，根据所有可能的守卫和转换代码编写优化后的函数。\n",
    "\n",
    "另一类编译器是即时编译器：就在函数执行之前，它会分析是否可以对执行进行优化，以及在什么条件下可以对函数执行进行优化。希望这个条件足够通用，以适应新的输入，从而使即时编译的好处大于成本。如果所有条件都失败，它将尝试在新的条件下优化代码。\n",
    "\n",
    "即时编译器的基本工作流程应该如下所示：\n",
    "\n",
    "```python\n",
    "def f(x, mod):\n",
    "    for guard, transformed_code in f.compiled_entries:\n",
    "        if guard(x, mod):\n",
    "            return transformed_code(x, mod)\n",
    "    try:\n",
    "        guard, transformed_code = compile_and_optimize(x, mod)\n",
    "        f.compiled_entries.append([guard, transformed_code])\n",
    "        return transformed_code(x, mod)\n",
    "    except FailToCompileError:\n",
    "        y = mod(x)\n",
    "        z = torch.log(y)\n",
    "        return z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即时编译器（Just-In-Time Compiler）仅针对其已经观察到的情况进行优化。每当它遇到新的输入，而这个输入不满足任何现有的保护条件时，它就会为这个新输入编译出新的保护条件和转换后的代码。\n",
    "\n",
    "逐步解释编译器的状态（就保护条件和转换后的代码而言）：\n",
    "```python\n",
    "import torch\n",
    "\n",
    "class A(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(2 * x)\n",
    "\n",
    "class B(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp(-x)\n",
    "\n",
    "@just_in_time_compile # an imaginary compiler function\n",
    "def f(x, mod):\n",
    "    y = mod(x)\n",
    "    z = torch.log(y)\n",
    "    return z\n",
    "\n",
    "a = A()\n",
    "b = B()\n",
    "x = torch.randn((5, 5, 5))\n",
    "\n",
    "# before executing f(x, a), f.compiled_entries == [] is empty.\n",
    "f(x, a)\n",
    "# after executing f(x, a), f.compiled_entries == [Guard(\"isinstance(x, torch.Tensor) and isinstance(mod, A)\"), TransformedCode(\"return 2 * x\")]\n",
    "\n",
    "# the second call of f(x, a) hit a condition, so we can just execute the transformed code\n",
    "f(x, a)\n",
    "\n",
    "# f(x, b) will trigger compilation and add a new compiled entry\n",
    "# before executing f(x, b), f.compiled_entries == [Guard(\"isinstance(x, torch.Tensor) and isinstance(mod, A)\"), TransformedCode(\"return 2 * x\")]\n",
    "f(x, b)\n",
    "# after executing f(x, b), f.compiled_entries == [Guard(\"isinstance(x, torch.Tensor) and isinstance(mod, A)\"), TransformedCode(\"return 2 * x\"), Guard(\"isinstance(x, torch.Tensor) and isinstance(mod, B)\"), TransformedCode(\"return -x\")]\n",
    "\n",
    "# the second call of f(x, b) hit a condition, so we can just execute the transformed code\n",
    "f(x, b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个示例中，我们对类类型进行防护检查，例如使用 `isinstance(mod, A)` 语句，而且转换后的代码仍然是 Python 代码；对于 torch.compile 来说，它需要对更多的条件进行防护，比如设备（CPU/GPU）、数据类型（int32, float32）、形状（[10], [8]），而它的转换代码则是 Python 字节码。我们可以从函数中提取这些编译条目，更多细节请参阅 [PyTorch 文档](https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html)。尽管在防护和转换代码方面有所不同，但 `torch.compile` 的基本工作流程与本例相同，即它充当即时编译器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超越代数简化的优化\n",
    "上述例子是关于代数简化的。然而，这样的优化在实践中相当罕见。让我们来看更实际的例子，并了解 PyTorch 编译器是如何对以下代码进行优化的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/lxw/envs/anaconda3x/envs/xxx/lib/python3.12/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/media/pc/data/lxw/envs/anaconda3x/envs/xxx/lib/python3.12/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "@torch.compile\n",
    "def function(inputs):\n",
    "    x = inputs[\"x\"]\n",
    "    y = inputs[\"y\"]\n",
    "    x = x.cos().cos()\n",
    "    if x.mean() > 0.5:\n",
    "        x = x / 1.1\n",
    "    return x * y\n",
    "\n",
    "shape_10_inputs = {\"x\": torch.randn(10, requires_grad=True), \"y\": torch.randn(10, requires_grad=True)}\n",
    "shape_8_inputs = {\"x\": torch.randn(8, requires_grad=True), \"y\": torch.randn(8, requires_grad=True)}\n",
    "# warmup\n",
    "for i in range(100):\n",
    "    output = function(shape_10_inputs)\n",
    "    output = function(shape_8_inputs)\n",
    "\n",
    "# execution of compiled functions\n",
    "output = function(shape_10_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码尝试实现 $\\text{cos}(\\text{cos}(x))$ 激活函数，并根据其激活值调整输出的大小，然后将输出与另一个张量 `y` 相乘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamo是如何转换和修改函数功能的？\n",
    "\n",
    "当理解 {func}`torch.compile` 作为即时编译器的整体图景后，可以更深入地探究其工作原理。与 `gcc` 或 `llvm` 这样的通用编译器不同，{func}`torch.compile` 是特定领域的编译器：它只专注于 PyTorch 相关的计算图。因此，需要工具来将用户的代码分为两部分：纯 Python 代码和计算图代码。\n",
    "\n",
    "Dynamo 就位于 {mod}`torch._dynamo` 模块内，是完成此任务的工具。通常不直接与这个模块交互。它是在 {func}`torch.compile` 函数内部被调用的。\n",
    "\n",
    "从概念上讲，Dynamo 执行以下操作：\n",
    "\n",
    "- 找到第一个无法在计算图中表示但需要计算图中计算值的算子（例如，打印张量的值，使用张量的值来决定 Python 中的 `if` 语句控制流）。\n",
    "- 将前面的算子分成两部分：一个是纯粹关于张量计算的计算图，另一个是一些关于操纵 Python 对象的 Python 代码。\n",
    "- 将剩余的算子保留为一两个新函数（称为 `resume` 函数），并再次触发上述分析。\n",
    "\n",
    "为了能够对函数进行这种细粒度的操作，Dynamo 在低于 Python 源代码级别的 Python 字节码层面运作。\n",
    "\n",
    "以下过程描述了 Dynamo 对函数所做的处理。![](https://depyf.readthedocs.io/en/latest/_images/dynamo-workflow.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dynamo` 的显著特性是它能够分析函数内部调用的所有函数。如果函数可以完全用计算图表示，那么这个函数的调用将被内联，从而消除该函数调用。\n",
    "\n",
    "Dynamo 的使命是以安全稳妥的方式从 Python 代码中提取计算图。一旦获得了计算图，就可以进入计算图优化的世界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "上述工作流程包含许多难以理解的字节码。对于那些无法阅读 Python 字节码的人来说，`depyf` 可以提供帮助！\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态形状支持来自 `Dynamo`\n",
    "\n",
    "深度学习编译器通常倾向于静态形状输入。这就是为什么上述保护条件包括形状保护的原因。第一次函数调用使用形状 `[10]` 的输入，但第二次函数调用使用的是形状 `[8]` 的输入。这将无法通过形状保护，因此触发新的代码转换。\n",
    "\n",
    "默认情况下，`Dynamo` 支持动态形状。当形状保护失败时，它会分析和比较形状，并尝试将形状泛化。在这种情况下，看到形状为 `[8]` 的输入后，它将尝试泛化为任意一维形状 `[s0]`，这被称为动态形状或符号形状。\n",
    "\n",
    "## `AOTAutograd`：从前向图生成反向计算图\n",
    "\n",
    "上述代码仅处理前向计算图。重要的缺失部分是如何获取反向计算图来计算梯度。\n",
    "\n",
    "在纯 PyTorch 代码中，反向计算是通过对某个标量损失值调用 `backward` 函数来触发的。每个 PyTorch 函数在前向计算期间存储了反向所需的信息。\n",
    "\n",
    "为了解释急切模式下反向期间发生了什么，有下面的实现，它模拟了 {func}`torch.cos` 函数的内置行为（需要一些关于如何在 PyTorch 中编写带有自动梯度支持的自定义函数的[背景知识](https://pytorch.org/docs/main/notes/extending.html#extending-torch-autograd)）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Cosine(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x0):\n",
    "        x1 = torch.cos(x0)\n",
    "        return x1, x0\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x1, x0 = output\n",
    "        print(f\"saving tensor of size {x0.shape}\")\n",
    "        ctx.save_for_backward(x0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x0, = ctx.saved_tensors\n",
    "        result = (-torch.sin(x0)) * grad_output\n",
    "        return result\n",
    "\n",
    "# Wrap Cosine in a function so that it is clearer what the output is\n",
    "def cosine(x):\n",
    "    # `apply` will call `forward` and `setup_context`\n",
    "    y, x= Cosine.apply(x)\n",
    "    return y\n",
    "\n",
    "def naive_two_cosine(x0):\n",
    "    x1 = cosine(x0)\n",
    "    x2 = cosine(x1)\n",
    "    return x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在执行上述函数时，如果输入需要计算梯度，可以观察到有两个张量被保存下来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving tensor of size torch.Size([5, 5, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A input that has been returned as-is as output is being saved for backward. This is not supported if you override setup_context. You should return and save a view of the input instead, e.g. with x.view_as(x) or setup ctx inside the forward function itself.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m naive_two_cosine(\u001b[38;5;28minput\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36mnaive_two_cosine\u001b[0;34m(x0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnaive_two_cosine\u001b[39m(x0):\n\u001b[0;32m---> 27\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m cosine(x0)\n\u001b[1;32m     28\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m cosine(x1)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x2\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mcosine\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcosine\u001b[39m(x):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# `apply` will call `forward` and `setup_context`\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     y, x\u001b[38;5;241m=\u001b[39m Cosine\u001b[38;5;241m.\u001b[39mapply(x)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/media/pc/data/lxw/envs/anaconda3x/envs/xxx/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A input that has been returned as-is as output is being saved for backward. This is not supported if you override setup_context. You should return and save a view of the input instead, e.g. with x.view_as(x) or setup ctx inside the forward function itself."
     ]
    }
   ],
   "source": [
    "input = torch.randn((5, 5, 5), requires_grad=True)\n",
    "output = naive_two_cosine(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xxx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
