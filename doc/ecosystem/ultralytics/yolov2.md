# YOLOv2（YOLO9000）

```{topic} 核心改进策略 
1. 引入Anchor Boxes机制  
   - 替代YOLOv1中全连接层直接预测边界框的方式，采用预定义锚框（Anchor Boxes）提升定位精度。  
   - 通过聚类分析训练数据中的目标尺寸，生成5种不同宽高比的锚框，减少定位误差。
 
2. 高分辨率分类器（High Resolution Classifier）  
   - 在ImageNet上先以224×224分辨率预训练分类网络，再以448×448分辨率微调10轮，使网络适应高分辨率输入，提升检测精度约4%。
 
3. 批归一化（Batch Normalization）  
   - 在所有卷积层后加入BN层，加速收敛并替代Dropout的正则化效果，mAP提升2.4%。
```

YOLOv2（YOLO9000）可以检测 9000 多个对象类别。使用一种新颖的多尺度训练方法，相同的 YOLOv2 模型可以以不同的规模运行，从而在速度和准确性之间轻松权衡。在 67 FPS 时，YOLOv2 在 VOC 2007 上获得 76.8 mAP。在 40 FPS 时，YOLOv2 的 mAP 为 78.6 mAP，优于使用 ResNet 和 SSD 的 Faster RCNN 等最先进的方法，同时运行速度仍然明显更快。最后，提出了一种联合训练对象检测和分类的方法。使用这种方法，在 COCO 检测数据集和 ImageNet 分类数据集上同时训练 YOLO9000。联合训练使 YOLO9000 能够预测没有标记检测数据的对象类别的检测。YOLO9000 在 ImageNet 检测验证集上获得 19.7 mAP，尽管只有 200 个类别中 44 个类别的检测数据。在不在 COCO 中的 156 个类上，YOLO9000 获得 16.0 mAP。但 YOLO 可以检测到的不仅仅是 200 个类；它可以预测 9000 多个不同对象类别的检测。而且它仍然实时运行。

YOLOv2 提出了一种新方法来利用我们已经拥有的大量分类数据，并使用它来扩展当前检测系统的范围。YOLOv2 方法使用对象分类的分层视图，允许我们将不同的数据集组合在一起。YOLOv2 还提出了一种联合训练算法，允许在检测和分类数据上训练对象检测器。YOLOv2 的方法利用标记的检测图像来学习精确定位对象，同时使用分类图像来增加其词汇量和鲁棒性。使用这种方法，训练了 YOLO9000，这是一种实时对象检测器，可以检测 9000 多种不同的对象类别。首先，在基础 YOLO 检测系统的基础上进行了改进，以生产 YOLOv2，这是一种最先进的实时检测器。然后，使用数据集组合方法和联合训练算法，在来自 ImageNet 的 9000 多个类以及来自 COCO 的检测数据上训练模型。

YOLOv2 从 YOLO 中删除全连接层，并使用锚框来预测边界框。首先，移除一个池化层，使网络卷积层的输出具有更高的分辨率。还缩小了网络以对 416 输入图像而不是 448×448。这样做是因为希望在特征图中有一个奇数个位置，所以只有一个中心单元格。对象，尤其是大型对象，往往占据图像的中心，因此最好在中心有一个位置来预测这些对象，而不是四个位置都在附近。YOLO 的卷积层将图像的采样系数降低 32 倍，因此通过使用 416 的输入图像，我们得到的输出特征图 $13 \times 13$。

当移动到锚框时，还将类预测机制与空间位置解耦，而是预测每个锚框的类和对象性。在 YOLO 之后，对象性预测仍然预测基本事实和建议的框的 IOU，并且类预测预测该类的条件概率，前提是存在对象。

使用**锚框**，我们的准确率会略有下降。YOLO 只预测每张图像 98 个框，但使用锚框，我们的模型预测了 1000 多个框。在没有锚盒的情况下，我们的中间型号获得 69.5 mAP，召回率为 81%。使用锚盒时，我们的模型获得 69.2 mAP，召回率为 88%。尽管 mAP 降低，但召回率的增加意味着我们的模型还有更多的改进空间。

**维度集群**。在将锚框与 YOLO 一起使用时，我们会遇到两个问题。首先是BOX 尺寸是手工挑选的。网络可以学习适当地调整方框，但如果我们为网络选择更好的先验开始，我们可以使网络更容易学习预测良好的检测。我们不是手动选择先验，而是在训练集边界框上运行 k-means 聚类以自动化找到好的先验。如果我们使用具有欧几里得距离的标准 k-means，则较大的框比较小的框产生更多的误差。然而，我们真正想要的是导致良好 IOU 分数的先验，这与盒子的大小无关。因此，对于我们的距离度量，我们使用：

```
d(box, centroid) = 1 − IOU(box, centroid)
```

我们对 k 的各种值运行 k-means，并绘制具有最接近质心的平均 IOU。我们选择 k = 5 作为模型复杂性和高召回率之间的良好权衡。集群质心与手动选取的锚框明显不同。短而宽的箱子较少，而高而薄的箱子多。

**直接位置预测**。在 YOLO 中使用锚框时，我们会遇到第二个问题：模型不稳定，尤其是在早期迭代期间。大多数不稳定性来自预测框的 $(x, y)$ 位置。在区域建议网络中，网络预测值 $t_x$ 和 $t_y$，$(x, y)$ 中心坐标的计算公式为：

$$
\begin{aligned}
&x = (t_x ∗ w_a) − x_a\\
&y = (t_y ∗ h_a) − y_a
\end{aligned}
$$

例如，$t_x = 1$ 的预测会将框向右移动锚框的宽度，$t_x = −1$ 的预测会将其向左移动相同的量。

此公式不受约束，因此任何锚框都可以位于图像中的任何点，而不管预测框的位置如何。使用随机初始化时，模型需要很长时间才能稳定下来以预测合理的偏移量。

我们遵循 YOLO 的方法并预测相对于网格单元位置的位置坐标，而不是预测偏移量。这会将 Ground Truth 限制为介于 $0$ 和 $1$ 之间。我们使用 Logistic 激活来限制网络的预测落在此范围内。

该网络在输出特征图中的每个单元格预测 5 个边界框。该网络为每个边界框 $t_x$、$t_y$、$t_w$、$t_h$ 和 $t_o$ 预测 5 个坐标。如果单元格从图像左上角偏移 $(c_x, c_y)$，并且边界框 prior 的宽度和高度为 $p_w$，$p_h$，则预测对应于：

$$
\begin{aligned}
&b_x = \sigma(t_x) + c_x\\
&b_y = \sigma(t_y) + c_y\\
&b_w = p_w e^{t_w}\\
&b_h = p_h e^{t_h}\\
&Pr(object) ∗ IOU(b, object)  = \sigma(t_o)\\
\end{aligned}
$$

由于我们约束了位置预测，因此参数化更容易学习，从而使网络更加稳定。使用维度集群并直接预测边界框中心位置，与使用锚框的版本相比，YOLO 提高了近 5%。

```{figure} images/yolov2.png
具有维度先验和位置预测的边界框。我们将框的宽度和高度预测为与集群质心的偏移量。使用 sigmoid 函数预测盒子相对于滤波器应用程序位置的中心坐标。
```
