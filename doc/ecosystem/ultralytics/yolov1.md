# YOLOv1
 
以下是关于 YOLOv1 的详细阐述，结合其核心原理、网络设计及局限性等关键点进行结构化分析：

相反，我们将对象检测定义为

```{topic} 核心思想与创新 
YOLOv1（You Only Look Once）是首个将目标检测任务统一为**空间分离的边界框和相关类概率的回归问题**的模型，其主要创新点包括：
1. 全局推理机制：通过单次前向传播直接预测边界框位置和类别概率，摒弃了传统两阶段方法中的区域建议步骤。
2. 网格划分策略：将输入图像划分为  $S \times S$ 的网格（默认 $S=7$），每个网格负责预测中心点落在该区域的物体，最多预测 $B$ （默认 $B=1$） 个边界框（Bounding Box）。
3. 实时性优势：在Titan X GPU上达到 45 FPS，Fast YOLO 版本甚至可达 155 FPS，显著优于同期算法如 Fast R-CNN。
```

## 网络架构设计

```{figure} images/yolov1-a.png
:name: fig-yolov1-grid

YOLOv1 将检测建模为 $S \times S \times (4 + C)$ 张量的回归问题。此张量对图像中所有对象的边界框和类概率进行编码。其中 $S=7$（网格大小），$C=20$（VOC数据集类别数）。
```

YOLOv1 将输入图像划分为 $S \times S$ 网格。如果对象的中心落入网格单元中，则该网格单元负责检测该对象。每个网格单元预测一个边界框和与该边界框关联的类别概率，请参见 {numref}`fig-yolov1-grid`。

```{figure} images/yolov1-net.png
:name: fig-yolov1-net

检测网络有 24 个卷积层(提取特征)，后跟 2 个全连接层(预测输出概率和坐标)。该网络使用跨步(strided)卷积层来对特征空间进行下采样，而不是最大池化层。交替使用 $1 \times 1$ 卷积层可减少前一层的特征空间。在 ImageNet 分类任务上以一半的分辨率（$224 \times 224$ 输入图像）对卷积层进行预训练，然后将分辨率提高一倍进行检测。
```

网络的最终输出是 $S \times S$ 的预测网格。每个网格单元格预测 $C$ 个条件类概率和 4 个边界框坐标。

最后一层预测类概率和边界框坐标。通过图像宽度和高度对边界框的宽度和高度进行标准化，使它们介于 $0$ 和 $1$ 之间。将边界框 $x$ 和 $y$ 坐标参数化为特定网格单元位置的偏移量，以便它们也以 $0$ 和 $1$ 为边界。使用 logistic 激活函数将这些约束反映在最后一层。所有其他层使用以下 Leaky ReLU 激活。

## 损失函数设计

针对模型输出中的 **和平方** 误差进行优化。使用和平方误差，因为它很容易优化，但它与最大化平均精度的目标并不完全一致。它将定位误差与分类误差同等加权，而分类误差可能并不理想。为了解决这个问题，使用比例因子 $\lambda$ 来调整分配给坐标预测误差与类概率误差的权重。在最终模型中，使用比例因子 $\lambda=4$。

和平方误差在大框和小框中的权重也相等。误差指标应该反映出，大盒子中的小偏差比小盒子中的小偏差更不重要。为了部分解决这个问题，预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。

如果单元格 $i$ 预测类概率 $\hat{p}_i(飞机)$，$\hat{p}_i(自行车)$ ...，而边界框 $\hat{x}_i$ 、$\hat{y}_i$ 、$\hat{w}_i$ 、$\hat{h}_i$ 则全损失函数为：

$$
\sum_{i=0}^{S^2-1} \left[ \lambda \mathbb{1}_{i}^{\text{obj}} ((x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 + (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2) + \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2 \right]
$$

其中 $\mathbb{1}_{i}^{\text{obj}}$ 编码单元格 $i$ 中是否出现任何对象。请注意，如果单元格中没有对象，不会考虑该单元格预测的边界框坐标的任何损失。在这种情况下，没有 ground truth 边界框，因此只惩罚与该区域相关的概率。

每个网格单元都预测图像该区域的类概率。有 $S^2$ 个像元，每个像元可能有 $C$ 个类，每张图像产生 $CS^2$ 个预测概率。这些概率中的大多数为零，因为任何给定图像中都只出现少数对象。如果不加以控制，这种不平衡会将所有概率推至零，从而导致训练期间出现背离。为了解决这个问题，为每个网格位置添加了额外的变量，即任何对象存在于该位置的概率，无论类如何。因此，有 1 个“对象性”概率，即 Pr(Object) 和 $C$ 个条件概率，而不是 $C$ 个类概率：Pr(Airplane|Object)、Pr(自行车|Object) 等。

要获得给定位置的对象类的无条件概率，只需将 “objectness” 概率乘以条件类概率：
$$
Pr(Dog) = Pr(Object) ∗ Pr(Dog|Object)
$$

可以独立或联合使用卷积网络中的新型“检测层”来优化这些概率。在训练的初始阶段，会独立优化它们以提高模型稳定性。在每个位置更新“对象性”概率，但只在实际包含对象的位置更新条件概率。这意味着被推向零的概率要小得多。

在训练的后期阶段，通过在网络中执行所需的乘法并根据结果计算误差来优化无条件概率。

## 训练策略

1. 预训练与微调：  
   - 在 ImageNet 上预训练前 20 层（输入 224×224），后 4 层卷积和全连接层随机初始化；  
   - 微调阶段将输入分辨率提升至 448×448，增强细节感知。
2. 数据增强：采用随机缩放、平移和调整曝光度，提升模型泛化能力。

 
## 局限性

YOLOv1 对边界框预测施加了很强的空间限制，因为每个网格单元格只预测一个框。此空间约束限制了模型可以预测的附近对象的数量。如果两个对象落入同一个单元格，模型只能预测其中一个。模型难以处理成群出现的小物体，例如鸟群。由于模型学习从数据中预测边界框，因此它很难推广到具有新的或不寻常的纵横比或配置的对象。模型还使用相对粗略的特征来预测边界框，因为架构从输入图像中有多个下采样层。最后，当我们训练近似检测性能的损失函数时，我们的损失函数在小边界框和大边界框中处理错误的方式相同。大盒子中的小错误通常是良性的，但小盒子中的小错误对 IOU 的影响要大得多。我们的主要错误来源是不正确的局部化。

