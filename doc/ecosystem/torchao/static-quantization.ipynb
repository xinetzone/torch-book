{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4daadfdc",
   "metadata": {},
   "source": [
    "# 静态量化\n",
    "\n",
    "参考：[静态量化](https://docs.pytorch.org/ao/stable/static_quantization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894bbb8",
   "metadata": {},
   "source": [
    "静态量化是指在推理或生成过程中使用固定的量化范围。与动态量化不同，动态量化会为每个新的输入批次动态计算新的量化范围，静态量化通常会导致更高效的计算，但可能会牺牲量化精度，因为无法实时适应输入分布的变化。\n",
    "\n",
    "在静态量化中，这个固定的量化范围通常会在量化模型之前，在类似输入上进行校准。在校准阶段，首先在模型中插入观察者，以“观察”要量化的输入的分布，并使用该分布来决定在量化模型时最终使用哪些缩放因子和零点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bea4e1",
   "metadata": {},
   "source": [
    "通过示例来说明如何在 `torchao` 中实现这一点。所有代码都可以在[示例脚本](https://github.com/pytorch/ao/tree/main/tutorials/calibration_flow/static_quant.py)中找到。从玩具线性模型开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3faf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "class ToyLinearModel(torch.nn.Module):\n",
    "    def __init__(self, m=64, n=32, k=64):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(m, k, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(k, n, bias=False)\n",
    "\n",
    "    def example_inputs(self, batch_size=1, dtype=torch.float32, device=\"cpu\"):\n",
    "        return (\n",
    "            torch.randn(\n",
    "                batch_size, self.linear1.in_features, dtype=dtype, device=device\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "m = ToyLinearModel().eval().to(dtype).to(\"cuda\")\n",
    "m = torch.compile(m, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a44d1",
   "metadata": {},
   "source": [
    "## 校准阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7cb22",
   "metadata": {},
   "source": [
    "`torchao` 随附了简单的观察者实现 `AffineQuantizedMinMaxObserver`，该观察者在校准阶段记录了流经观察者的最小值和最大值。用户可以实现自己所需的更高级的观察技术，例如依赖于移动平均或直方图的方法，并且这些方法将来可以添加到 `torchao` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84f4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.quantization.granularity import PerAxis, PerTensor\n",
    "from torchao.quantization.observer import AffineQuantizedMinMaxObserver\n",
    "from torchao.quantization.quant_primitives import MappingType\n",
    "\n",
    "# per tensor input activation asymmetric quantization\n",
    "act_obs = AffineQuantizedMinMaxObserver(\n",
    "    MappingType.ASYMMETRIC,\n",
    "    torch.uint8,\n",
    "    granularity=PerTensor(),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# per channel weight asymmetric quantization\n",
    "weight_obs = AffineQuantizedMinMaxObserver(\n",
    "    MappingType.ASYMMETRIC,\n",
    "    torch.uint8,\n",
    "    granularity=PerAxis(axis=0),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25085c0",
   "metadata": {},
   "source": [
    "接下来，定义被观测的线性层，将用它替换 `torch.nn.Linear`。这是高精度（例如 `fp32`）的线性模块，并且在此模块中插入了上述观察者，用于在校准期间记录输入激活值和权重值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff90d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ObservedLinear(torch.nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        act_obs: torch.nn.Module,\n",
    "        weight_obs: torch.nn.Module,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.act_obs = act_obs\n",
    "        self.weight_obs = weight_obs\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        observed_input = self.act_obs(input)\n",
    "        observed_weight = self.weight_obs(self.weight)\n",
    "        return F.linear(observed_input, observed_weight, self.bias)\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, float_linear, act_obs, weight_obs):\n",
    "        observed_linear = cls(\n",
    "            float_linear.in_features,\n",
    "            float_linear.out_features,\n",
    "            act_obs,\n",
    "            weight_obs,\n",
    "            False,\n",
    "            device=float_linear.weight.device,\n",
    "            dtype=float_linear.weight.dtype,\n",
    "        )\n",
    "        observed_linear.weight = float_linear.weight\n",
    "        observed_linear.bias = float_linear.bias\n",
    "        return observed_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7addf",
   "metadata": {},
   "source": [
    "要在玩具模型中实际插入这些观察者："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "190fc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.quantization.quant_api import (\n",
    "    _replace_with_custom_fn_if_matches_filter,\n",
    ")\n",
    "\n",
    "def insert_observers_(model, act_obs, weight_obs):\n",
    "    _is_linear = lambda m, fqn: isinstance(m, torch.nn.Linear)\n",
    "\n",
    "    def replacement_fn(m):\n",
    "        copied_act_obs = copy.deepcopy(act_obs)\n",
    "        copied_weight_obs = copy.deepcopy(weight_obs)\n",
    "        return ObservedLinear.from_float(m, copied_act_obs, copied_weight_obs)\n",
    "\n",
    "    _replace_with_custom_fn_if_matches_filter(model, replacement_fn, _is_linear)\n",
    "\n",
    "insert_observers_(m, act_obs, weight_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bd18c",
   "metadata": {},
   "source": [
    "现在校准模型，这将在校准过程中记录统计数据的观察器填充数据。可以通过向“观察”模型输入一些示例数据来完成这一过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93269fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/lxw/envs/anaconda3a/envs/ai/lib/python3.12/site-packages/torch/_inductor/cudagraph_trees.py:2442: UserWarning: Unable to hit fast path of CUDAGraphs because of pending, uninvoked backwards. Consider running with torch.no_grad() or using torch.compiler.cudagraph_mark_step_begin() before each model invocation\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    example_inputs = m.example_inputs(dtype=dtype, device=\"cuda\")\n",
    "    m(*example_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bec60",
   "metadata": {},
   "source": [
    "## 量化阶段\n",
    "\n",
    "有多种方式进行模型量化。介绍一种更简单的替代方法，即定义 `QuantizedLinear` 类，将用这个类替换掉 `ObservedLinear`。定义这个类并不是必要的。对于另一种方法，可以使用 `torch.nn.Linear` 类，请参阅[示例类](https://github.com/pytorch/ao/tree/main/tutorials/calibration_flow/static_quant.py)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb9901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.dtypes import to_affine_quantized_intx_static\n",
    "\n",
    "class QuantizedLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        act_obs: torch.nn.Module,\n",
    "        weight_obs: torch.nn.Module,\n",
    "        weight: torch.Tensor,\n",
    "        bias: torch.Tensor,\n",
    "        target_dtype: torch.dtype,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.act_scale, self.act_zero_point = act_obs.calculate_qparams()\n",
    "        weight_scale, weight_zero_point = weight_obs.calculate_qparams()\n",
    "        assert weight.dim() == 2\n",
    "        block_size = (1, weight.shape[1])\n",
    "        self.target_dtype = target_dtype\n",
    "        self.bias = bias\n",
    "        self.qweight = to_affine_quantized_intx_static(\n",
    "            weight, weight_scale, weight_zero_point, block_size, self.target_dtype\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        block_size = input.shape\n",
    "        qinput = to_affine_quantized_intx_static(\n",
    "            input,\n",
    "            self.act_scale,\n",
    "            self.act_zero_point,\n",
    "            block_size,\n",
    "            self.target_dtype,\n",
    "        )\n",
    "        return F.linear(qinput, self.qweight, self.bias)\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, observed_linear, target_dtype):\n",
    "        quantized_linear = cls(\n",
    "            observed_linear.in_features,\n",
    "            observed_linear.out_features,\n",
    "            observed_linear.act_obs,\n",
    "            observed_linear.weight_obs,\n",
    "            observed_linear.weight,\n",
    "            observed_linear.bias,\n",
    "            target_dtype,\n",
    "        )\n",
    "        return quantized_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600f552",
   "metadata": {},
   "source": [
    "这个线性类在开始时计算输入激活和权重的缩放因子和零点，从而在未来的前向计算中固定量化范围。现在，可以定义以下配置并将其传递给 `torchao` 的 主 `quantize_` API，以实际对模型进行量化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74fcfb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from torchao.core.config import AOBaseConfig\n",
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.transform_module import (\n",
    "    register_quantize_module_handler,\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class StaticQuantConfig(AOBaseConfig):\n",
    "    target_dtype: torch.dtype\n",
    "\n",
    "@register_quantize_module_handler(StaticQuantConfig)\n",
    "def _apply_static_quant(\n",
    "    module: torch.nn.Module,\n",
    "    config: StaticQuantConfig,\n",
    "):\n",
    "    \"\"\"\n",
    "    Define a transformation associated with `StaticQuantConfig`.\n",
    "    This is called by `quantize_`, not by the user directly.\n",
    "    \"\"\"\n",
    "    return QuantizedLinear.from_observed(module, config.target_dtype)\n",
    "\n",
    "# filter function to identify which modules to swap\n",
    "is_observed_linear = lambda m, fqn: isinstance(m, ObservedLinear)\n",
    "\n",
    "# perform static quantization\n",
    "quantize_(m, StaticQuantConfig(torch.uint8), is_observed_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824dfd4",
   "metadata": {},
   "source": [
    "现在，将看到模型中的线性层被替换为 `QuantizedLinear` 类，并且输入激活的量化缩放因子和量化权重都是固定的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce7129e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): ToyLinearModel(\n",
       "    (linear1): QuantizedLinear()\n",
       "    (linear2): QuantizedLinear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16075db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0221], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.linear1.act_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ed0a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AffineQuantizedTensor(tensor_impl=PlainAQTTensorImpl(data=tensor([[  6, 216, 194,  ..., 254,   3, 124],\n",
       "        [ 60,  58, 240,  ...,  76, 104, 209],\n",
       "        [ 63,  47,   2,  ...,  73, 181, 199],\n",
       "        ...,\n",
       "        [127, 111,  94,  ...,  71, 253,  36],\n",
       "        [186, 252, 137,  ..., 200,  29,  55],\n",
       "        [242, 217, 169,  ..., 162, 119, 186]], device='cuda:0',\n",
       "       dtype=torch.uint8)... , scale=tensor([0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010, 0.0010,\n",
       "        0.0010, 0.0009, 0.0009, 0.0010, 0.0009, 0.0009, 0.0010, 0.0010, 0.0009,\n",
       "        0.0009, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010, 0.0009,\n",
       "        0.0009, 0.0010, 0.0009, 0.0010, 0.0009, 0.0009, 0.0010, 0.0009, 0.0010,\n",
       "        0.0009, 0.0009, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010,\n",
       "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
       "        0.0009, 0.0010, 0.0009, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010,\n",
       "        0.0010], device='cuda:0')... , zero_point=tensor([121., 131., 127., 126., 124., 127., 129., 129., 126., 125., 132., 131.,\n",
       "        128., 127., 126., 129., 128., 131., 127., 128., 123., 129., 132., 124.,\n",
       "        132., 129., 131., 126., 129., 126., 129., 125., 126., 129., 128., 128.,\n",
       "        130., 125., 126., 129., 128., 128., 127., 124., 129., 127., 129., 128.,\n",
       "        128., 129., 126., 128., 129., 126., 134., 129., 124., 128., 130., 124.,\n",
       "        128., 129., 128., 128.], device='cuda:0')... , _layout=PlainLayout()), block_size=(1, 64), shape=torch.Size([64, 64]), device=cuda:0, dtype=torch.bfloat16, requires_grad=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.linear1.qweight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
