{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b166530",
   "metadata": {},
   "source": [
    "# `torchao` 快速上手\n",
    "\n",
    "详细介绍见博文：[PyTorch Native Architecture Optimization: torchao](https://pytorch.org/blog/pytorch-native-architecture-optimization/)。\n",
    "\n",
    "安装\n",
    "```bash\n",
    "pip install torchao\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65168010",
   "metadata": {},
   "source": [
    "## 推理\n",
    "\n",
    "[推理量化](https://github.com/pytorch/ao/tree/main/torchao/quantization)算法适用于包含 `nn.Linear` 层的任意 PyTorch 模型。您可以使用顶级的 `quantize_` api 选择仅权重量化和各种数据类型及稀疏布局的动态激活量化。\n",
    "\n",
    "```python\n",
    "from torchao.quantization import (  \n",
    "    quantize_,  \n",
    "    int4_weight_only,  \n",
    ")  \n",
    "quantize_(model, int4_weight_only())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d514bcd",
   "metadata": {},
   "source": [
    "有时量化一层可能会因为开销变慢，如果希望选择模型中每一层的量化方式，那么你可以改用\n",
    "```python\n",
    "model = torchao.autoquant(torch.compile(model, mode='max-autotune'))\n",
    "```\n",
    "\n",
    "`quantize_` API 会根据你的模型是计算受限还是内存受限，提供几种不同的选项。\n",
    "\n",
    "```python\n",
    "from torchao.quantization import (  \n",
    "    # Memory bound models  \n",
    "    int4_weight_only,  \n",
    "    int8_weight_only,\n",
    "\n",
    "    # Compute bound models  \n",
    "    int8_dynamic_activation_int8_semi_sparse_weight,  \n",
    "    int8_dynamic_activation_int8_weight,  \n",
    "      \n",
    "    # Device capability 8.9+  \n",
    "    float8_weight_only,  \n",
    "    float8_dynamic_activation_float8_weight,  \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ec46b",
   "metadata": {},
   "source": [
    "与 HuggingFace diffusers 团队合作的 [diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao) 中进行了大量的扩散模型基准测试，结果显示在 Flux.1-Dev 上实现了 $53.88\\%$ 的加速，在 CogVideoX-5b 上实现了 $27.33\\%$ 的加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ccb67",
   "metadata": {},
   "source": [
    "API 是可组合的，因此例如将稀疏性和量化组合起来，为 [ViT-H 推理](https://github.com/pytorch/ao/tree/main/torchao/sparsity)带来了 $5\\%$ 的加速。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7914208",
   "metadata": {},
   "source": [
    "还可以将权重量化为 int4，将 kv 缓存量化为 int8，以支持在 128K 上下文长度下使用不到 18.9GB 的 [VRAM 运行 Llama 3.1 8B 模型](https://github.com/pytorch/ao/pull/738)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7c9ad",
   "metadata": {},
   "source": [
    "## QAT 量化训练\n",
    "\n",
    "后训练量化，尤其是在低于 4 位的情况下，可能会遭受严重的准确率下降。通过使用[量化感知训练](https://pytorch.org/blog/quantization-aware-training/)（QAT），成功地在 hellaswag 数据集上恢复了高达 $96\\%$ 的准确率下降。已在 torchtune 中将此集成为了一端到端的解决方案，并附带了简短的[教程](https://github.com/pytorch/ao/tree/main/torchao/quantization/prototype/qat)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f10aae",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "### 低精度计算和通信\n",
    "\n",
    "`torchao` 提供了易于使用的端到端工作流，用于降低训练计算和分布式通信的精度，从 `torch.nn.Linear` 层的 float8 开始。以下是将训练运行中的计算 GEMM 转换为 float8 的一行代码：\n",
    "```python\n",
    "from torchao.float8 import convert_to_float8_training  \n",
    "convert_to_float8_training(model)\n",
    "```\n",
    "\n",
    "要查看如何通过使用 `float8` 将 LLaMa 3 70B 的预训练速度提高至多 1.5 倍的端到端示例，请参阅 [README](https://github.com/pytorch/ao/tree/main/torchao/float8)，以及 torchtitan 的[博客](https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359)和 [float8 配方](https://github.com/pytorch/torchtitan/blob/main/docs/float8.md)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa77c21",
   "metadata": {},
   "source": [
    "### LLaMa 3 70B 浮点 8 位预训练与 bfloat16 的性能和准确性对比\n",
    "\n",
    "![图像来源：<https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359>](https://pytorch.org/assets/images/Figure_4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e06e8f",
   "metadata": {},
   "source": [
    "扩展训练工作流以支持更多的数据类型和布局\n",
    "- [NF4 QLoRA 在 torchtune 中](https://pytorch.org/torchtune/main/tutorials/qlora_finetune.html)\n",
    "- [原型训练支持 int8](https://github.com/pytorch/ao/pull/748)\n",
    "- [加速稀疏 2:4 训练](https://pytorch.org/blog/accelerating-neural-network-training/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23365e7e",
   "metadata": {},
   "source": [
    "## 低比特优化器\n",
    "\n",
    "受 Bits and Bytes 的启发，还为 8 位和 4 位优化器添加了原型支持，作为 AdamW 的即插即用替代品。\n",
    "\n",
    "```python\n",
    "from torchao.prototype.low_bit_optim import AdamW8bit, AdamW4bit  \n",
    "optim = AdamW8bit(model.parameters())\n",
    "```\n",
    "\n",
    "![](https://pytorch.org/wp-content/uploads/2024/11/image-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5d03d",
   "metadata": {},
   "source": [
    "## 集成\n",
    "\n",
    "`torchao` 能够在开源项目中的一些最重要项目中正常工作\n",
    "- Huggingface transformers 作为[推理后端](https://huggingface.co/docs/transformers/main/quantization/torchao)\n",
    "- 在 [diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao) 中作为加速扩散模型的参考实现\n",
    "- 在 HQQ 中进行[快速 4 bit 推理](https://github.com/mobiusml/hqq#faster-inference)\n",
    "- 在 [`torchtune`](https://github.com/pytorch/torchtune) 中进行 PyTorch 原生 QLoRA 和 QAT 食谱\n",
    "- 在 [`torchchat`](https://github.com/pytorch/torchchat) 中进行后训练量化\n",
    "- 在 SGLang 中进行 [int4 和 int8 后训练量化](https://github.com/sgl-project/sglang/pull/1341)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7008c3d",
   "metadata": {},
   "source": [
    "## 量化示例\n",
    "\n",
    "torchao 中量化的主入口点是 `quantize_` API。此函数会原地修改你的模型，根据用户配置插入自定义的量化逻辑。本指南中的所有代码都可以在示例脚本中找到。\n",
    "\n",
    "首先，设置玩具模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "class ToyLinearModel(torch.nn.Module):\n",
    "    def __init__(self, m: int, n: int, k: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(m, n, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(n, k, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = ToyLinearModel(1024, 1024, 1024).eval().to(torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "# Optional: compile model for faster inference and generation\n",
    "model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n",
    "model_bf16 = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e7ad6",
   "metadata": {},
   "source": [
    "现在调用主要的量化 API，将模型中的线性权重原地量化为 `int4`。具体来说，这应用了基于 `uint4` 仅权重非对称分组量化，并利用 [tinygemm int4mm CUDA 内核](https://github.com/pytorch/pytorch/blob/a8d6afb511a69687bbb2b7e88a3cf67917e1697e/aten/src/ATen/native/cuda/int4mm.cu#L1097)进行高效的混合数据类型矩阵乘法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch 2.4+ only\n",
    "from torchao.quantization import Int4WeightOnlyConfig, quantize_\n",
    "quantize_(model, Int4WeightOnlyConfig(group_size=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261703c6",
   "metadata": {},
   "source": [
    "现在，量化后的模型已经准备好使用了！请注意，量化逻辑是通过张量子类插入的，因此模型的整体结构没有变化；只有权重张量被更新，但 `nn.Linear` 模块仍然保持为 `nn.Linear` 模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea8349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49856cf8",
   "metadata": {},
   "source": [
    "首先，验证 `int4` 量化模型大约是原 `bfloat16` 模型大小的四分之一："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "temp_dir = Path(\".temp\")\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "with tempfile.TemporaryDirectory(dir=temp_dir) as tmpdirname:\n",
    "    torch.save(model, f\"{tmpdirname}/int4_model.pt\")\n",
    "    torch.save(model_bf16, f\"{tmpdirname}/bfloat16_model.pt\")\n",
    "    int4_model_size_mb = os.path.getsize(f\"{tmpdirname}/int4_model.pt\") / 1024 / 1024\n",
    "    bfloat16_model_size_mb = os.path.getsize(f\"{tmpdirname}/bfloat16_model.pt\") / 1024 / 1024\n",
    "    print(f\"int4 model size: {int4_model_size_mb:.2f} MB\")\n",
    "    print(f\"bfloat16 model size: {bfloat16_model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba9d42",
   "metadata": {},
   "source": [
    "接下来，演示量化模型不仅更小，而且还要快得多！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchao.utils import (\n",
    "    TORCH_VERSION_AT_LEAST_2_5,\n",
    "    benchmark_model,\n",
    "    unwrap_tensor_subclass,\n",
    ")\n",
    "\n",
    "# Temporary workaround for tensor subclass + torch.compile\n",
    "# Only needed for torch version < 2.5\n",
    "if not TORCH_VERSION_AT_LEAST_2_5:\n",
    "    unwrap_tensor_subclass(model)\n",
    "\n",
    "num_runs = 100\n",
    "torch._dynamo.reset()\n",
    "example_inputs = (torch.randn(1, 1024, dtype=torch.bfloat16, device=\"cuda\"),)\n",
    "bf16_time = benchmark_model(model_bf16, num_runs, example_inputs)\n",
    "int4_time = benchmark_model(model, num_runs, example_inputs)\n",
    "\n",
    "print(\"bf16 mean time: %0.3f ms\" % bf16_time)\n",
    "print(\"int4 mean time: %0.3f ms\" % int4_time)\n",
    "print(\"speedup: %0.1fx\" % (bf16_time / int4_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d20252",
   "metadata": {},
   "source": [
    "在单块带有 80GB 内存的 A100 GPU 上，这会打印：\n",
    "```bash\n",
    "bf16 mean time: 30.393 ms\n",
    "int4 mean time: 4.410 ms\n",
    "speedup: 6.9x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a5a85",
   "metadata": {},
   "source": [
    "## PyTorch Export 量化\n",
    "\n",
    "PyTorch 2 Export 量化是一种全图量化工作流，主要用于静态量化。它针对需要输入和输出激活以及权重进行量化的硬件，并依赖于识别算子模式来做出量化决策（例如 `linear - relu`）。PT2E 量化会在操作周围插入量化和反量化算子，并在  lowering 过程中将量化算子模式融合为实际的量化算子。目前有两种典型的降低路径：\n",
    "1. 通过 inductor lowering 的 `torch.compile` \n",
    "2. 通过委托的 ExecuTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0579e74f",
   "metadata": {},
   "source": [
    "展示使用 `X86InductorQuantizer` 的示例\n",
    "```python\n",
    "import torch\n",
    "from torchao.quantization.pt2e.quantize_pt2e import prepare_pt2e\n",
    "from torch.export import export\n",
    "from torchao.quantization.pt2e.quantizer.x86_inductor_quantizer import (\n",
    "    X86InductorQuantizer,\n",
    "    get_default_x86_inductor_quantization_config,\n",
    ")\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(5, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "       return self.linear(x)\n",
    "\n",
    "# initialize a floating point model\n",
    "float_model = M().eval()\n",
    "\n",
    "# define calibration function\n",
    "def calibrate(model, data_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            model(image)\n",
    "\n",
    "# Step 1. program capture\n",
    "m = export(float_model, *example_inputs).module()\n",
    "# we get a model with aten ops\n",
    "\n",
    "# Step 2. quantization\n",
    "# backend developer will write their own Quantizer and expose methods to allow\n",
    "# users to express how they\n",
    "# want the model to be quantized\n",
    "quantizer = X86InductorQuantizer()\n",
    "quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())\n",
    "\n",
    "# or prepare_qat_pt2e for Quantization Aware Training\n",
    "m = prepare_pt2e(m, quantizer)\n",
    "\n",
    "# run calibration\n",
    "# calibrate(m, sample_inference_data)\n",
    "m = convert_pt2e(m)\n",
    "\n",
    "# Step 3. lowering\n",
    "# lower to target backend\n",
    "\n",
    "# Optional: using the C++ wrapper instead of default Python wrapper\n",
    "import torch._inductor.config as config\n",
    "config.cpp_wrapper = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    optimized_model = torch.compile(converted_model)\n",
    "\n",
    "    # Running some benchmark\n",
    "    optimized_model(*example_inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7857a9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810d454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
