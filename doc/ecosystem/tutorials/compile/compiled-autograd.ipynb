{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编译自动求导：捕获更大的反向传播图以进行 {func}`torch.compile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译自动微分（Compiled Autograd）是 PyTorch 2.4 中引入的 {func}`torch.compile` 扩展，它能够捕获更大的反向传播图。\n",
    "\n",
    "尽管 {func}`torch.compile` 确实捕获了反向传播图，但它只是部分捕获。AOTAutograd 组件提前捕获反向传播图，但存在一些限制：\n",
    "\n",
    "- 前向传播中的图断裂会导致反向传播中的图断裂\n",
    "- [反向传播钩子](https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution)未被捕获\n",
    "\n",
    "编译自动微分通过直接与自动微分引擎集成，解决了这些限制，使其能够在运行时捕获完整的反向传播图。具有这两种特性的模型应该尝试使用编译自动微分，并可能观察到更好的性能。\n",
    "\n",
    "然而，编译自动微分也引入了自身的限制：\n",
    "\n",
    "- 在反向传播开始时增加了缓存查找的运行时开销\n",
    "- 由于捕获的图更大，在 dynamo 中更容易导致重新编译和图断裂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "编译自动求导功能正在积极开发中，尚未与所有现有的 PyTorch 功能兼容。要了解特定功能的最新状态，请参阅[编译自动求导功能登陆](https://docs.google.com/document/d/11VucFBEewzqgkABIjebZIzMvrXr3BtcY1aGKpX61pJY)页面。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本教程中，我们将基于这个简单的神经网络模型进行示例演示。该模型接收 10 维的输入向量，通过线性层进行处理，并输出另一个 10 维的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "      self.linear = torch.nn.Linear(10, 10)\n",
    "\n",
    "   def forward(self, x):\n",
    "      return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本用法\n",
    "在调用 {func}`torch.compile` API 之前，请确保将 {data}`torch._dynamo.config.compiled_autograd` 设置为 `True`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc/data/lxw/envs/anaconda3a/envs/ai/lib/python3.12/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/media/pc/data/lxw/envs/anaconda3a/envs/ai/lib/python3.12/site-packages/onnxscript/converter.py:820: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "x = torch.randn(10)\n",
    "\n",
    "torch._dynamo.config.compiled_autograd = True\n",
    "@torch.compile\n",
    "def train(model, x):\n",
    "   loss = model(x).sum()\n",
    "   loss.backward()\n",
    "\n",
    "train(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述代码中，我们创建了 ``Model`` 类的一个实例，并通过 ``torch.randn(10)`` 生成了一个随机的 10 维张量 ``x``。\n",
    "我们定义了训练循环函数 ``train``，并使用 ``@torch.compile`` 装饰它以优化其执行。\n",
    "当调用 ``train(model, x)`` 时：\n",
    "\n",
    "* Python 解释器调用 Dynamo，因为此调用被 ``@torch.compile`` 装饰。\n",
    "* Dynamo 拦截 Python 字节码，模拟其执行并记录操作到图中。\n",
    "* ``AOTDispatcher`` 禁用钩子并调用 autograd 引擎来计算 ``model.linear.weight`` 和 ``model.linear.bias`` 的梯度，并将操作记录到图中。使用 ``torch.autograd.Function``，AOTDispatcher 重写了 ``train`` 的前向和反向实现。\n",
    "* Inductor 生成一个与 AOTDispatcher 前向和反向的优化实现相对应的函数。\n",
    "* Dynamo 设置优化后的函数，以便 Python 解释器接下来对其进行求值。\n",
    "* Python 解释器执行优化后的函数，该函数执行 ``loss = model(x).sum()``。\n",
    "* Python 解释器执行 ``loss.backward()``，调用 autograd 引擎，由于我们设置了 ``torch._dynamo.config.compiled_autograd = True``，因此路由到编译自动求导引擎。\n",
    "* 编译自动求导引擎计算 ``model.linear.weight`` 和 ``model.linear.bias`` 的梯度，并将操作记录到图中，包括它遇到的任何钩子。在此过程中，它将记录之前由 AOTDispatcher 重写的反向操作。编译自动求导引擎随后生成一个新函数，该函数对应于 ``loss.backward()`` 的完全追踪实现，并在推理模式下使用 ``torch.compile`` 执行它。\n",
    "* 相同的步骤递归地应用于编译自动求导图，但这次 AOTDispatcher 不需要对图进行分区。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查编译自动求导日志\n",
    "\n",
    "通过设置 ``TORCH_LOGS`` 环境变量运行脚本：\n",
    "\n",
    "* 仅打印编译自动求导图，使用 ``TORCH_LOGS=\"compiled_autograd\" python example.py``\n",
    "* 打印包含更多张量元数据和重新编译原因的图，但会牺牲性能，使用 ``TORCH_LOGS=\"compiled_autograd_verbose\" python example.py``\n",
    "\n",
    "重新运行上述代码片段，编译自动求导图现在应记录到 ``stderr``。某些图节点将带有前缀 ``aot0_``，这些节点对应于之前在 AOTAutograd 反向图 0 中提前编译的节点，例如，``aot0_view_2`` 对应于 id=0 的 AOT 反向图中 ``view_2``。\n",
    "\n",
    "在下图中，红色框包含由 ``torch.compile`` 捕获的 AOT 反向图，而未使用编译自动求导。\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/entire_verbose_log.png)\n",
    "\n",
    "```{note}\n",
    "这是我们将调用 ``torch.compile`` 的图，**不是**优化后的图。编译自动求导本质上生成一些未优化的 Python 代码来表示整个 C++ 自动求导执行过程。\n",
    "```\n",
    "\n",
    "## 使用不同标志编译前向和反向传播\n",
    "\n",
    "你可以为两次编译使用不同的编译器配置，例如，即使前向传播中存在图断裂，反向传播也可能是全图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x):\n",
    "    model = torch.compile(model)\n",
    "    loss = model(x).sum()\n",
    "    torch._dynamo.config.compiled_autograd = True\n",
    "    torch.compile(lambda: loss.backward(), fullgraph=True)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者，你可以使用上下文管理器，它将应用于其作用域内的所有自动求导调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x):\n",
    "   model = torch.compile(model)\n",
    "   loss = model(x).sum()\n",
    "   with torch._dynamo.compiled_autograd.enable(torch.compile(fullgraph=True)):\n",
    "      loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编译自动求导解决了 AOTAutograd 的某些限制\n",
    "\n",
    "1. 前向传播中的图断裂不再必然导致反向传播中的图断裂："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(backend=\"aot_eager\")\n",
    "def fn(x):\n",
    "   # 1st graph\n",
    "   temp = x + 10\n",
    "   torch._dynamo.graph_break()\n",
    "   # 2nd graph\n",
    "   temp = temp + 10\n",
    "   torch._dynamo.graph_break()\n",
    "   # 3rd graph\n",
    "   return temp.sum()\n",
    "\n",
    "x = torch.randn(10, 10, requires_grad=True)\n",
    "torch._dynamo.utils.counters.clear()\n",
    "loss = fn(x)\n",
    "\n",
    "# 1. base torch.compile\n",
    "loss.backward(retain_graph=True)\n",
    "assert(torch._dynamo.utils.counters[\"stats\"][\"unique_graphs\"] == 3)\n",
    "torch._dynamo.utils.counters.clear()\n",
    "\n",
    "# 2. torch.compile with compiled autograd\n",
    "with torch._dynamo.compiled_autograd.enable(torch.compile(backend=\"aot_eager\")):\n",
    "   loss.backward()\n",
    "\n",
    "# single graph for the backward\n",
    "assert(torch._dynamo.utils.counters[\"stats\"][\"unique_graphs\"] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一个 ``torch.compile`` 案例中，由于编译函数 ``fn`` 中的 2 个图断裂，生成了 3 个反向图。\n",
    "而在第二个使用编译自动求导的 ``torch.compile`` 案例中，尽管存在图断裂，仍然追踪到了一个完整的反向图。\n",
    "\n",
    "```{note}\n",
    "当追踪由编译自动求导捕获的反向钩子时，Dynamo 仍然可能发生图断裂。\n",
    "```\n",
    "\n",
    "2. 现在可以捕获反向钩子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(backend=\"aot_eager\")\n",
    "def fn(x):\n",
    "   return x.sum()\n",
    "\n",
    "x = torch.randn(10, 10, requires_grad=True)\n",
    "x.register_hook(lambda grad: grad+10)\n",
    "loss = fn(x)\n",
    "\n",
    "with torch._dynamo.compiled_autograd.enable(torch.compile(backend=\"aot_eager\")):\n",
    "   loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图中应该有 ``call_hook`` 节点，Dynamo 稍后会将其内联到以下内容中：![](https://pytorch.org/tutorials/_images/call_hook_node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编译自动求导的常见重新编译原因\n",
    "\n",
    "1. 由于损失值的自动求导结构发生变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.config.compiled_autograd = True\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "for op in [torch.add, torch.sub, torch.mul, torch.div]:\n",
    "   loss = op(x, x).sum()\n",
    "   torch.compile(lambda: loss.backward(), backend=\"eager\")()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，我们在每次迭代时调用不同的操作符，导致 ``loss`` 每次都跟踪不同的自动求导历史。你应该会看到一些重新编译的消息：**由于新的自动求导节点导致的缓存未命中**。\n",
    "![](https://pytorch.org/tutorials/_images/recompile_due_to_node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 由于张量形状的变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.config.compiled_autograd = True\n",
    "for i in [10, 100, 10]:\n",
    "   x = torch.randn(i, i, requires_grad=True)\n",
    "   loss = x.sum()\n",
    "   torch.compile(lambda: loss.backward(), backend=\"eager\")()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的例子中，``x`` 的形状发生了变化，编译后的自动求导会在第一次变化后将 ``x`` 标记为动态形状的张量。你应该会看到重新编译的消息：**由于形状变化导致的缓存未命中**。\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/recompile_due_to_dynamic.png)\n",
    "\n",
    "## 结论\n",
    "\n",
    "在本教程中，我们探讨了 ``torch.compile`` 与编译后的自动求导的高级生态系统，介绍了编译后的自动求导的基础知识，以及一些常见的重新编译原因。请继续关注 [dev-discuss](https://dev-discuss.pytorch.org/) 的深入探讨。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
