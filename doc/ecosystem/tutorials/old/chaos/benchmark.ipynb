{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Benchmark\n",
    "\n",
    "基准测试（Benchmark）是编写代码的重要步骤。它帮助验证代码是否满足性能预期，比较解决相同问题的不同方法，防止性能倒退。\n",
    "\n",
    "当涉及到 PyTorch 代码的基准测试时，有许多选项，包括 Python 内置的 {mod}`timeit` 模块。然而，对 PyTorch 代码进行基准测试有许多容易被忽略的注意事项，例如管理线程数量和同步 CUDA 设备。此外，为基准测试生成张量输入可能相当乏味。\n",
    "\n",
    "本教程演示了如何使用 PyTorch `benchmark` 模块来避免常见错误，同时更容易比较不同代码的性能，为基准测试生成输入等。\n",
    "\n",
    "## 定义 benchmark 函数\n",
    "\n",
    "比较使用现有 torch 算子实现 `torch.dot` 两种方法：一种方法使用 `mul` 和 `sum` 的组合，而另一种方法将问题归约到 `bmm`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def batched_dot_mul_sum(a, b):\n",
    "    '''Computes batched dot by multiplying and summing'''\n",
    "    return a.mul(b).sum(-1)\n",
    "\n",
    "\n",
    "def batched_dot_bmm(a, b):\n",
    "    '''Computes batched dot by reducing to bmm'''\n",
    "    a = a.reshape(-1, 1, a.shape[-1])\n",
    "    b = b.reshape(-1, b.shape[-1], 1)\n",
    "    return torch.bmm(a, b).flatten(-3)\n",
    "\n",
    "\n",
    "# 输入的基准测试\n",
    "x = torch.randn(10000, 64)\n",
    "\n",
    "# 确保两个函数计算相同的输出\n",
    "assert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 {class}`timeit.Timer` 作基准测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):  158.7 us\n",
      "bmm(x, x):      113.3 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 {class}`torch.utils.benchmark.Timer` 作基准测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc88e3b7d60>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  342.40 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc92d5ba8c0>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  800.27 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尽管 API 基本功能是相同的，但仍有一些重要的区别。`benchmark.Timer.timeit()` 返回每次运行的时间，而不是像 `timeit.Timer.timeit()` 那样返回总运行时。PyTorch `benchmark` 模块还提供了格式化的字符串表示，用于打印结果。\n",
    "\n",
    "另一个重要的区别，也是导致结果分歧的原因是 PyTorch `benchmark` 模块默认运行在一个线程中。可以使用 `num_threads` 参数来更改线程数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 24 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc93e8aa7d0>\n",
      "Multithreaded batch dot: Implemented using mul and sum\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  154.94 us\n",
      "  1 measurement, 100 runs , 24 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc92d7d9390>\n",
      "Multithreaded batch dot: Implemented using bmm\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  137.89 us\n",
      "  1 measurement, 100 runs , 24 threads\n"
     ]
    }
   ],
   "source": [
    "num_threads = torch.get_num_threads()\n",
    "print(f'Benchmarking on {num_threads} threads')\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x},\n",
    "    num_threads=num_threads,\n",
    "    label='Multithreaded batch dot',\n",
    "    sub_label='Implemented using mul and sum')\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x},\n",
    "    num_threads=num_threads,\n",
    "    label='Multithreaded batch dot',\n",
    "    sub_label='Implemented using bmm')\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在所有线程都可用的情况下运行 `benchmark` 会得到与 `timeit` 模块类似的结果。更重要的是，哪个版本更快取决于我们运行代码的线程数。这就是为什么用代表实际用例的线程设置来对代码进行基准测试是很重要的。另一件需要记住的重要事情是在 GPU 上进行基准测试时同步 CPU 和 CUDA。让我们在 CUDA 张量上再次运行上述基准测试，看看会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):  514.0 us\n",
      "mul_sum(x, x):   27.7 us\n",
      "bmm(x, x):      7679.2 us\n",
      "bmm(x, x):       35.8 us\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10000, 1024, device='cuda')\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "# Ran each twice to show difference before/after warmup\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc88e3b7e20>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  231.28 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc88e3b7460>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  249.74 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "# Run only once since benchmark module does warmup for us\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果揭示了一些有趣的事情。使用 `timeit` 模块的 `bmm` 版本的第一次运行比第二次运行要长得多。这是因为 `bmm` 调用 cuBLAS 需要在第一次调用时加载，这需要一些时间。这就是为什么在进行基准测试之前进行热身是很重要的，幸运的是，PyTorch 的基准测试模块会处理这些问题。\n",
    "\n",
    "`timeit` 和基准测试模块之间的结果差异是因为 `timeit` 模块没有同步 CUDA，因此只是计时内核的启动时间。PyTorch 的基准测试模块为我们完成同步。\n",
    "\n",
    "## Blocked Autorange 基准测试\n",
    "\n",
    "而 `timeit.Timer.autorange` 需要至少 0.2 秒的单个连续测量，`torch.utils.benchmark.blocked_autorange` 执行许多度量，这些度量的总时间至少为 0.2 秒（可以通过 `min_run_time` 参数更改），这取决于计时开销只占整体度量的一小部分的约束。这是通过首先在每个循环中运行不断增加的运行次数来实现的，直到运行时远远大于度量开销（这也可以作为热身），然后进行度量，直到达到目标时间。这有一个有用的特性，它浪费较少的数据，并允许我们计算统计数据来估计测量的可靠性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc93e8a9690>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  229.31 us\n",
      "  1 measurement, 1000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc88e3b75e0>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  Median: 182.84 us\n",
      "  2 measurements, 1000 runs per measurement, 1 thread\n"
     ]
    }
   ],
   "source": [
    "m0 = t0.blocked_autorange()\n",
    "m1 = t1.blocked_autorange()\n",
    "\n",
    "print(m0)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以从返回的度量对象中检查单个统计信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:   229.31 us\n",
      "Median: 229.31 us\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean:   {m0.mean * 1e6:6.2f} us\")\n",
    "print(f\"Median: {m0.median * 1e6:6.2f} us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 比较基准测试结果\n",
    "\n",
    "到目前为止，我们一直在将两个版本的批处理 `dot` 与单个输入进行比较。在实践中，我们希望尝试输入的组合以及不同数量的线程。`Compare` 类帮助在格式化的表中显示许多度量的结果。它使用上面描述的注释 （`label`、`sub_label`、`num_threads` 等）以及描述对表进行分组和组织。让我们使用 `Compare` 来查看函数在不同输入大小和线程数量下的执行情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------- Batched dot ----------------]\n",
      "                      |  mul/sum   |    bmm   \n",
      "1 threads: -----------------------------------\n",
      "      [1, 1]          |       5.2  |       9.1\n",
      "      [1, 64]         |       5.7  |       9.1\n",
      "      [1, 1024]       |       5.8  |      10.6\n",
      "      [1, 10000]      |      10.2  |      11.8\n",
      "      [64, 1]         |       9.7  |      15.7\n",
      "      [64, 64]        |       7.5  |      14.4\n",
      "      [64, 1024]      |      36.5  |     230.4\n",
      "      [64, 10000]     |     294.3  |    2105.8\n",
      "      [1024, 1]       |       7.0  |      16.7\n",
      "      [1024, 64]      |      42.5  |      88.0\n",
      "      [1024, 1024]    |     482.4  |    3505.7\n",
      "      [1024, 10000]   |   27680.6  |   34140.8\n",
      "      [10000, 1]      |      16.9  |      81.1\n",
      "      [10000, 64]     |     338.7  |     746.7\n",
      "      [10000, 1024]   |   27325.7  |   34208.9\n",
      "      [10000, 10000]  |  333021.2  |  333304.7\n",
      "4 threads: -----------------------------------\n",
      "      [1, 1]          |       5.2  |       9.3\n",
      "      [1, 64]         |       5.7  |       9.1\n",
      "      [1, 1024]       |       5.9  |      12.0\n",
      "      [1, 10000]      |      11.9  |      19.7\n",
      "      [64, 1]         |       9.8  |      20.6\n",
      "      [64, 64]        |       7.6  |      15.7\n",
      "      [64, 1024]      |      37.9  |     319.9\n",
      "      [64, 10000]     |      95.1  |    2994.2\n",
      "      [1024, 1]       |       6.8  |      15.7\n",
      "      [1024, 64]      |      46.2  |      36.7\n",
      "      [1024, 1024]    |     139.4  |     991.1\n",
      "      [1024, 10000]   |   10841.3  |    9254.5\n",
      "      [10000, 1]      |      16.9  |      32.4\n",
      "      [10000, 64]     |     105.1  |     217.0\n",
      "      [10000, 1024]   |   11494.8  |    9471.9\n",
      "      [10000, 10000]  |  108579.2  |   92878.2\n",
      "16 threads: ----------------------------------\n",
      "      [1, 1]          |       5.2  |       9.2\n",
      "      [1, 64]         |       5.6  |      15.5\n",
      "      [1, 1024]       |       6.3  |      14.3\n",
      "      [1, 10000]      |      15.0  |      21.2\n",
      "      [64, 1]         |       9.9  |      17.1\n",
      "      [64, 64]        |      12.4  |      23.9\n",
      "      [64, 1024]      |      49.7  |     717.2\n",
      "      [64, 10000]     |      56.9  |    5740.0\n",
      "      [1024, 1]       |       7.3  |      24.7\n",
      "      [1024, 64]      |      43.0  |      24.7\n",
      "      [1024, 1024]    |      75.3  |     299.0\n",
      "      [1024, 10000]   |    8515.2  |    2623.2\n",
      "      [10000, 1]      |      22.8  |      31.5\n",
      "      [10000, 64]     |      62.6  |      83.5\n",
      "      [10000, 1024]   |    8212.7  |    2638.4\n",
      "      [10000, 10000]  |   85685.6  |   24352.7\n",
      "32 threads: ----------------------------------\n",
      "      [1, 1]          |       5.4  |      10.0\n",
      "      [1, 64]         |       5.8  |       9.3\n",
      "      [1, 1024]       |       6.3  |      11.3\n",
      "      [1, 10000]      |      15.2  |      19.1\n",
      "      [64, 1]         |       5.8  |      29.5\n",
      "      [64, 64]        |       7.5  |      27.2\n",
      "      [64, 1024]      |      65.6  |     643.7\n",
      "      [64, 10000]     |     157.0  |    8721.4\n",
      "      [1024, 1]       |       6.9  |      27.5\n",
      "      [1024, 64]      |      65.8  |      33.1\n",
      "      [1024, 1024]    |      95.3  |     213.9\n",
      "      [1024, 10000]   |    7574.0  |    1503.8\n",
      "      [10000, 1]      |      17.2  |      31.1\n",
      "      [10000, 64]     |     140.7  |      93.3\n",
      "      [10000, 1024]   |    7618.5  |    1396.9\n",
      "      [10000, 10000]  |   77288.0  |   18593.7\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Compare takes a list of measurements which we'll save in results.\n",
    "results = []\n",
    "\n",
    "sizes = [1, 64, 1024, 10000]\n",
    "for b, n in product(sizes, sizes):\n",
    "    # label and sub_label are the rows\n",
    "    # description is the column\n",
    "    label = 'Batched dot'\n",
    "    sub_label = f'[{b}, {n}]'\n",
    "    x = torch.ones((b, n))\n",
    "    for num_threads in [1, 4, 16, 32]:\n",
    "        results.append(benchmark.Timer(\n",
    "            stmt='batched_dot_mul_sum(x, x)',\n",
    "            setup='from __main__ import batched_dot_mul_sum',\n",
    "            globals={'x': x},\n",
    "            num_threads=num_threads,\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description='mul/sum',\n",
    "        ).blocked_autorange(min_run_time=1))\n",
    "        results.append(benchmark.Timer(\n",
    "            stmt='batched_dot_bmm(x, x)',\n",
    "            setup='from __main__ import batched_dot_bmm',\n",
    "            globals={'x': x},\n",
    "            num_threads=num_threads,\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description='bmm',\n",
    "        ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的结果表明，对于多线程上运行的大张量，归约到 `bmm` 的版本更好，而对于较小的和/或单线程代码，另一个版本更好。\n",
    "\n",
    "`Compare` 还提供了更改表格格式的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------- Batched dot --------------]\n",
      "                      |  mul/sum  |   bmm  \n",
      "1 threads: --------------------------------\n",
      "      [1, 1]          |  \u001b[92m\u001b[1m      5\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 64]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |        6  |      11\n",
      "      [1, 10000]      |       10  |      12\n",
      "      [64, 1]         |       10  |      16\n",
      "      [64, 64]        |        8  |      10\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     36\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   230\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m    294\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2100\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        7  |      17\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     42\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    88\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m    480\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  3500\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m  28000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 30000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     17\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    81\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m    340\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   750\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m  27000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 30000\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m 300000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m300000\u001b[0m\u001b[0m\n",
      "4 threads: --------------------------------\n",
      "      [1, 1]          |  \u001b[92m\u001b[1m      5\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 64]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |        6  |      12\n",
      "      [1, 10000]      |  \u001b[2m\u001b[91m     10\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    20\u001b[0m\u001b[0m\n",
      "      [64, 1]         |       10  |  \u001b[2m\u001b[91m    21\u001b[0m\u001b[0m\n",
      "      [64, 64]        |        8  |      16\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     38\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   320\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m     95\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  3000\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        7  |      16\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     46\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    40\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m    139\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   990\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m  11000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  9300\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     17\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    30\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m    105\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   220\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m  11490\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  9470\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m 100000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 90000\u001b[0m\u001b[0m\n",
      "16 threads: -------------------------------\n",
      "      [1, 1]          |  \u001b[92m\u001b[1m      5\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 64]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |      16\n",
      "      [1, 1024]       |        6  |      14\n",
      "      [1, 10000]      |  \u001b[2m\u001b[91m     10\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    21\u001b[0m\u001b[0m\n",
      "      [64, 1]         |       10  |      17\n",
      "      [64, 64]        |  \u001b[2m\u001b[91m     12\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    20\u001b[0m\u001b[0m\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     50\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   700\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m     60\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  6000\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        7  |  \u001b[2m\u001b[91m    25\u001b[0m\u001b[0m\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     40\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    20\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m     80\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   300\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m   8520\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2620\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     23\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    30\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m     63\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    80\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m   8210\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2600\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m  86000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 24400\u001b[0m\u001b[0m\n",
      "32 threads: -------------------------------\n",
      "      [1, 1]          |  \u001b[92m\u001b[1m      5\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m    10\u001b[0m\u001b[0m\n",
      "      [1, 64]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     9\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |        6  |      11\n",
      "      [1, 10000]      |  \u001b[2m\u001b[91m     20\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    19\u001b[0m\u001b[0m\n",
      "      [64, 1]         |  \u001b[34m\u001b[1m      6\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    30\u001b[0m\u001b[0m\n",
      "      [64, 64]        |        8  |  \u001b[2m\u001b[91m    27\u001b[0m\u001b[0m\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     66\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   600\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m    160\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  9000\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        7  |  \u001b[2m\u001b[91m    30\u001b[0m\u001b[0m\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     70\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    33\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m     95\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   210\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m   7600\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     17\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    31\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m    100\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    90\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m   7600\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  1000\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m  80000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 20000\u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保证和加载基准测试结果\n",
    "\n",
    "Measurement（以及 CallgrindStats）是可 pickle 的。这使得 A/B 测试变得容易，因为您可以从两个不同的环境中收集度量值，pickle 它们，然后将它们加载到一个环境中。Timer 甚至接受一个 env 构造函数参数，以便这样的 A/B 测试能够无缝地工作。\n",
    "\n",
    "让我们想象一下，`add/sum` 和 `bmm` 方法不是在两个 Python 函数中，而是在 PyTorch 的两个不同版本中。下面的例子演示了 A/B 如何测试它们。为了简单起见，我们只使用了形状的子集，并通过 pickle 简单地往返传递结果，而不是实际使用多个环境并将结果写入磁盘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Batched dot -------------------------------------]\n",
      "                                               |  [1, 1]  |  [1024, 10000]  |  [10000, 1]\n",
      "1 threads: ------------------------------------------------------------------------------\n",
      "  (environment A: mul/sum)  batched_dot(x, x)  |  \u001b[92m\u001b[1m 5.3  \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    28000    \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    17    \u001b[0m\u001b[0m\n",
      "  (environment B: bmm)      batched_dot(x, x)  |   9.3    |      34000      |  \u001b[31m\u001b[1m    89    \u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "ab_test_results = []\n",
    "for env in ('environment A: mul/sum', 'environment B: bmm'):\n",
    "    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n",
    "        x = torch.ones((b, n))\n",
    "        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n",
    "        m = benchmark.Timer(\n",
    "            stmt='batched_dot(x, x)',\n",
    "            globals={'x': x, 'batched_dot': dot_fn},\n",
    "            num_threads=1,\n",
    "            label='Batched dot',\n",
    "            description=f'[{b}, {n}]',\n",
    "            env=env,\n",
    "        ).blocked_autorange(min_run_time=1)\n",
    "        ab_test_results.append(pickle.dumps(m))\n",
    "\n",
    "ab_results = [pickle.loads(i) for i in ab_test_results]\n",
    "compare = benchmark.Compare(ab_results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And just to show that we can round trip all of the results from earlier:\n",
    "round_tripped_results = pickle.loads(pickle.dumps(results))\n",
    "assert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成带有模糊参数的输入\n",
    "\n",
    "正如我们在前一节中看到的，根据输入张量的不同，可能会有一些明显的性能差异。因此，在许多不同的输入上运行基准测试是一个好主意。然而，创建所有这些输入张量可能是乏味的，这就是 `torch.utils.benchmark.Fuzzer` 和相关类的作用。让我们看看如何使用 `Fuzzer` 为基准测试创建一些测试用例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------- Batched dot ---------------------]\n",
      "                                     |  mul/sum  |   bmm \n",
      "1 threads: ----------------------------------------------\n",
      "      725    x 257                   |      94   |    200\n",
      "      49     x 383                   |      14   |     31\n",
      "      34     x 1468                  |      32   |    180\n",
      "      187    x 5039                  |     430   |   3100\n",
      "      2140   x 1296 (discontiguous)  |    1900   |  73000\n",
      "      78     x 1598                  |      62   |    430\n",
      "      519    x 763                   |     180   |   1300\n",
      "      141    x 1082                  |      74   |    540\n",
      "      78     x 5    (discontiguous)  |       7   |     11\n",
      "      187    x 1                     |       7   |     12\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n",
    "\n",
    "# Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a\n",
    "# loguniform distribution in [1, 10000], 40% of which will be discontiguous on average.\n",
    "example_fuzzer = Fuzzer(\n",
    "    parameters = [\n",
    "        FuzzedParameter('k0', minval=1, maxval=10000, distribution='loguniform'),\n",
    "        FuzzedParameter('k1', minval=1, maxval=10000, distribution='loguniform'),\n",
    "    ],\n",
    "    tensors = [\n",
    "        FuzzedTensor('x', size=('k0', 'k1'), min_elements=128, max_elements=10000000, probability_contiguous=0.6)\n",
    "    ],\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for tensors, tensor_params, params in example_fuzzer.take(10):\n",
    "    # description is the column label\n",
    "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_mul_sum(x, x)',\n",
    "        setup='from __main__ import batched_dot_mul_sum',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='mul/sum',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_bmm(x, x)',\n",
    "        setup='from __main__ import batched_dot_bmm',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='bmm',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义自己的 Fuzzers 有很大的灵活性，这对于创建一组强大的基准测试输入非常有用。但是为了使事情更简单，PyTorch 基准测试模块提供了一些内建 Fuzzers 来满足常见的基准测试需求。让我们来看看如何使用这些内置模糊器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------------------- Batched dot ------------------------]\n",
      "                                         |  mul/sum  |   bmm  \n",
      "1 threads: ---------------------------------------------------\n",
      "      64     x 473  (discontiguous)      |  \u001b[92m\u001b[1m 14000 \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 80000\u001b[0m\u001b[0m\n",
      "      16384  x 12642115 (discontiguous)  |  \u001b[92m\u001b[1m    32 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m   100\u001b[0m\u001b[0m\n",
      "      8192   x 892                       |  \u001b[92m\u001b[1m  7100 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 24300\u001b[0m\u001b[0m\n",
      "      512    x 64   (discontiguous)      |  \u001b[92m\u001b[1m 98000 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m340000\u001b[0m\u001b[0m\n",
      "      493    x 27   (discontiguous)      |  \u001b[92m\u001b[1m  2100 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  4950\u001b[0m\u001b[0m\n",
      "      118    x 32   (discontiguous)      |  \u001b[92m\u001b[1m   890 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  2790\u001b[0m\u001b[0m\n",
      "      16     x 495  (discontiguous)      |  \u001b[92m\u001b[1m 28000 \u001b[0m\u001b[0m  |   37000\n",
      "      488    x 62374                     |  \u001b[92m\u001b[1m 93000 \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m100000\u001b[0m\u001b[0m\n",
      "      240372 x 69                        |  \u001b[2m\u001b[91m 47000 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 20000\u001b[0m\u001b[0m\n",
      "      40156  x 32   (discontiguous)      |  \u001b[92m\u001b[1m  1800 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  5300\u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark.op_fuzzers import binary\n",
    "\n",
    "results = []\n",
    "for tensors, tensor_params, params in binary.BinaryOpFuzzer(seed=0).take(10):\n",
    "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_mul_sum(x, x)',\n",
    "        setup='from __main__ import batched_dot_mul_sum',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='mul/sum',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_bmm(x, x)',\n",
    "        setup='from __main__ import batched_dot_bmm',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='bmm',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize(rowwise=True)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Callgrind 收集指令计数\n",
    "\n",
    "优化代码的挑战之一是墙时间的变化和不透明。不确定性的来源有很多，从自适应时钟速度到与其他进程的资源争用。此外，端到端时间不能洞察时间花在什么地方，而这正是我们在优化代码时真正感兴趣的。\n",
    "\n",
    "另一种补充方法是收集指令计数（instruction counts）。这些计数是一个代理指标，并没有捕捉性能的所有方面（例如内存或 I/O 绑定任务），但是它们确实有一些有用的属性。指令计数是可重复的，不受环境变化的影响，并提供对程序在何处花费周期的细粒度洞察。\n",
    "\n",
    "为了了解指令计数的效用，让我们看看如何减少 `batched_dot_mul_sum` 的开销。显而易见的解决方案是将其转移到 C++，这样我们就可以避免多次在 Python 和 C++ 之间切换。\n",
    "\n",
    "幸运的是，源码几乎是相同的。在 C++ 中我们必须要问的一个问题是，我们是应该通过值还是引用来获取参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc8811b4dc0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup:\n",
      "  from __main__ import batched_dot_mul_sum\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  5.22 us\n",
      "  1 measurement, 100000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc92d5ba830>\n",
      "cpp_lib.batched_dot_mul_sum_v0(x, x)\n",
      "setup:\n",
      "  import cpp_lib\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  4.35 us\n",
      "  1 measurement, 100000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fc88118aef0>\n",
      "cpp_lib.batched_dot_mul_sum_v1(x, x)\n",
      "setup:\n",
      "  import cpp_lib\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  4.11 us\n",
      "  1 measurement, 100000 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "batched_dot_src = \"\"\"\\\n",
    "/* ---- Python ---- */\n",
    "// def batched_dot_mul_sum(a, b):\n",
    "//     return a.mul(b).sum(-1)\n",
    "\n",
    "torch::Tensor batched_dot_mul_sum_v0(\n",
    "    const torch::Tensor a,\n",
    "    const torch::Tensor b) {\n",
    "  return a.mul(b).sum(-1);\n",
    "}\n",
    "\n",
    "torch::Tensor batched_dot_mul_sum_v1(\n",
    "    const torch::Tensor& a,\n",
    "    const torch::Tensor& b) {\n",
    "  return a.mul(b).sum(-1);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# PyTorch makes it easy to test our C++ implementations by providing a utility\n",
    "# to JIT compile C++ source into Python extensions:\n",
    "import os\n",
    "from torch.utils import cpp_extension\n",
    "cpp_lib = cpp_extension.load_inline(\n",
    "    name='cpp_lib',\n",
    "    cpp_sources=batched_dot_src,\n",
    "    extra_cflags=['-O3'],\n",
    "    extra_include_paths=[\n",
    "        # `load_inline` needs to know where to find Pybind11 headers.\n",
    "        os.path.join(os.getenv('CONDA_PREFIX'), 'include')\n",
    "    ],\n",
    "    functions=['batched_dot_mul_sum_v0', 'batched_dot_mul_sum_v1']\n",
    ")\n",
    "\n",
    "# `load_inline` will create a shared object that is loaded into Python. When we collect\n",
    "# instruction counts Timer will create a subprocess, so we need to re-import it. The\n",
    "# import process is slightly more complicated for C extensions, but that's all we're\n",
    "# doing here.\n",
    "module_import_str = f\"\"\"\\\n",
    "# https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"cpp_lib\", {repr(cpp_lib.__file__)})\n",
    "cpp_lib = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cpp_lib)\"\"\"\n",
    "\n",
    "import textwrap\n",
    "def pretty_print(result):\n",
    "    \"\"\"Import machinery for cpp_lib.so can get repetitive to look at.\"\"\"\n",
    "    print(repr(result).replace(textwrap.indent(module_import_str, \"  \"), \"  import cpp_lib\"))\n",
    "\n",
    "\n",
    "t_baseline = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='''\\\n",
    "from __main__ import batched_dot_mul_sum\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='cpp_lib.batched_dot_mul_sum_v0(x, x)',\n",
    "    setup=f'''\\\n",
    "{module_import_str}\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='cpp_lib.batched_dot_mul_sum_v1(x, x)',\n",
    "    setup=f'''\\\n",
    "{module_import_str}\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "# Moving to C++ did indeed reduce overhead, but it's hard to tell which\n",
    "# calling convention is more efficient. v1 (call with references) seems to\n",
    "# be a bit faster, but it's within measurement error.\n",
    "pretty_print(t_baseline.blocked_autorange())\n",
    "pretty_print(t0.blocked_autorange())\n",
    "pretty_print(t1.blocked_autorange())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Callgrind to determine which is better.\n",
    "stats_v0 = t0.collect_callgrind()\n",
    "stats_v1 = t1.collect_callgrind()\n",
    "\n",
    "pretty_print(stats_v0)\n",
    "pretty_print(stats_v1)\n",
    "\n",
    "# `.as_standardized` removes file names and some path prefixes, and makes\n",
    "# it easier to read the function symbols.\n",
    "stats_v0 = stats_v0.as_standardized()\n",
    "stats_v1 = stats_v1.as_standardized()\n",
    "\n",
    "# `.delta` diffs the instruction counts, and `.denoise` removes several\n",
    "# functions in the Python interpreter that are known to have significant\n",
    "# jitter.\n",
    "delta = stats_v1.delta(stats_v0).denoise()\n",
    "\n",
    "# `.transform` is a convenience API for transforming function names. It is\n",
    "# useful for increasing cancelation when diff-ing instructions, as well as\n",
    "# just generally improving readability.\n",
    "replacements = (\n",
    "    (\"???:void pybind11\", \"pybind11\"),\n",
    "    (\"batched_dot_mul_sum_v0\", \"batched_dot_mul_sum_v1\"),\n",
    "    (\"at::Tensor, at::Tensor\", \"...\"),\n",
    "    (\"at::Tensor const&, at::Tensor const&\", \"...\"),\n",
    "    (\"auto torch::detail::wrap_pybind_function_impl_\", \"wrap_pybind_function_impl_\"),\n",
    ")\n",
    "for before, after in replacements:\n",
    "    delta = delta.transform(lambda l: l.replace(before, after))\n",
    "\n",
    "# We can use print options to control how much of the function to display.\n",
    "torch.set_printoptions(linewidth=160)\n",
    "\n",
    "# Once parsed, the instruction counts make clear that passing `a` and `b`\n",
    "# by reference is more efficient as it skips some c10::TensorImpl bookkeeping\n",
    "# for the intermediate Tensors, and is also works better with PyBind11. This\n",
    "# is consistent with our noisy wall time observations.\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvmx': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e579259ee6098e2b9319de590d145b4b096774fe457bdf04260e3ba5c171e887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}