{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTQ 小范例（eager）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.1.0+cu121 \n",
      "torchvision: 0.16.0+cu121\n"
     ]
    }
   ],
   "source": [
    "def print_version():\n",
    "    import torch\n",
    "    import torchvision\n",
    "\n",
    "    # 查看核心包的版本\n",
    "    print(f'torch: {torch.__version__} \\n'\n",
    "          f'torchvision: {torchvision.__version__}')\n",
    "\n",
    "print_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面将逐步展开 PTQ 的知识点。\n",
    "\n",
    "## 可量化模型(PTQ)\n",
    "\n",
    "```{rubric} 定义浮点模块\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(1, 3, 3)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = torch.nn.Conv2d(3, 16, 1)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        '''提供便捷函数'''\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x= self._forward_impl(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{rubric} 转换可量化模块\n",
    "```\n",
    "\n",
    "将浮点模块 M 转换为可量化模块 QM（量化流程的最关键的一步）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "\n",
    "class QM(M):\n",
    "    '''\n",
    "    Args:\n",
    "        is_print: 为了测试需求，打印一些信息\n",
    "    '''\n",
    "    def __init__(self, is_print: bool=False):\n",
    "        super().__init__()\n",
    "        self.is_print = is_print\n",
    "        self.quant = QuantStub() # 将张量从浮点转换为量化\n",
    "        self.dequant = DeQuantStub() # 将张量从量化转换为浮点\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.is_print:\n",
    "            print('原始类型：', x.dtype)\n",
    "        # 手动指定张量将在量化模型中从浮点模块转换为量化模块的位置\n",
    "        x = self.quant(x)\n",
    "        if self.is_print:\n",
    "            print('量化前的类型：', x.dtype)\n",
    "        x = self._forward_impl(x)\n",
    "        if self.is_print:\n",
    "            print('量化中的类型：',x.dtype)\n",
    "        # 在量化模型中手动指定张量从量化到浮点的转换位置\n",
    "        x = self.dequant(x)\n",
    "        if self.is_print:\n",
    "            print('量化后的类型：', x.dtype)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以简写为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantWrapper(\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (module): M(\n",
       "    (conv): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (relu): ReLU()\n",
       "    (conv2): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.ao.quantization.stubs import QuantWrapper\n",
    "QuantWrapper(M())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{class}`~torch.ao.quantization.stubs.QuantStub` 和 {class}`~torch.ao.quantization.stubs.DeQuantStub` 在量化过程中起到观测者的作用。{class}`~torch.ao.quantization.stubs.QuantStub` 在训练阶段会将输入数据量化为较低的精度表示，而 {class}`~torch.ao.quantization.stubs.DeQuantStub` 层在推理阶段会将输入数据从较低的精度表示恢复为原始的高精度数据。这样，模型在推理时可以获得与训练阶段相同的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始类型： torch.float32\n",
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4) # 输入的数据\n",
    "\n",
    "m = QM(is_print=True)\n",
    "x = m(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看权重的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.conv.weight.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，此时模块 `m` 是浮点模块。\n",
    "\n",
    "要使 `PTQ` 生效，必须将模型设置为 `eval` 模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (conv2): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建浮点模型实例\n",
    "model_fp32 = QM(is_print=True)\n",
    "model_fp32.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看此时的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始类型： torch.float32\n",
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n",
      "激活和权重的数据类型分别为：torch.float32, torch.float32\n"
     ]
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32(input_fp32)\n",
    "print('激活和权重的数据类型分别为：'\n",
    "      f'{x.dtype}, {model_fp32.conv.weight.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义观测器(PTQ)\n",
    "\n",
    "赋值实例变量 `qconfig`，其中包含关于要附加哪种观测器的信息：\n",
    "\n",
    "- 使用 [`'fbgemm'`](https://github.com/pytorch/FBGEMM) 用于带 AVX2 的 x86（没有AVX2，一些运算的实现效率很低）；使用 [`'qnnpack'`](https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu/qnnpack) 用于 ARM CPU（通常出现在移动/嵌入式设备中）。\n",
    "- 其他量化配置，如选择对称或非对称量化和 `MinMax` 或 `L2Norm` 校准技术，可以在这里指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合激活层(PTQ)\n",
    "\n",
    "在适用的地方，融合 activation 到前面的层（这需要根据模型架构手动完成）。常见的融合包括 `conv + relu` 和 `conv + batchnorm + relu`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (conv2): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,\n",
    "                                                      [['conv', 'relu']])\n",
    "                                                    \n",
    "model_fp32_fused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 `model_fp32_fused` 中 `ConvReLU2d` 融合 `model_fp32` 的两个层 `conv` 和 `relu`。\n",
    "\n",
    "## 启用观测器(PTQ)\n",
    "\n",
    "在融合后的模块中启用观测器，用于在校准期间观测激活（activation）张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (conv2): Conv2d(\n",
       "    3, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "model_fp32_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 校准准备好的模型(PTQ)\n",
    "\n",
    "校准准备好的模型，以确定量化参数的激活在现实世界的设置，校准具有代表性的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始类型： torch.float32\n",
      "量化前的类型： torch.float32\n",
      "量化中的类型： torch.float32\n",
      "量化后的类型： torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): HistogramObserver(min_val=0.0, max_val=1.430925965309143)\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (conv2): Conv2d(\n",
       "    3, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "    (activation_post_process): HistogramObserver(min_val=-0.8590439558029175, max_val=0.9416270852088928)\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=-1.7396681308746338, max_val=2.5327765941619873)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "\n",
    "x = model_fp32_prepared(input_fp32)\n",
    "model_fp32_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型转换(PTQ)\n",
    "\n",
    "```{note}\n",
    "量化权重，计算和存储每个激活张量要使用的尺度（scale）和偏差（bias）值，并用量化实现替换关键算子。\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换已校准好的模型为量化模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QM(\n",
       "  (conv): QuantizedConvReLU2d(1, 3, kernel_size=(3, 3), stride=(1, 1), scale=0.005608734209090471, zero_point=0)\n",
       "  (relu): Identity()\n",
       "  (conv2): QuantizedConv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.007058007176965475, zero_point=122)\n",
       "  (quant): Quantize(scale=tensor([0.0167]), zero_point=tensor([104]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "model_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看权重的数据类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.qint8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8.conv.weight().dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出此时权重的元素大小为 1 字节，而不是 FP32 的 4 字节："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8.conv.weight().element_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行模型，相关的计算将在 {data}`torch.qint8` 中发生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始类型： torch.float32\n",
      "量化前的类型： torch.quint8\n",
      "量化中的类型： torch.quint8\n",
      "量化后的类型： torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model_int8(input_fp32)\n",
    "res.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.ao.nn.quantized as nnq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
